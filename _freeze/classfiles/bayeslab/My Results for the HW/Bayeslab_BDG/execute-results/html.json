{
  "hash": "66673d9f18108bded8f5bbcb5b435967",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Bayesian Assignment\"\nauthor: \"Brian D. Gerber\"\ndate: \"2024-9-17\"\n---\n\n\n\n\n# Part 1\n\n## Priors\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Bayeslab_BDG_files/figure-html/part 1 priors-1.png){width=672}\n:::\n:::\n\n\n## Posteriors\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Bayeslab_BDG_files/figure-html/part 1 posteriors-1.png){width=672}\n:::\n:::\n\n\n## Posterior Means and 95% CIs\n\n*95% CI interpretation: There is a 95% probability that the true parameter value lies within the reported 95% credible interval.*\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n        Prior1lemurs1 Prior2lemurs1 Prior2lemurs2 Prior2lemurs2\nMLE         0.5555556     0.5555556     0.4545455     0.4545455\nmean        0.9640434     0.7931241     0.7291829     0.5037267\nlow.ci      0.7862633     0.4447163     0.6164777     0.3793931\nhigh.ci     1.1533789     1.2407402     0.8532128     0.6454779\n```\n\n\n:::\n:::\n\n\nThe highly informative prior had a strong influence on the posterior distribution of the small sample size data set and a bit less, but still a strong influence on the large sample size data set. The less informative prior had a moderate effect on the low sample size data and very little effect on the large sample size data set. \n\n\n# Part 2\n\n## Posterior distributions\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#JAGS data lists (different for 2 datasets)\n  data1 <- list(\n            lemurs1=lemurs1[,1],\n            N1=length(lemurs1[,1])\n            )\n\n  data2 <- list(\n            lemurs2=lemurs2[,1],\n            N2=length(lemurs2)\n            )\n\n#MCMC inputs  (same across priors and datasets)\n  n.chains = 2\n  n.adapt = 1000\n  n.iter = 10000\n  thin = 2\n  burn = 5000\n\n# Model Parameters to save values of (same across priors and datasets)\n  parms <- c(\"p\")\t\n\n\n# Setup the Models\n  jm1 <- jags.model(file=\"model.jags1.r\", data = data1, n.chains = n.chains)\n  jm2 <- jags.model(file=\"model.jags2.r\", data = data1, n.chains = n.chains)\n  jm3 <- jags.model(file=\"model.jags3.r\", data = data2, n.chains = n.chains)\n  jm4 <- jags.model(file=\"model.jags4.r\", data = data2, n.chains = n.chains)\n\n# Update the models with the burnin\n  update(jm1, n.iter = burn, n.adapt = n.adapt)\n  update(jm2, n.iter = burn, n.adapt = n.adapt)\n  update(jm3, n.iter = burn, n.adapt = n.adapt)\n  update(jm4, n.iter = burn, n.adapt = n.adapt)\n\n# Fit the models\n  post1=coda.samples(jm1, variable.names = parms, n.iter = n.iter, thin = thin)\n  post2=coda.samples(jm2, variable.names = parms, n.iter = n.iter, thin = thin)\n  post3=coda.samples(jm3, variable.names = parms, n.iter = n.iter, thin = thin)\n  post4=coda.samples(jm4, variable.names = parms, n.iter = n.iter, thin = thin)\n\n# Plots of posteriors for lambda\n  par(mfrow = c(2,2))\n  hist(as.matrix(post1), main = \"Prior 1, Dataset 1\")\n  hist(as.matrix(post2), main = \"Prior 2, Dataset 1\")\n  hist(as.matrix(post3), main = \"Prior 1, Dataset 2\")\n  hist(as.matrix(post4), main = \"Prior 2, Dataset 2\")\n```\n\n::: {.cell-output-display}\n![](Bayeslab_BDG_files/figure-html/jags implementation-1.png){width=672}\n:::\n:::\n\n\n## Posterior means and 95% CIs \n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n        Prior1lemurs1 Prior2lemurs1 Prior2lemurs2 Prior2lemurs2\nMLE         0.5555556     0.5555556     0.4545455    0.45454545\nmean       49.9368464    19.8976887     0.4946738    0.19839566\nlow.ci     16.3523063     2.3961125     0.1621196    0.02443574\nhigh.ci   102.5103574    55.9200630     1.0261025    0.54544397\n```\n\n\n:::\n:::\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}