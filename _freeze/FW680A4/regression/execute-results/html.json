{
  "hash": "c5cc73678cb5895c90c2483fcab5fe18",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: <span style=\"color:white\">Linear Regression</span>\ntitle-slide-attributes:\n    data-background-image: /img/backgroundforest.png\nformat:\n  revealjs:\n    chalkboard: true\n    multiplex: true\n---\n\n\n<!-- knitr::purl(\"./FW680A4/regression.qmd\", output=\"./classfiles/regressionlab/regression.R\") -->\n\n## Objectives\n\n-   fundamentals\n-   assumptions\n-   lm / glm functions\n-   confidence intervals\n-   case study\n\n## \n\n\n```{=html}\n\n<br>\n<br>\n<br>\n<br>\n<br>\n<center><span style=\"color:#C70039; font-size: 65px;\";><b>Fundamental Idea</b></span></center>\n```\n\n\n## Why model data?\n\nIn ecology, we use models to,\n\n-   describe relationships among outcomes and processes\n-   to estimate hidden (latent) processes\n-   predict unobserved values\n-   forecast future outcomes, such as responses to management\n\n## Linear Regression (motivation)\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-revealjs/unnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n\n## Linear Regression (Equation)\n\n![](/img/index.notation.png){fig-align=\"center\" width=\"400\"}\n\n## Linear Regression (Equation 2) \n\n<br>\n\n$$\ny_{i} \\sim \\text{Normal}(\\mu_{i}, \\sigma)\\\\\n\\mu_{i} = \\beta_0 + \\beta_1 \\times x_i \\\\\n$$\n\n## Linear Regression Line 1 {.scrollable}\n$$\n\\hat{\\mu_{i}} = 9.06 + 2\\times x_i\n$$\n\n\n::: {.cell layout-align=\"center\" width='1000'}\n::: {.cell-output-display}\n![](regression_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Linear Regression Line 2 \n$$\n\\hat{\\mu_{i}} = 9.06 + 2\\times x_i\n$$\n\n\n::: {.cell layout-align=\"center\" width='1000'}\n::: {.cell-output-display}\n![](regression_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Linear Regression Line 3 \n$$\n\\hat{\\mu_{i}} = 9.06 + 2\\times x_i\n$$\n\n\n::: {.cell layout-align=\"center\" width='1000'}\n::: {.cell-output-display}\n![](regression_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=960}\n:::\n:::\n\n## Residuals\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n\n<!-- ## Visual -->\n<!-- ![](/img/regression.png){fig-align=\"center\" width=\"450\"} -->\n\n\n## One Sample\n\n$$\n\\begin{align*}\ny_{i} \\sim& \\text{Normal}(\\mu_{i},\\sigma) \\\\\n\\mu_{i} =& \\beta_{0} + \\beta_{1}x_{i}\\\\\n\\mu_{i} =& 1 + 0.5 \\times x_{i}\n\\end{align*}\n$$\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](regression_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Sampling Distributions of y\n\n$$\n\\begin{align*}\n\\mu_{i} = 1 + 0.5 \\times x_{i}\n\\end{align*}\n$$\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n\n## Sampling Distributions of $\\hat{\\mu}$\n\n$$\n\\begin{align*}\n\\mu_{i} = 1 + 0.5 \\times x_{i}\n\\end{align*}\n$$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n\n\n## \n\n\n```{=html}\n\n<br>\n<br>\n<br>\n<br>\n<br>\n<center><span style=\"color:#C70039; font-size: 65px;\";><b>Assumptions</b></span></center>\n```\n\n\n\n## Assumptions\n#### Independence of the errors\n\n\n<br>\n\nCorrelation($\\epsilon_{i}$,$\\epsilon_{j}$) = 0, $\\forall$ pairs of $i$ and $j$\n\n<br>\n\nThis means that knowing how far observation  $i$ will be from the true regression line tells us nothing about how far observation  $j$  will be from the regression line.\n\n## Assumptions {.scrollable}\n\n#### Homogeniety of the variance\n\nvar($\\epsilon_{i}$) = $\\sigma^2$\n\nConstancy in the scatter of observations above and below the line, going left to right.\n\n\n:::{.fragment}\n\n::: {.cell width='200'}\n::: {.cell-output-display}\n![](regression_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n:::\n\n## Assumptions\n\n#### Heteroskedasticity\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](regression_files/figure-revealjs/unnamed-chunk-11-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Assumptions\n\n#### Linearity\n\n<br>\n$$\nE[y_i|x_i] = \\mu_i = \\beta_0 + \\beta_1 \\times x_i \\\\\n$$\n<br>\n\nThe hypothesis about the variables included in the model (e.g., $x_i$) characterizes the mean well. \n\n## Assumptions\n#### Normality\n\n<br>\n$$\n\\epsilon_i \\sim \\text{Normal}(0,\\sigma)\n$$\n<br>\n\nEach $i^{th}$ residual \n\n::: {.incremental}\n\n-   comes from a Normal distribution with a  mean of zero\n-   is symmetrically disributed around zero \n-   varies around zero by $\\sigma$, which is the same for each residual. \n:::\n\n## Assumption Violations\n\n### Robustness\n\n<br>\n\n\nLinearity and constant variance are often more important than the assumption of normality (see e.g., [Knief & Forstmeier, 2021](https://link.springer.com/article/10.3758/s13428-021-01587-5) and references therein)\n\n\n<br>\n\n::: {.fragment}\n\n<p style=\"text-align:center\">\n<b>This is especially true for large sample sizes</b>\n</p>\n\n:::\n\n## \n\n\n```{=html}\n\n<br>\n<br>\n<br>\n<br>\n<br>\n<center><span style=\"color:#C70039; font-size: 65px;\";><b>lm and glm </b></span></center>\n```\n\n\n\n## Intercept-Only Model\n\n$$\ny_{i} \\sim \\text{Normal}(\\mu_{i}, \\sigma^2)\\\\\n\\mu_{i} = \\beta_0\n$$\n\n::: {.fragment}\n\n#### Generate Data\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n#Setup parameters\n  n = 100 # sample size\n  beta0 = 10 # true mean\n  sigma = 2 # true std.dev\n\n# Simulate a data set of observations\n  set.seed(43243)\n  y = rnorm(n, mean = beta0, sd = sigma)\n```\n:::\n\n\n:::\n\n## Visualize Intercept-Only Model\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n  hist(y)\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-revealjs/unnamed-chunk-13-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Fit Intercept-Only Model \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit model/hypothesis using maximum likelihood\n  model1.0 = lm(y~1)\n\n  model1.1 = glm(y~1)\n\n  model1.2 = glm(y~1, family=gaussian(link = identity))\n\n  \n  \n# Compare Results  \n  data.frame(intercept=c(model1.0$coefficients,model1.1$coefficients, model1.2$coefficients),\n             SE = c(summary(model1.0)$coefficients[, 2], summary(model1.1)$coefficients[, 2],summary(model1.2)$coefficients[, 2])\n            )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  intercept        SE\n1  9.913821 0.1765343\n2  9.913821 0.1765343\n3  9.913821 0.1765343\n```\n\n\n:::\n:::\n\n\n## Fit Intercept-Only Model \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Summary of model results  \n  summary(model1.0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ 1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.7815 -1.2733 -0.0581  1.1558  4.4979 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   9.9138     0.1765   56.16   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.765 on 99 degrees of freedom\n```\n\n\n:::\n:::\n\n\n\n## Fitted-values {.scrollable}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Predict response for all data\n  preds = predict(model1.0, se.fit = TRUE)\n  preds\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$fit\n       1        2        3        4        5        6        7        8 \n9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 \n       9       10       11       12       13       14       15       16 \n9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 \n      17       18       19       20       21       22       23       24 \n9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 \n      25       26       27       28       29       30       31       32 \n9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 \n      33       34       35       36       37       38       39       40 \n9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 \n      41       42       43       44       45       46       47       48 \n9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 \n      49       50       51       52       53       54       55       56 \n9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 \n      57       58       59       60       61       62       63       64 \n9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 \n      65       66       67       68       69       70       71       72 \n9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 \n      73       74       75       76       77       78       79       80 \n9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 \n      81       82       83       84       85       86       87       88 \n9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 \n      89       90       91       92       93       94       95       96 \n9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 \n      97       98       99      100 \n9.913821 9.913821 9.913821 9.913821 \n\n$se.fit\n  [1] 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343\n  [8] 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343\n [15] 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343\n [22] 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343\n [29] 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343\n [36] 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343\n [43] 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343\n [50] 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343\n [57] 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343\n [64] 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343\n [71] 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343\n [78] 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343\n [85] 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343\n [92] 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343\n [99] 0.1765343 0.1765343\n\n$df\n[1] 99\n\n$residual.scale\n[1] 1.765343\n```\n\n\n:::\n:::\n\n\n## \n\n\n```{=html}\n\n<br>\n<br>\n<br>\n<br>\n<br>\n<center><span style=\"color:#C70039; font-size: 65px;\";><b>Confidence Intervals </b></span></center>\n```\n\n\n\n## Confidence Intervals \n#### Normal Approximation\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get 90% confidence intervals (Type I error = 0.1)\n  c(\n    (preds$fit+preds$se.fit*qnorm(0.05))[1],\n    (preds$fit+preds$se.fit*qnorm(0.95))[1]\n   )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        1         1 \n 9.623448 10.204195 \n```\n\n\n:::\n:::\n\n\n:::{.fragment}\n\n::: {.cell}\n\n```{.r .cell-code}\n  CI.Normal=confint(model1.0, level=0.9)\n  CI.Normal\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                 5 %     95 %\n(Intercept) 9.620705 10.20694\n```\n\n\n:::\n:::\n\n:::  \n  \n\n\n## What is a CI?\n\n. . .\n\n\"A confidence interval for a parameter is an interval computed using sample data ...\n\n. . .\n\n\"... by a method that will contain the parameter for a specified proportion of all samples.\n\n. . .\n\nThe success rate (proportion of all samples whose intervals contain the parameter) is known as the confidence level.\" [R. H. Lock et al. (2020)](https://www.wileyplus.com/math-and-statistics/lock-statistics-unlocking-the-power-of-data-3e-eprof20502/)\n\n## What is a CI?\n\n#### Key\n\n-   the parameter we are trying to estimate is a fixed unknown (i.e., it is not varying across samples)\n\n\n:::{fragment}\n-   the endpoints of our confidence interval are random and will change every time we collect a new data set (the endpoints themselves actually have a sampling distribution!)\n:::\n\n[More at Stats4Ecologists](https://statistics4ecologists-v2.netlify.app/linreg#confidence-intervals)\n\n\n## What is a CI?\n\n![](/img/CI.png){fig-align=\"center\"}\n\n## Bootstrapping \n\n\nInstead of relying on the 95% intervals from an assumed normal distribution, we will create a distribution by \nresampling our data.\n\n<br>\n\nSee, [Stats4Ecologists](https://fw8051statistics4ecologists.netlify.app/boot.html)   \n\n## Bootstrapping (idea)\n![](/img/fieberg0.png){fig-align=\"center\" width=\"450\"}\n\n## Bootstrapping (idea)\n\n![](/img/fieberg1.png){fig-align=\"center\" width=\"450\"}\n\n## Bootstrapping (idea)\n![](/img/fieberg3.png){fig-align=\"center\" width=\"450\"}\n\n## Bootstrapping (code)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Setup\n  nboot <- 1000 # number of bootstrap samples\n  nobs <- length(y)\n  bootcoefs <- rep(NA, nboot)\n# Start loop  \nfor(i in 1:nboot){\n  set.seed(43243+i)\n  # Create bootstrap data set by sampling original observations w/ replacement  \n  bootdat <- y[sample(1:nobs, nobs, replace=TRUE)] \n  # Calculate bootstrap statistic\n  glmboot <- glm(bootdat ~ 1)\n  bootcoefs[i] <- coef(glmboot)\n}\n```\n:::\n\n## Bootstrapping (code)\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-revealjs/unnamed-chunk-20-1.png){width=960}\n:::\n:::\n\n\n## Bootstrapping (code)\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate bootstrap standard errors\n  boot.se = sd(bootcoefs)\n\n# boostrap-normal CI\n  boot.normal = c(\n        (preds$fit+boot.se*qnorm(0.05))[1],\n        (preds$fit+boot.se*qnorm(0.95))[1]\n        )\n\n# bootstrap percentile\nconfdat.boot.pct <- quantile(bootcoefs, probs = c(0.05, 0.95))\n```\n:::\n\n\n## Comparison\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-revealjs/unnamed-chunk-22-1.png){width=960}\n:::\n:::\n\n\n\n\n## \n\n\n```{=html}\n\n<br>\n<br>\n<br>\n<br>\n<br>\n<center><span style=\"color:#C70039; font-size: 65px;\";><b>Independent Variables</b></span></center>\n```\n\n\n\n## Prairie Dog Calling {.scrollable}\n\n\n::: {.cell}\n\n:::\n\n\n![](/img/pdog.png){fig-align=\"center\" width=\"1200\"}\n\n y = # of calls per 5 minute\n \n temp = temperature, degrees F\n \n dist.center = location from center of colony\n\n\n::: {.cell}\n\n```{.r .cell-code}\n  head(dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           y     temp dist.center\n1 0.38129119 39.96298    11.75578\n2 0.57770546 47.95317    14.63583\n3 0.00347806 56.13182    22.89228\n4 0.00000000 29.76491    21.14168\n5 2.67643282 40.25998    32.41442\n6 1.71519689 38.31469    35.99573\n```\n\n\n:::\n:::\n\n\n\n## Exploratory {.scrollable}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-revealjs/unnamed-chunk-25-1.png){width=960}\n:::\n\n::: {.cell-output-display}\n![](regression_files/figure-revealjs/unnamed-chunk-25-2.png){width=960}\n:::\n\n::: {.cell-output-display}\n![](regression_files/figure-revealjs/unnamed-chunk-25-3.png){width=960}\n:::\n:::\n\n## Model Fitting 1\n\n\n::: {.cell}\n\n```{.r .cell-code}\n  model = lm(y ~ temp+dist.center, data=dat)\n  summary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ temp + dist.center, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.6714 -1.6690 -0.1506  1.4910  5.0041 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -8.312197   1.074281  -7.737 9.65e-12 ***\ntemp         0.156393   0.024605   6.356 6.75e-09 ***\ndist.center  0.036010   0.003388  10.629  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.338 on 97 degrees of freedom\nMultiple R-squared:  0.9233,\tAdjusted R-squared:  0.9217 \nF-statistic: 583.8 on 2 and 97 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n## Model Notation 1\n\n\n::: {.cell}\n\n```{.r .cell-code}\nequatiomatic::extract_eq(model)\n```\n\n::: {.cell-output-display}\n$$\n\\operatorname{y} = \\alpha + \\beta_{1}(\\operatorname{temp}) + \\beta_{2}(\\operatorname{dist.center}) + \\epsilon\n$$\n\n:::\n\n```{.r .cell-code}\nequatiomatic::extract_eq(model, use_coefs = TRUE)\n```\n\n::: {.cell-output-display}\n$$\n\\operatorname{\\widehat{y}} = -8.31 + 0.16(\\operatorname{temp}) + 0.04(\\operatorname{dist.center})\n$$\n\n:::\n:::\n\n\n\n## Model Fitting 2\n\nMean-center the intercept. Slopes do not change.\n\n::: {.cell}\n\n```{.r .cell-code}\n  model = lm(y ~ I(temp - mean(temp)) + dist.center, data=dat)\n  summary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ I(temp - mean(temp)) + dist.center, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.6714 -1.6690 -0.1506  1.4910  5.0041 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)          2.614311   0.895284   2.920  0.00435 ** \nI(temp - mean(temp)) 0.156393   0.024605   6.356 6.75e-09 ***\ndist.center          0.036010   0.003388  10.629  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.338 on 97 degrees of freedom\nMultiple R-squared:  0.9233,\tAdjusted R-squared:  0.9217 \nF-statistic: 583.8 on 2 and 97 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n## Normalizing x\n\nMean-center and standardize by std. deviation\n\n::: {.cell}\n\n```{.r .cell-code}\n  dat$temp.sc = scale(dat$temp)\n  dat$dist.sc = scale(dat$dist.center)\n```\n:::\n\n\n:::{.fragment}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n  mean(dat$temp.sc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.40337e-16\n```\n\n\n:::\n\n```{.r .cell-code}\n  sd(dat$temp.sc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1\n```\n\n\n:::\n:::\n\n\n:::\n\n## Comparison\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-revealjs/unnamed-chunk-31-1.png){width=960}\n:::\n:::\n\n\n\n## Model Fitting 3 {.scrollable}\n\nWhat does the slope mean now?\n\n$$\n\\mu_{i} = \\beta_0 + \\beta_1 \\times 1 + \\beta_2 \\times 0\n$$\n\n::: {.cell}\n\n```{.r .cell-code}\n  model = lm(y ~ temp.sc + dist.sc, data=dat)\n  summary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ temp.sc + dist.sc, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.6714 -1.6690 -0.1506  1.4910  5.0041 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  11.8002     0.2338  50.471  < 2e-16 ***\ntemp.sc       3.0958     0.4871   6.356 6.75e-09 ***\ndist.sc       5.1771     0.4871  10.629  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.338 on 97 degrees of freedom\nMultiple R-squared:  0.9233,\tAdjusted R-squared:  0.9217 \nF-statistic: 583.8 on 2 and 97 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n\n## Marginal Predictions\n\n$$\n\\hat{\\mu_{i}} = \\hat{\\beta_{0}} + \\hat{\\beta_{1}}\\times 0 +  \\hat{\\beta_{2}} \\times \\text{dist.center}_i\n$$\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot marginal predictions of dist.center\n  marginaleffects::plot_predictions(model, condition=c(\"dist.center\"))\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-revealjs/unnamed-chunk-34-1.png){width=960}\n:::\n:::\n\n\n## Add data points\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot marginal predictions of dist.center\n  plot1 = marginaleffects::plot_predictions(model, condition=c(\"dist.center\"))\n  plot1+geom_point(data=dat, aes(x=dist.center,y=y))\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-revealjs/unnamed-chunk-35-1.png){width=960}\n:::\n:::\n\n\n\n## Combined Predictions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmarginaleffects::plot_predictions(model, condition=list(\"temp\",\"dist.center\"))\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-revealjs/unnamed-chunk-36-1.png){width=960}\n:::\n:::\n\n\n## Combined Predictions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmarginaleffects::plot_predictions(model, condition=list(\"temp\",\"dist.center\" = 0:5))\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-revealjs/unnamed-chunk-37-1.png){width=960}\n:::\n:::\n\n\n\n\n## Taking Control \n\n\n::: {.cell}\n\n```{.r .cell-code}\nnewdata = expand.grid(dat$temp,0:5)\ncolnames(newdata)=c(\"temp\",\"dist.center\")\n\npreds = predict(model,newdata = newdata,interval=\"confidence\", level=0.95)\n\npred.plot = data.frame(newdata,preds)\n```\n:::\n\n\n## Combined Predictions 2 {.scrollable}\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-revealjs/unnamed-chunk-39-1.png){width=960}\n:::\n:::\n\n\n\n\n\n## Evaluating Assumptions\n### Largely done based on the residuals\n\n$y_{i} - \\hat{y}_{i}$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhist(model$residuals)\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-revealjs/unnamed-chunk-40-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n<!-- ## [Linearity & Variance Assumption](https://fw8051statistics4ecologists.netlify.app/linreg.html#exploring-assumptions-using-r) -->\n\n<!-- ![](/img/assumptions2.png){fig-align=\"center\" width=\"1000\"} -->\n\n\n## Linearity Assumption\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(model,1)\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-revealjs/unnamed-chunk-41-1.png){fig-align='center' width=960}\n:::\n:::\n\nIdeally, there will be no pattern and the red line should be roughly horizontal near zero.\n\n\n## Homogeneity of Variance\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(model,3)\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-revealjs/unnamed-chunk-42-1.png){fig-align='center' width=960}\n:::\n:::\n\n\nResiduals should be spread equally along the ranges of predictors. We want a horizontal red line; otherwise, suggests a non-constant variances in the residuals (i.e., heteroscedasticity).\n\n\n## Normality of Residuals\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(model,2)\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-revealjs/unnamed-chunk-43-1.png){fig-align='center' width=960}\n:::\n:::\n\n\nShows theoretical quantiles versus empirical quantiles of the residuals. We want to see cicles on the dotted line.\n\n## Outliers\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npar(mfrow=c(1,2))\nplot(model,4)\nplot(model,5)\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-revealjs/unnamed-chunk-44-1.png){fig-align='center' width=960}\n:::\n:::\n\n\nOutlier: extreme value that can affect the $\\beta$ estimate. Leverage plot: points in the upper right and lower right corner. \n\n## Exploring Assumptions {.scrollable}\n\nNicer looking Plots\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggResidpanel)\nresid_panel(model)\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-revealjs/unnamed-chunk-45-1.png){width=960}\n:::\n:::\n\n\n## Go to Lab\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}