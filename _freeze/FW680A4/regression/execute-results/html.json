{
  "hash": "bac8ee983cf9fd271f1a65d8b0ff7ddb",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: <span style=\"color:white\">Linear Regression</span>\ntitle-slide-attributes:\n    data-background-image: /img/backgroundforest.png\nformat:\n  revealjs:\n    chalkboard: true\n    multiplex: true\n---\n\n\n## Objectives\n::: {.incremental}\n-   linear regression fundamentals\n-   assumptions\n-   lm / glm function\n-   confidence intervals\n-   categorical and continuous independent variables\n-   additive and interactions b/w variables\n:::\n\n## Linear Regression (motivation)\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-revealjs/unnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n\n## Linear Regression (Equation)\n\n![](/img/index.notation.png){fig-align=\"center\" width=\"400\"}\n\n## Linear Regression (Equation 2) \n\n<br>\n\n$$\ny_{i} \\sim \\text{Normal}(\\mu_{i}, \\sigma)\\\\\n\\mu_{i} = \\beta_0 + \\beta_1 \\times x_i\n$$\n\n## Linear Regression Line 1 {.scrollable}\n\n\n::: {.cell layout-align=\"center\" width='1000'}\n::: {.cell-output-display}\n![](regression_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Linear Regression Line 2 {.scrollable}\n\n\n::: {.cell layout-align=\"center\" width='1000'}\n::: {.cell-output-display}\n![](regression_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Linear Regression Line 3 {.scrollable}\n\n\n::: {.cell layout-align=\"center\" width='1000'}\n::: {.cell-output-display}\n![](regression_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Visual\n\n![](/img/regression.png){fig-align=\"center\" width=\"450\"}\n\n## Assumptions\n#### Independence of the errors\n\n\n<br>\n\nCorrelation($\\epsilon_{i}$,$\\epsilon_{j}$) = 0, $\\forall$ pairs of $i$ and $j$\n\n<br>\n\nThis means that knowing how far observation  $i$ will be from the true regression line tells us nothing about how far observation  $j$  will be from the regression line.\n\n## Assumptions\n#### Homogeniety of the variance\n\n<br>\n\nvar($\\epsilon_{i}$) = $\\sigma^2$\n\n<br>\n\nConstancy in the scatter of observations above and below the line\n\n## Assumptions\n\n#### Linearity\n\n<br>\n$$\nE[y_i|x_i] = \\mu_i = \\beta_0 + \\beta_1 \\times x_i \\\\\n$$\n<br>\n\nThe hypothesis about the variables included in the model (e.g., $x_i$) characterizes the mean well. \n\n## Assumptions\n#### Normality\n\n<br>\n$$\n\\epsilon_i \\sim \\text{Normal}(0,\\sigma)\n$$\n<br>\n\nEach $i^{th}$ residual \n\n::: {.incremental}\n\n-   comes from a Normal distribution with a  mean of zero\n-   is symmetrically disributed around zero \n-   varies around zero by $\\sigma$, which is the same for each residual. \n:::\n\n## Assumption Violations\n\n<br>\n\n### Robustness\n\n<br>\n\n::: {.fragment}\n\nLinearity and constant variance are often more important than the assumption of normality (see e.g., [Knief & Forstmeier, 2021](https://link.springer.com/article/10.3758/s13428-021-01587-5) and references therein)\n\n::: \n\n<br>\n\n::: {.fragment}\n\n<p style=\"text-align:center\">\n<b>This is especially true for large sample sizes</b>\n</p>\n\n:::\n\n## Intercept-Only Model\n\n$$\ny_{i} \\sim \\text{Normal}(\\mu_{i}, \\sigma^2)\\\\\n\\mu_{i} = \\beta_0\n$$\n\n## Simulate Intercept-Only Model\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n#Setup parameters\n  n = 100 # sample size\n  beta0 = 10 # true mean\n  sigma = 2 # true std.dev\n\n# Simulate a data set of observations\n  set.seed(43243)\n  y = rnorm(n, mean = beta0, sd = sigma)\n```\n:::\n\n\n## Visualize Intercept-Only Model\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n  hist(y)\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Fit Intercept-Only Model \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit model/hypothesis using maximum likelihood\n  model1 =   glm(y~1, family=gaussian(link = identity))\n\n  model1.1 = glm(y~1)\n\n  model1.2 = lm(y~1)\n  \n# Compare Results  \n  rbind(model1$coefficients,model1.1$coefficients, model1.2$coefficients)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     (Intercept)\n[1,]    9.913821\n[2,]    9.913821\n[3,]    9.913821\n```\n\n\n:::\n:::\n\n\n## Fit Intercept-Only Model \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Summary of model results  \n  summary(model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = y ~ 1, family = gaussian(link = identity))\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   9.9138     0.1765   56.16   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 3.116437)\n\n    Null deviance: 308.53  on 99  degrees of freedom\nResidual deviance: 308.53  on 99  degrees of freedom\nAIC: 400.45\n\nNumber of Fisher Scoring iterations: 2\n```\n\n\n:::\n:::\n\n\n\n## Fitted-values Intercept-Only Model {.scrollable}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Predict response for all data\n  preds=predict(model1, se.fit = TRUE)\n  preds\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$fit\n       1        2        3        4        5        6        7        8 \n9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 \n       9       10       11       12       13       14       15       16 \n9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 \n      17       18       19       20       21       22       23       24 \n9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 \n      25       26       27       28       29       30       31       32 \n9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 \n      33       34       35       36       37       38       39       40 \n9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 \n      41       42       43       44       45       46       47       48 \n9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 \n      49       50       51       52       53       54       55       56 \n9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 \n      57       58       59       60       61       62       63       64 \n9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 \n      65       66       67       68       69       70       71       72 \n9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 \n      73       74       75       76       77       78       79       80 \n9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 \n      81       82       83       84       85       86       87       88 \n9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 \n      89       90       91       92       93       94       95       96 \n9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 9.913821 \n      97       98       99      100 \n9.913821 9.913821 9.913821 9.913821 \n\n$se.fit\n  [1] 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343\n  [8] 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343\n [15] 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343\n [22] 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343\n [29] 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343\n [36] 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343\n [43] 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343\n [50] 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343\n [57] 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343\n [64] 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343\n [71] 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343\n [78] 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343\n [85] 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343\n [92] 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343 0.1765343\n [99] 0.1765343 0.1765343\n\n$residual.scale\n[1] 1.765343\n```\n\n\n:::\n:::\n\n\n## Confidence Intervals Intercept-Only Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get 90% confidence intervals (Type I error = 0.1)\n  c(\n    (preds$fit+preds$se.fit*qnorm(0.05))[1],\n    (preds$fit+preds$se.fit*qnorm(0.95))[1]\n   )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        1         1 \n 9.623448 10.204195 \n```\n\n\n:::\n\n```{.r .cell-code}\n  CI.Normal=confint(model1, level=0.9)\n  CI.Normal\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      5 %      95 % \n 9.623448 10.204195 \n```\n\n\n:::\n:::\n\n\n## What is a CI?\n\n\"A confidence interval for a parameter is an interval computed using sample data by a method that will contain the parameter for a specified proportion of all samples. The success rate (proportion of all samples whose intervals contain the parameter) is known as the confidence level.\" [R. H. Lock et al. (2020)](https://www.wileyplus.com/math-and-statistics/lock-statistics-unlocking-the-power-of-data-3e-eprof20502/)\n\n## What is a CI?\n\n#### Key\n\n-   the parameter we are trying to estimate is a fixed unknown (i.e., it is not varying across samples)\n-   the endpoints of our confidence interval are random and will change every time we collect a new data set (the endpoints themselves actually have a sampling distribution!)\n\n[See more Stats4Ecologists](https://statistics4ecologists-v2.netlify.app/linreg#confidence-intervals)\n\n## What is a CI?\n\n![](/img/CI.png){fig-align=\"center\"}\n\n## Bootstrapping \n\nSee, [Stats4Ecologists](https://fw8051statistics4ecologists.netlify.app/boot.html)   \n\n<br>\n\nInstead of relying on the 95% intervals from an assumed normal distribution, we will create a distribution by \nresampling our data.\n\n## Bootstrapping (idea)\n![](/img/fieberg0.png){fig-align=\"center\" width=\"450\"}\n\n## Bootstrapping (idea)\n\n![](/img/fieberg1.png){fig-align=\"center\" width=\"450\"}\n\n## Bootstrapping (idea)\n![](/img/fieberg3.png){fig-align=\"center\" width=\"450\"}\n\n## Bootstrapping (code)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Setup\n  nboot <- 1000 # number of bootstrap samples\n  nobs <- length(y)\n  bootcoefs <- rep(NA, nboot)\n# Start loop  \nfor(i in 1:nboot){\n  set.seed(43243+i)\n  # Create bootstrap data set by sampling original observations w/ replacement  \n  bootdat <- y[sample(1:nobs, nobs, replace=TRUE)] \n  # Calculate bootstrap statistic\n  glmboot <- glm(bootdat ~ 1)\n  bootcoefs[i] <- coef(glmboot)\n}\n```\n:::\n\n## Bootstrapping (code)\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-revealjs/unnamed-chunk-12-1.png){width=960}\n:::\n:::\n\n\n## Bootstrapping (code)\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate bootstrap standard errors\n  boot.se=sd(bootcoefs)\n\n# boostrap-normal CI\n  boot.normal=c(\n        (preds$fit+boot.se*qnorm(0.05))[1],\n        (preds$fit+boot.se*qnorm(0.95))[1])\n\n# bootstrap percentile\nconfdat.boot.pct <- quantile(bootcoefs, probs = c(0.05, 0.95))\n```\n:::\n\n\n## Comparison\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n\n## Categorical Variable w/ 2 levels {.scrollable}\n$$\ny_{i} \\sim \\text{Normal}(\\mu_{i}, \\sigma^2)\\\\\n\\mu_{i} = \\beta_0+\\beta_1\\times x_i\n$$\nwhere $x_i$ is either a zero or one, indicating whether the $i^{th}$ row is from <span style=\"color:blue\">site 1</span> (*0*) or <span style=\"color:blue\">site 2</span> (*1*).\n\n<br>\n\nThis is called *Dummy Coding*.\n\n\n## Categorical Variable w/ 2 levels {.scrollable}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Setup data\n  x=as.factor(rep(c(\"Site 1\",\"Site 2\"),n/2))\n  levels(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Site 1\" \"Site 2\"\n```\n\n\n:::\n:::\n\n\n<br>\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Turn the factor into 0 and 1's\n  head(model.matrix(~x))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept) xSite 2\n1           1       0\n2           1       1\n3           1       0\n4           1       1\n5           1       0\n6           1       1\n```\n\n\n:::\n\n```{.r .cell-code}\n  x.var=model.matrix(~x)[,2]\n```\n:::\n\n\n. . .\n\n<br>\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Parameters  \n  b0=50\n  b1=-20\n  mu=b0+b1*x.var\n\n# Sample Data\n  set.seed(43243)\n  y=rnorm(n,mean=mu,sd=4)\n```\n:::\n\n\n<br>\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#fit the model \n  model2=glm(y~x)\n  model2.1=glm(y~x.var)\n\n#comparison  \n  rbind(coef(model2), coef(model2.1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     (Intercept)  xSite 2\n[1,]    50.51109 -21.3669\n[2,]    50.51109 -21.3669\n```\n\n\n:::\n:::\n\n\n## Side-Bar: Maximum Likelihood Optimization {.scrollable}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Here is our negative log-likelihood function with three\n#parameters - the mean (2) and stdev (1)\nneg.log.like = function(par) {\n  mu=par[1]+par[2]*x.var\n  sum(-dnorm(y,mean = mu,sd = par[3],log = TRUE))\n}\n```\n:::\n\n\n<br>\n\n. . .\n\n### Use our function in an optimization function\n\n::: {.cell}\n\n```{.r .cell-code}\n#use optim with initial values and define the lower and upper limits of the possible values\nfit1 <- optim(\n    par = c(0, 0,1),\n    fn = neg.log.like,\n    method = \"L-BFGS-B\",\n    lower = c(-100, -100, 0.01),\n    upper = c(400, 400, 100)\n  )\n\nfit1$par\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  50.511067 -21.366842   3.445873\n```\n\n\n:::\n:::\n\n\n<br>\n\n## Categorical Variable w/ 2 levels {.scrollable}\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = y ~ x)\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  50.5111     0.4923  102.61   <2e-16 ***\nxSite 2     -21.3669     0.6962  -30.69   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 12.11631)\n\n    Null deviance: 12601.0  on 99  degrees of freedom\nResidual deviance:  1187.4  on 98  degrees of freedom\nAIC: 537.22\n\nNumber of Fisher Scoring iterations: 2\n```\n\n\n:::\n:::\n\n\n## Relevel {.scrollable}\n\nWe can manipulate the factor levels of $x$ to indicate that <span style=\"color:blue\">site 1</span> is denoted by a **1** now and <span style=\"color:blue\">site 2</span> is denoted by a **0**.\n\n. . .\n\n<br>\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#change intercept meaning\n  x.relev=relevel(x,ref=\"Site 2\")\n  levels(x.relev)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Site 2\" \"Site 1\"\n```\n\n\n:::\n:::\n\n\n. . .\n\n<br>\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#fit the model again\n  model2.2=glm(y~x.relev)\n\n#Look at coefs  \n  rbind(coef(model2),coef(model2.1),coef(model2.2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     (Intercept)  xSite 2\n[1,]    50.51109 -21.3669\n[2,]    50.51109 -21.3669\n[3,]    29.14419  21.3669\n```\n\n\n:::\n:::\n\n\n. . .\n\n<br>\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#compare predictions  \n  rbind(predict(model2)[1:2],  \n        predict(model2.1)[1:2],\n        predict(model2.2)[1:2]  \n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            1        2\n[1,] 50.51109 29.14419\n[2,] 50.51109 29.14419\n[3,] 50.51109 29.14419\n```\n\n\n:::\n:::\n\n\n## Categorical Variable w/ 4 levels \n### Dummy Coding\n$$\ny_{i} \\sim \\text{Normal}(\\mu_{i}, \\sigma^2)\\\\\n\\mu_{i} = \\beta_0+(\\beta_1\\times x_{1,i}) + (\\beta_2\\times x_{2,i}) + (\\beta_3\\times x_{3,i})\n$$\n$x_{1,i} =$ indicator of <span style=\"color:blue\">site 2</span> (1) or not (0)\n\n$x_{2,i} =$ indicator of <span style=\"color:blue\">site 3</span> (1) or not (0)\n\n$x_{3,i} =$ indicator of <span style=\"color:blue\">site 4</span> (1) or not (0)\n\n\n\n## Categorical Variable w/ 4 levels {.scrollable}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Setup Data\n  x=as.factor(rep(c(\"Site 1\",\"Site 2\",\"Site 3\", \"Site 4\"),n/4))\n  levels(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Site 1\" \"Site 2\" \"Site 3\" \"Site 4\"\n```\n\n\n:::\n\n```{.r .cell-code}\n#Convert factors to 0 and 1's\n  head(model.matrix(~x))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept) xSite 2 xSite 3 xSite 4\n1           1       0       0       0\n2           1       1       0       0\n3           1       0       1       0\n4           1       0       0       1\n5           1       0       0       0\n6           1       1       0       0\n```\n\n\n:::\n\n```{.r .cell-code}\n  x.var=model.matrix(~x)[,2:4]\n```\n:::\n\n\n<br>\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set Parameters  \n  b0=50 #Site 1\n  b1=-20 #Diff of site 2 to site 1\n  b2=-200 #Diff of site 3 to site 1\n  b3=100 #Diff of site 4 to site 1\n  \n# Mean  \n  mu = b0+b1*x.var[,1]+b2*x.var[,2]+b3*x.var[,3]\n\n#True mean group-level values\n  unique(mu)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]   50   30 -150  150\n```\n\n\n:::\n\n```{.r .cell-code}\n#Grand Mean\n  mean(unique(mu))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 20\n```\n\n\n:::\n:::\n\n\n<br>\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate Data  \n  set.seed(43243)\n  y=rnorm(n,mean=mu,sd=4)\n```\n:::\n\n\n<br>\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# fit the model\n  model3=glm(y~x)\n  model3.1=glm(y~x.var)\n\n# Compare coefs    \n  rbind(coef(model3), coef(model3.1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     (Intercept)   xSite 2   xSite 3  xSite 4\n[1,]    50.59576 -21.91446 -200.1693 99.01133\n[2,]    50.59576 -21.91446 -200.1693 99.01133\n```\n\n\n:::\n:::\n\n\n## Effect Coding w/ 4 levels {.scrollable}\n\n[Effect Coding Link](https://stats.stackexchange.com/questions/52132/how-to-do-regression-with-effect-coding-instead-of-dummy-coding-in-r)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Use effect coding to make the intercept the grand mean\n  model3.2=glm(y~x,contrasts = list(x = contr.sum))\n\n\n# Intercept = grand mean of group-means\n# Coef 1 = effect difference of Site 1 from Grand Mean\n# Coef 2 = effect difference of Site 2 from Grand Mean\n# Coef 3 = effect difference of Site 3 from Grand Mean\n\n  coef(model3.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)          x1          x2          x3 \n  19.827643   30.768114    8.853654 -169.401216 \n```\n\n\n:::\n:::\n\n\n<br>\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#The coefficient for site-level 4 (difference from the grand mean)\n  sum(coef(model3.2)[-1]*(-1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 129.7794\n```\n\n\n:::\n:::\n\n\n<br>\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Predict the values and compare them to the true means for\n#each site\n  rbind(unique(mu),\n  predict(model3.2)[1:4])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            1       2         3        4\n[1,] 50.00000 30.0000 -150.0000 150.0000\n[2,] 50.59576 28.6813 -149.5736 149.6071\n```\n\n\n:::\n:::\n\n\n## Continuous Variable\n\nADD here and mean-centering\n\n::: {.cell}\n\n```{.r .cell-code}\n#lm(Temperature ~ I(Chirps - mean(Chirps)), data = CricketChirps)\n```\n:::\n\n\n\n## Additive Model\n\n#### Categorical (2 levels) and Continuous Variable\n\n$$\ny_{i} \\sim \\text{Normal}(\\mu_{i}, \\sigma^2)\\\\\n\\mu_{i} = \\beta_0+(\\beta_1\\times x_{1,i}) + (\\beta_2\\times x_{2,i})\n$$\n$x_{1,i} =$ indicator of <span style=\"color:blue\">site 2</span> (1) or not (0)\n\n$x_{2,i} =$ is a continuous numeric value\n\n\n## Additive Model {.scrollable}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#A continuous and categorical variable \n  x=as.factor(rep(c(\"Site 1\",\"Site 2\"),n/2))\n  levels(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Site 1\" \"Site 2\"\n```\n\n\n:::\n\n```{.r .cell-code}\n  x.var=model.matrix(~x)[,2]\n```\n:::\n\n\n<br>\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Simulate x2 variable\n  set.seed(54334)\n  x2=rpois(n,100)\n\n#Parameters\n  b0=50\n  b1=-50\n  b2=4\n\n#Mean  \n  mu=b0+b1*x.var+b2*x2\n\n#Simualte Date  \n  set.seed(43243)\n  y=rnorm(n,mean=mu,sd=50)\n\n# fit the model\n  model4=glm(y~x+x2)\n\n  coef(model4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)     xSite 2          x2 \n  96.615158  -66.748462    3.597896 \n```\n\n\n:::\n:::\n\n\n<br>\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Confidence intervals of coefs\n  confint(model4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                 2.5 %     97.5 %\n(Intercept)  14.604365 178.625952\nxSite 2     -83.822719 -49.674206\nx2            2.787034   4.408758\n```\n\n\n:::\n:::\n\n\n. . .\n\n<br>\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Summary  \n  summary(model4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = y ~ x + x2)\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  96.6152    41.8430   2.309   0.0231 *  \nxSite 2     -66.7485     8.7115  -7.662 1.39e-11 ***\nx2            3.5979     0.4137   8.697 8.70e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 1894.243)\n\n    Null deviance: 428531  on 99  degrees of freedom\nResidual deviance: 183742  on 97  degrees of freedom\nAIC: 1043.4\n\nNumber of Fisher Scoring iterations: 2\n```\n\n\n:::\n:::\n\n\n. . .\n\n<br>\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fitted Values\n  newdata=expand.grid(x,x2)\n  head(newdata)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    Var1 Var2\n1 Site 1  112\n2 Site 2  112\n3 Site 1  112\n4 Site 2  112\n5 Site 1  112\n6 Site 2  112\n```\n\n\n:::\n\n```{.r .cell-code}\n  colnames(newdata)=c(\"x\",\"x2\")\n\n\n  preds=predict(model4,newdata=newdata,type=\"response\",\n              se.fit = TRUE)\n```\n:::\n\n\n## Additive Model Plot 1\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sjPlot)\nplot_model(model4, type = \"pred\", terms = c(\"x\"))\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-revealjs/unnamed-chunk-38-1.png){width=960}\n:::\n:::\n\n\n## Additive Model Plot 2\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_model(model4, type = \"pred\", terms = c(\"x2\",\"x\"))\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-revealjs/unnamed-chunk-39-1.png){width=960}\n:::\n:::\n\n\n## Interaction Model\n\n#### Categorical (2 levels) and Continuous Variable\n\n$$\ny_{i} \\sim \\text{Normal}(\\mu_{i}, \\sigma^2)\\\\\n\\mu_{i} = \\beta_0+(\\beta_1\\times x_{1,i}) + (\\beta_2\\times x_{2,i}) + (\\beta_3*(x_{1,i}\\times x_{2,i}))\n$$\n$x_{1,i} =$ indicator of <span style=\"color:blue\">site 2</span> (1) or not (0)\n\n$x_{2,i} =$ is a numeric value\n\n$x_{1,i} \\times x_{2,i}=$ is zero for <span style=\"color:blue\">site 1</span> values and the numeric value for site 2 values\n\n\n\n## Interaction Model {.scrollable}\n\n### Categorical (2 levels) and Continuous Variable\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate Variables\n  x=as.factor(rep(c(\"Site 1\",\"Site 2\"),n/2))\n  levels(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Site 1\" \"Site 2\"\n```\n\n\n:::\n\n```{.r .cell-code}\n  x.var=model.matrix(~x)[,2]\n\n  set.seed(5453)\n  x2=rpois(n,100)\n\n# Parameters \n  b0=50\n  b1=-50\n  b2=4\n  b3=-20\n\n# Mean  \n  mu = b0+b1*x.var+b2*x2+b3*(x.var*x2)\n\n#Simulate Data\n  set.seed(43243)\n  y=rnorm(n,mean=mu,sd=10)\n\n# fit the model\n  model5=glm(y~x2*x)\n  model5.1=glm(y~x+x2+x:x2)\n\n#comparison  \n  rbind(coef(model5),coef(model5.1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     (Intercept)         x2    xSite 2 x2:xSite 2\n[1,]    47.51796   4.037598 -60.864156  -19.92472\n[2,]    47.51796 -60.864156   4.037598  -19.92472\n```\n\n\n:::\n:::\n\n\n. . .\n\n<br>\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Confidence intervals of coefs\n  confint(model5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                 2.5 %     97.5 %\n(Intercept)  23.600505  71.435410\nx2            3.799657   4.275539\nxSite 2     -95.499112 -26.229199\nx2:xSite 2  -20.270675 -19.578762\n```\n\n\n:::\n:::\n\n\n## Interaction Model\n\n::: {.cell}\n\n```{.r .cell-code}\n#Summary  \n  summary(model5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = y ~ x2 * x)\n\nCoefficients:\n            Estimate Std. Error  t value Pr(>|t|)    \n(Intercept)  47.5180    12.2030    3.894 0.000182 ***\nx2            4.0376     0.1214   33.258  < 2e-16 ***\nxSite 2     -60.8642    17.6712   -3.444 0.000850 ***\nx2:xSite 2  -19.9247     0.1765 -112.881  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 76.60874)\n\n    Null deviance: 1.0550e+08  on 99  degrees of freedom\nResidual deviance: 7.3544e+03  on 96  degrees of freedom\nAIC: 723.58\n\nNumber of Fisher Scoring iterations: 2\n```\n\n\n:::\n:::\n\n\n## Interaction Model Plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntheme_set(theme_sjplot())\nplot_model(model5, type = \"pred\", terms = c(\"x2\",\"x\"))\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-revealjs/unnamed-chunk-43-1.png){width=960}\n:::\n:::\n\n\n## Evaluating Assumptions\n### Largely done based on the residuals\n\n$y_{i} - \\hat{y}_{i}$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhist(model5$residuals)\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-revealjs/unnamed-chunk-44-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## [Linearity & Variance Assumption](https://fw8051statistics4ecologists.netlify.app/linreg.html#exploring-assumptions-using-r)\n\n![](/img/assumptions.png){fig-align=\"center\" width=\"1000\"}\n\n\n## Normality Assumption {.scrollable}\n\nTop-left: Normal Q-Q plot. Quantiles of the standardized residuals versus quantiles of a standard normal distribution. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npar(mfrow=c(2,2))\nplot(model5)\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-revealjs/unnamed-chunk-45-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Notes {.scrollable }\n\nThe scale-location plot is very similar to residuals vs fitted, but simplifies analysis of the homoskedasticity assumption. It takes the square root of the absolute value of standardized residuals instead of plotting the residuals themselves.\n\nLeverage refers to the extent to which the coefficients in the regression model would change if a particular observation was removed from the dataset.\n\n## Exploring Assumptions {.scrollable}\n\nNicer looking Plots\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggResidpanel)\nresid_panel(model5)\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-revealjs/unnamed-chunk-46-1.png){width=960}\n:::\n:::\n",
    "supporting": [
      "regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}