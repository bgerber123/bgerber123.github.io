{
  "hash": "8a79c8b8cdf318c18a772fae396537af",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: <span style=\"color:orange\">Probability</span>\ntitle-slide-attributes:\n    data-background-image: /img/chalkboard.png\nformat:\n  revealjs:\n    chalkboard: true\n    multiplex: true\n---\n\n\n\n\n## Objectives\n\n<!-- knitr::purl(\"./FW680A4/probability.qmd\", output=\"./FW680A4/probability.R\") -->\n\n- Connect random variables, probabilities, and parameters\n\n- define prob. functions\n\n  - discrete and continuous random variables\n  \n- use/plot prob. functions\n\n- learn some notation\n\n## Probability/Statistics\n\n\n```{=html}\n<style type=\"text/css\">\n\nbody, td {\n   font-size: 14px;\n}\ncode.r{\n  font-size: 30px;\n}\npre {\n  font-size: 20px\n}\n</style>\n```\n\n\n\n<br>\n\n<span style=\"color:red\">Probability</span> and <span style=\"color:blue\">statistics</span> are the opposite sides of the same coin.\n\n<br>\n\n. . .\n\nTo understand <span style=\"color:blue\">statistics</span>, we need to understand <span style=\"color:red\">probability</span> and <span style=\"color:red\">probability functions</span>. \n\n\n<br>\n\n. . .\n\nThe two key things to understand this connection is the <span style=\"color:orange\">random variable (RV)</span> and <span style=\"color:orange\">parameters</span> (e.g., $\\theta$, $\\sigma$, $\\epsilon$, $\\mu$).\n\n## Motivation {.fit-text}\n<p style=\"text-align:center\">\nWhy learn about RVs and probability math?\n</p>\n\n. . .\n\n\n<span style=\"color:red\">Foundations of:</span> \n\n-   linear regression\n-   generalized linear models\n-   mixed models\n\n. . .\n\n<span style=\"color:red\">Our Goal:</span> \n\n- conceptual framework to think about *data*, *probabilities*, and *parameters*\n- mathematical connections and notation\n\n## Not Random Variables\n\n$$\n\\begin{align*} \na =& 10 \\\\\nb =& \\text{log}(a) \\times 12 \\\\\nc =& \\frac{a}{b} \\\\\ny =& \\beta_0 + \\beta_1*c\n\\end{align*}\n$$\n\n. . .\n\nAll variables here are <span style=\"color:red\">scalars</span>. They are what they are and that is it.  $\\beta$ variables and $y$ are currently unknown, but still <span style=\"color:red\">scalars</span>. \n\n. . .\n\n<p style=\"text-align:center\">\n<span style=\"color:red\">Scalars</span> are quantities that are fully described by a magnitude (or numerical value) alone.\n</p>\n\n## Random Variables {.scrollable}\n\n$$\ny \\sim f(y)\n$$\n\n$y$ is a random variable which may change values each observation; it changes based on a probability function, $f(y)$. \n\n<br>\n\n. . .\n\n\nThe tilde ($\\sim$) denotes \"has the probability distribution of\".\n\n<br>\n\n. . .\n\n\nWhich value (y) is observed is predictable. Need to know *parameters* ($\\theta$) of the probability function $f(y)$.\n\n<br>\n\n. . .\n\nSpecifically, $f(y|\\theta)$, where '|' is read as 'given'. \n\n. . .\n\n<p style=\"text-align:center\">\n<span style=\"color:blue\">Toss of a coin</span> <br>\n<span style=\"color:blue\">Roll of a die</span> <br>\n<span style=\"color:blue\">Weight of a captured elk</span> <br>\n<span style=\"color:blue\">Count of plants in a sampled plot</span> <br>\n</p>\n\n<br>\n\n. . .\n\n<p style=\"text-align:center\"> The values observed can be understand based on the frequency within the <span style=\"color:red\">population</span> or presumed <span style=\"color:red\">super-population</span>. These frequencies can be described by probabilities. </p>\n\n## Frequency / Probabilitities {.scrollable}\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"NA\"}\npar(mfrow=c(1,2))\nhist(y, breaks=20,xlim=c(0,25),main=main)\nhist(y, breaks=20,xlim=c(0,25),freq = FALSE,main=main)\n```\n\n::: {.cell-output-display}\n![](Probability_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n. . . \n\nWe often only get to see **ONE** sample from this distribution.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Probability_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n\n## Random Variables \n\nWe are often interested in the characteristics of the whole population of frequencies,\n\n- <span style=\"color:blue\">central tendency</span> (mean, mode, median)\n- <span style=\"color:blue\">variability</span>  (var, sd)\n- <span style=\"color:blue\">proportion of the population that meets some condition</span><br> P($8 \\leq y \\leq$ 12) =0.68\n\n. . .\n\n\nWe infer what these are based on our sample (i.e., statistical inference). \n\n## Philosophy\n\n<span style=\"color:blue\">Frequentist Paradigm:</span>  \n\nData (e.g., $y$) are random variables that can be described by probability distributions with unknown parameters that (e.g., $\\theta$) are *fixed* (scalars). \n\n<br>\n\n. . .\n\n<span style=\"color:blue\">Bayesian Paradigm:</span>  \n\nData (e.g., $y$) are random variables that can be described by probability functions where the unknown parameters (e.g., $\\theta$) are also random variables that have probability functions that describe them.\n\n## Random Variables \n$$\n\\begin{align*}\ny =& \\text{ event/outcome} \\\\\nf(y|\\boldsymbol{\\theta}) =& [y|\\boldsymbol{\\theta}]=  \\text{ process governing the value of } y \\\\\n\\boldsymbol{\\theta} =& \\text{ parameters} \\\\\n\\end{align*}\n$$\n\n. . .\n\n$f()$ or [ ] is conveying a function (math). \n\n. . .\n\nIt is called a PDF when $y$ is continuous and a PMF when $y$ is discrete.\n\n-  PDF: **probability density function** \n-  PMF: **probability mass function** \n\n\n## Functions\nWe commonly use *deterministic* functions (indicated by non-italic letter); e.g., log(), exp(). Output is always the same with the same input.\n$$ \n\\hspace{-12pt}\\text{g} \\\\\nx \\Longrightarrow\\fbox{DO STUFF\n} \\Longrightarrow \\text{g}(x)\n$$ \n\n. . .\n\n$$ \n\\hspace{-14pt}\\text{g} \\\\\nx \\Longrightarrow\\fbox{+7\n} \\Longrightarrow \\text{g}(x)\n$$ \n\n. . .\n\n$$ \n\\text{g}(x) = x + 7\n$$ \n\n. . .\n\n\n## Random Variables\n\n<span style=\"color:red\">Probability</span>: Interested in $y$, the data, and the probability function that \"generates\" the data.\n$$\n\\begin{align*}\ny \\leftarrow& f(y|\\boldsymbol{\\theta}) \\\\\n\\end{align*}\n$$\n\n. . .\n\n<span style=\"color:blue\">Statistics</span>: Interested in population characteristics of $y$; i.e., the parameters,\n\n$$\n\\begin{align*}\ny \\rightarrow& f(y|\\boldsymbol{\\theta}) \\\\\n\\end{align*}\n$$\n\n\n## Probability Functions {.scrollable}\n\nSpecial functions with rules to guarantee our logic of probabilities are maintained. \n\n. . .\n\n### Discrete RVs {.numbered}\n\n$y$ can only be a certain <span style=\"color:red\">set</span> of values. \n\n::: {.incremental}\n\n1.   $y \\in  \\{0,1\\}$\n      - 0 = dead, 1 = alive\n2.   $y \\in  \\{0,1, 2\\}$ \n      - 0 = site unoccupied, 1 = site occupied w/o young, 2 = site occupied with young\n3.   $y \\in  \\{0, 1, 2, ..., 15\\}$ \n      - count of pups in a litter; max could by physiological constraint\n      \n:::\n\n::: {.fragment}\n\nThese sets are called the <span style=\"color:blue\">sample space</span> ($\\Omega$) or the <span style=\"color:blue\">support</span> of the RV.\n\n:::\n\n## PMF {.scrollable}\n\n$$\nf(y) = P(Y=y)\n$$\n\n. . .\n\nData has two outcomes (0 = dead, 1 = alive) \n\n$y \\in  \\{0,1\\}$\n\n. . .\n\nThere are two probabilities\n\n-   $f(0) = P(Y=0)$\n-   $f(1) = P(Y=1)$\n\n. . .\n\n<span style=\"color:blue\">Axiom 1:</span> The probability of an event is greater than or equal to zero and less than or equal to 1.\n\n\n$$\n0 \\leq f(y) \\leq 1\n$$\nExample,\n\n-   $f(0) = 0.1$\n-   $f(1) = 0.9$ \n\n. . .\n\n<span style=\"color:blue\">Axiom 2:</span> The sum of the probabilities of all possible values (sample space) is one. \n\n. . .\n\n$$\n\\sum_{i} f(y_i) = f(y_1) + f(y_2) + ... = P(\\Omega) =1\n$$\nExample, \n\n-   $f(0) + f(1) = 0.1 + 0.9 = 1$\n\n\n## PMF\n\nStill need to define $f()$, our PMF for $y \\in  \\{0,1\\}$\n\n\n. . .\n\nThe [Bernoulli distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution)\n\n$$\nf(y|\\theta) = [y|\\theta]=\n  \\begin{align}\n        \\theta^{y}\\times(1-\\theta)^{1-y}\n  \\end{align}\n$$\n\n::: {.fragment}\n$\\theta$ = P(Y = 1) = 0.2 \n:::\n\n::: {.fragment}\n$$\nf(y|\\theta) = [y|\\theta]=\n  \\begin{align}\n        = 0.2^{1}\\times(1-0.2)^{0-0} \n  \\end{align}\n$$\n:::\n\n\n::: {.fragment}\n$$\nf(y|\\theta) = [y|\\theta]=\n  \\begin{align}\n        = 0.2 \\times (0.8)^{0} = 0.2\n  \\end{align}\n$$\n:::\n\n\n## Bernoulli PMF\n\n$$\nf(y|\\theta) = [y|\\theta]=\n  \\begin{align}\n        \\theta^{y}\\times(1-\\theta)^{1-y}\n  \\end{align}\n$$\n\n\nSample space support ($\\Omega$):\n\n-   $y \\in  \\{0,1\\}$\n\n\n::: {.fragment}\nParameter space support ($\\Theta$):\n\n-   $\\theta \\in  [0,1]$\n-   General: $\\theta \\in \\Theta$\n\n::: \n\n## Bernoulli PMF (Code)\n\nWhat would our data look like for 10 ducks that had a probability of survival (Y=1) of 0.20?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Define inputs\n  theta=0.2;  N=1 \n\n#Random sample - 1 duck\n  rbinom(n=1,size=N,theta)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n\n```{.r .cell-code}\n#Random sample - 10 ducks\n  rbinom(n=10,size=N,theta)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 1 0 0 0 1 0 1 0 1 0\n```\n\n\n:::\n:::\n\n\n## **Why is this useful to us?**\n\nHow about to evaluate the sample size of ducks needed to estimate $\\theta$?\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny.mat = replicate(1000,rbinom(n = 10,size=N,theta))\ntheta.hat = apply(y.mat, 2, mean)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](Probability_files/figure-revealjs/bern3-1.png){width=75%}\n:::\n:::\n\n\n\n\n## Binomial PMF\n\nThe Bernoulli is a special case of the [Binomial Distribution](https://en.wikipedia.org/wiki/Binomial_distribution). \n\n$$\nf(y|\\theta) = [y|\\theta]=\n  \\begin{align}\n        {N\\choose y} \\theta^{y}\\times(1-\\theta)^{N-y}\n  \\end{align}\n$$\n\n::: {.fragment}\n\n$N$ = total trials / tagged and released animals\n\n:::\n\n<br>\n\n::: {.fragment}\n\n$y$ = number of successes / number of alive animals at the of the study.\n\n:::\n\n## Binomial PMF (Code) {.scrollable}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1 duck tagged/released and one simulation\n  theta=0.2;  N=1 \n  rbinom(n=1,size=N,theta)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1\n```\n\n\n:::\n:::\n\n\n\n\n::: {.fragment}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1000 ducks tagged/released and one simulation\n  theta=0.2;  N=1000 \n  rbinom(n=1,size=N,theta)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 198\n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.fragment}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1000 ducks tagged/released and 10 simulation\n  theta=0.2;  N=1000 \n  rbinom(n=10,size=N,theta)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 180 190 198 192 169 192 192 217 206 216\n```\n\n\n:::\n:::\n\n\n:::\n\n\n::: {.fragment}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1 duck tagged for each of 1000 simulations\n  theta=0.2;  N=1\n  y = rbinom(n=1000,size=N,theta)\n  y\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   [1] 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0\n  [38] 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0\n  [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [112] 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0\n [149] 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0\n [186] 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0\n [223] 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0\n [260] 0 1 1 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n [297] 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0\n [334] 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n [371] 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1\n [408] 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0\n [445] 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0\n [482] 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n [519] 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0 1 0 0\n [556] 1 0 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0\n [593] 1 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0\n [630] 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1\n [667] 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0\n [704] 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n [741] 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1\n [778] 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1\n [815] 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0\n [852] 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0\n [889] 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0 0 0 0\n [926] 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n [963] 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0\n[1000] 0\n```\n\n\n:::\n:::\n\n\n:::\n\n\n::: {.fragment}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 186\n```\n\n\n:::\n:::\n\n\n\n:::\n\n\n\n\n## Support \n\nUse a probability function that makes sense for your data/RV. In Bayesian infernece, we also pick prob. functions that make sense for parameters.\n\n<br>\n\nThe sample space and parameter support can be found on Wikipedia for many probability functions. \n\n\n\n## Normal PDF \n\nFor example, the [Normal/Gaussian distribution ](https://en.wikipedia.org/wiki/Normal_distribution) describes the sample space for all values on the real number line. \n\n$$y \\sim \\text{Normal}(\\mu, \\sigma) \\\\ y \\in (-\\infty, \\infty) \\\\ y \\in \\mathbb{R}$$\n\nWhat is the parameter space for $\\mu$ and $\\sigma$?\n\n## Normal Distribution {.scrollable}\n\nWe collect data on adult alligator lengths (in). \n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n [1]  90.30  83.02 103.67  85.17  99.20 106.74  90.76 105.28  99.41 101.72\n```\n\n\n:::\n:::\n\n\n<br>\n\n<p style=\"text-align:center\">\n<span style=\"color:blue\">\nShould we use the Normal Distribution <br>to estimate the mean? <br>\n</span>\n</p>\n\n. . .\n\n<p style=\"text-align:center\">\nDoes the support of our data match <br>the support of the PDF?\n</p>\n\n. . .\n\n<p style=\"text-align:center\">\nWhat PDF does?\n</p>\n\n. . .\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Probability_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\n<p style=\"text-align:center\"> <span style=\"color:blue\">Are they exactly the same?</span></p>\n\n<br>\n\n. . .\n\nThe issue is when the data are near 0, we might estimate non-sensical values (e.g. negative).\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Probability_files/figure-revealjs/unnamed-chunk-11-1.png){width=960}\n:::\n:::\n\n\n\n## PDF\n\n### Continuous RVs {.numbered}\n\n$y$ are an uncountable set of values.\n\n<br>\n\n::: {.fragment}\nProvide ecological data examples that match the support?\n:::\n\n::: incremental \n\n1. <span style=\"color:blue\">Gamma</span>:  $y \\in  (0,\\infty)$\n2. <span style=\"color:blue\">Beta</span>:   $y \\in  (0,1)$ \n3. <span style=\"color:blue\">Continuous Uniform</span>:   $y \\in  [a,b]$\n\n:::\n\n\n## PDF {.scrollable}\n\nPDFs of continious RVs follow the same rules as PMFs.\n\n#### Confusing Differences\n\n::: {.fragment}\n\n<span style=\"color:blue\">Axiom 1:</span>\n\n-   $f(y) \\geq 0$\n\n\nPDFs output <span style=\"color:orange\">probability densities</span>, not probabilities.\n\n:::\n\n::: {.fragment}\n\n<br>\n\n<span style=\"color:blue\">Axiom 2:</span>\n\n-   Probs are the area b/w a lower and upper value of $y$; i.e, area under the curve\n\n:::\n\n\n::: {.fragment}\n\n$$\ny \\sim \\text{Normal}(\\mu, \\sigma) \\\\\nf(y|\\mu,\\sigma ) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{\\frac{1}{2}(\\frac{y-\\mu}{\\sigma})^{2}} \\\\\n$$\n\n:::\n\n::: {.fragment}\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nvisualize.it(dist = 'norm', stat = c(100),\n             list(mu = 100 , sd = 10), section = \"upper\")\n```\n\n::: {.cell-output-display}\n![](Probability_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\n:::\n\n\n::: {.fragment}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Probability_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n:::\n\n\n::: {.fragment}\n\nThe math,\n\n$$\n\\int_{120}^{\\infty} f(y| \\mu, \\sigma)dy = P(120<Y<\\infty)\n$$\n:::\n\n\n::: {.fragment}\n\nRead this as \"the integral of the probability density function between 120 and infinity (on the left-hand side) is equal to the probability that the outcome of the random variable is between 120 and infinity (on the right-hand side)\".\n\n:::\n\n<br>\n\n::: {.fragment}\n\nThe code\n\n::: {.cell}\n\n```{.r .cell-code}\npnorm(120,mean=100,sd=10,lower.tail = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.02275013\n```\n\n\n:::\n:::\n\n\n:::\n\n<br>\n\n::: {.fragment}\n\nOr, we could reverse the question. \n\n::: {.cell}\n\n```{.r .cell-code}\nqnorm(0.02275,100,10,lower.tail = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 120\n```\n\n\n:::\n:::\n\n\n::: \n\n<!-- How do we use quantiles/probabilities to justify 95\\% confidencen intervals? -->\n\n<!-- ```{r,echo=FALSE,eval=TRUE} -->\n<!-- visualize.it(dist = 'norm', stat = c(-1.96,1.96), -->\n<!--              list(mu = 0 , sd = 1), section = \"tails\") -->\n<!-- ``` -->\n<!-- . . . -->\n\n<!-- Code as, -->\n\n<!-- ```{r, echo=TRUE} -->\n\n<!-- 0.975-0.025 -->\n\n<!-- qnorm(0.025) -->\n\n<!-- qnorm(0.975) -->\n<!-- ``` -->\n\n\n<!-- . . . -->\n\n<!-- What is the probability of $y$ = 80?  -->\n\n<!-- . . . -->\n\n<!-- $$ -->\n<!-- \\begin{align*} -->\n<!-- &\\int_{80}^{80} f(y|\\mu=100, \\sigma=10) dy\\\\ -->\n<!-- \\end{align*} -->\n<!-- $$ -->\n<!-- . . . -->\n\n<!-- A definite integral of a continuous function is the limit of a Riemann sum as the number of subdivisions (n) approaches infinity.  -->\n\n<!-- $$ -->\n<!-- \\begin{align*} -->\n<!-- &= \\lim_{n \\to \\infty} \\sum_{i=1}^{n} \\Delta y_i \\times f(y_{i}|\\mu=100, \\sigma=10) = 0\\\\ -->\n<!-- &\\text{where } \\Delta y = \\frac{80-80}{n} -->\n<!-- \\end{align*} -->\n<!-- $$ -->\n\n\n## PDF {.scrollable}\n\n<span style=\"color:blue\">Axiom 3:</span>\n\n-   $\\int_{\\text{lower support}}^{\\text{upper suppport}}f(y)dy = 1$\n\nThe sum of the probability densities of all possible outcomes is equal to 1.\n\n\n## Normal Distribution (PDF Code)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny = rnorm(1000, mean = 20, sd = 3)\nhist(y,freq=FALSE,ylim=c(0,0.14))\nlines(density(y),lwd=3,col=4)\n```\n\n::: {.cell-output-display}\n![](Probability_files/figure-revealjs/pdf1-1.png){width=960}\n:::\n:::\n\n\n## Normal Distribution (PDF Code)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncurve(dnorm(x, mean= 20, sd = 3),\n      xlim=c(0,40),lwd=3,col=2,ylab=\"Probability Density\",xlab=\"y\")\nabline(v=20, lwd=3, col=1, lty=4)\n```\n\n::: {.cell-output-display}\n![](Probability_files/figure-revealjs/pdf1b-1.png){width=960}\n:::\n:::\n\n\n## Normal Distribution (PDF Code)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncurve(dnorm(x, mean = 10, sd = 3),xlim=c(0,40),lwd=4,col=3,add=TRUE)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](Probability_files/figure-revealjs/pdf2b-1.png){width=960}\n:::\n:::\n\n\n## Moments\n\nProperties of all probability functions.\n\n::: {.fragment}\n\n- 1${^{st}}$ moment is central tendency\n- 2${^{nd}}$ moment is the dispersion\n-  ...\n\n:::\n\n::: {.fragment}\n**Normal Distribution**: parameters ($\\mu$ and $\\sigma$) are 1${^{st}}$ and 2${^{nd}}$ moments\n:::\n\n\n## Moments {.scrollable}\n\n[**Gamma Distribution**](https://en.wikipedia.org/wiki/Gamma_distribution): parameters are not moments\n\n#### Parameters \n\nShape = $\\alpha$, Rate = $\\beta$ \n\n\n**OR** \n\nShape = $\\kappa$, Scale = $\\theta$, where $\\theta = \\frac{1}{\\beta}$\n\n::: {.fragment}\n**NOTE**: probability functions can have *Alternative Parameterizations*, such they have different parameters.\n:::\n\n::: {.fragment}\nMoments are functions of these parameters: \n\n- mean = $\\kappa\\theta$ or $\\frac{\\alpha}{\\beta}$\n\n- var = $\\kappa\\theta^2$ or $\\frac{\\alpha}{\\beta^2}$\n\n\n:::\n\n\n## Gamma Distribution {.scrollable}\n\n<span style=\"color:red\">Probability</span>:\nInterested in the variation of y,\n$$\n\\begin{align*}\ny \\leftarrow& f(y|\\boldsymbol{\\theta'}) \\\\\n\\end{align*}\n$$\n\n. . .\n\n\n$$\n\\begin{align*}\n\\boldsymbol{\\theta'}  =& \\begin{matrix} [\\kappa & \\theta] \\end{matrix} \\\\\nf(y|\\boldsymbol{\\theta}') &= \\text{Gamma(}\\kappa, \\theta) \\\\\n\\end{align*}\n$$\n\n. . .\n\n\n$$\n\\begin{align*}\nf(y|\\boldsymbol{\\theta}') &= \\frac{1}{\\Gamma(\\kappa)\\theta^{\\kappa}}y^{\\kappa-1} e^{-y/\\theta} \\\\\n\\end{align*}\n$$\n\n. . .\n\n\n[Sample/parameter Support:](https://en.wikipedia.org/wiki/Log-normal_distribution)\n\n-   $y \\in (0,\\infty)$\n-   $\\kappa \\in (0,\\infty)$\n-   $\\theta \\in (0,\\infty)$\n\n\n## Gamma Distribution (PDF Code)\n\n[Gamma Wikipedia](https://en.wikipedia.org/wiki/Gamma_distribution)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshape =10\nscale = 2\n\nmean1 = shape*scale\nmean1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 20\n```\n\n\n:::\n\n```{.r .cell-code}\nmode1 = (shape-1)*scale\nmode1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 18\n```\n\n\n:::\n\n```{.r .cell-code}\nstdev = sqrt(shape*scale^2)\nstdev\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6.324555\n```\n\n\n:::\n:::\n\n\n## Gamma Distribution (PDF Code)\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Probability_files/figure-revealjs/pdf4-1.png){width=960}\n:::\n:::\n\n\n\n## Gamma Distribution {.scrollable}\n\n\n\n\nWhat is the probability we would sample a value >40? <br>\nIn this population, how common is a value >40?\n\n. . .\n\n\n$$\n\\begin{align*}\np(y>40) = \\int_{40}^{\\infty} f(y|\\boldsymbol{\\theta}) \\,dy \n\\end{align*}\n$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npgamma(q=40, shape=10, scale=2,lower.tail=FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.004995412\n```\n\n\n:::\n:::\n\n\n<br>\n\n. . .\n\n\nWhat is the probability of observing $y$ < 20\n\n\n::: {.cell}\n\n```{.r .cell-code}\npgamma(q=20,shape=10, scale=2,lower.tail=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5420703\n```\n\n\n:::\n:::\n\n\n<br>\n\n. . .\n\n\nWhat is the probability of observing 20 < $y$ < 40\n\n\n::: {.cell}\n\n```{.r .cell-code}\npgamma(q=40,shape=10, scale=2,lower.tail=TRUE)-\npgamma(q=20,shape=10, scale=2,lower.tail=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4529343\n```\n\n\n:::\n:::\n\n\n<br>\n\n. . .\n\n\n\nReverse the question: What values of $y$ and lower have a probability of 0.025\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqgamma(p=0.025,shape=10, scale=2,lower.tail=TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 9.590777\n```\n\n\n:::\n:::\n\n\n<br>\n\n. . .\n\nWhat values of $y$ and higher have a probability of 0.025\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqgamma(p=0.025,shape=10, scale=2,lower.tail=FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 34.16961\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncurve(dgamma(x,shape=10, scale=2),xlim=c(0,50),lwd=3,\n      xlab=\"y\", ylab=\"dgamma(x,shape=10, scale=2)\")\nabline(v=c(9.590777,34.16961),lwd=3,col=2)\n```\n\n::: {.cell-output-display}\n![](Probability_files/figure-revealjs/unnamed-chunk-22-1.png){width=960}\n:::\n:::\n\n\n\n. . .\n\nWe can consider samples from this population,\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(154434)\ny <- rgamma(100, shape=10, scale=2)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](Probability_files/figure-revealjs/unnamed-chunk-24-1.png){width=960}\n:::\n:::\n\n\n  <!-- . . . -->\n  \n<!-- <p style=\"text-align:center\"> -->\n<!-- What use is this to us? -->\n<!-- </p> -->\n\n\n\n\n## The others side of the coin {.scrollable}\n<span style=\"color:blue\">Statistics</span>:\nInterested in estimating population-level characteristics; i.e., the parameters\n\n$$\n\\begin{align*}\ny \\rightarrow& f(y|\\boldsymbol{\\theta}) \\\\\n\\end{align*}\n$$\n\n. . .\n\n<p style=\"text-align:center\">\n<span style=\"color:red\">\n**REMEMBER**\n</span>\n</p>\n$f(y|\\boldsymbol{\\theta})$ is a probability statement about $y$, <span style=\"color:red\"> **NOT** </span> $\\boldsymbol{\\theta}$.\n\n<br>\n\n",
    "supporting": [
      "Probability_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}