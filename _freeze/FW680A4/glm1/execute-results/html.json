{
  "hash": "6cb2461da0c4d523a7a5cafa955ed885",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle:  <span style=\"color:black\">Generalized Linear Models</span>\ntitle-slide-attributes:\n   data-background-image: /img/background2.png\nformat:\n  revealjs:\n    chalkboard: true\n    multiplex: true\n---\n\n\n## Objectives\n\n<!-- knitr::purl(\"./FW680A4/glm1.qmd\", output=\"./FW680A4/glm1.R\") -->\n\n\n```{=html}\n<style type=\"text/css\">\n\nbody, td {\n   font-size: 14px;\n}\ncode.r{\n  font-size: 30px;\n}\npre {\n  font-size: 12px\n}\n</style>\n```\n\n\n-   GLM framework\n-   matrix notation\n-   link functions\n-   glm function\n-   categorical independent variable\n-   linear and logistic regression\n\n## GLM\n<p style=\"color:purple\">Generalized linear model framework using matrix notation</p>\n\n. . .\n\n$$\n\\begin{align*}\n\\textbf{y}\\sim& [\\textbf{y}|\\boldsymbol{\\mu},\\sigma] \\\\\n\\text{g}(\\boldsymbol{\\mu}) =& \\textbf{X}\\boldsymbol{\\beta}\n\\end{align*}\n$$\n\n![](/img/tada.png){fig-align=\"center\" width=\"275\"}\n\n## Motivation\n\n<p style=\"color:purple\"><b>GLMs:</b></p>\n\n- t-test\n- ANOVA/ANCOVA\n- linear regression\n- logistic / probit regression\n- Poisson regression\n- log-linear regression\n- survival analysis\n- AND MORE!\n\n\n\n## GLM {.scrollable}\n<p style=\"color:purple\">Generalized linear model framework</p>\n\n\n$$\n\\begin{align*}\n\\textbf{y}\\sim& [\\textbf{y}|\\boldsymbol{\\mu},\\sigma] \\\\\n\\text{g}(\\boldsymbol{\\mu}) =& \\textbf{X}\\boldsymbol{\\beta}\n\\end{align*}\n$$\n\n### Elements\n::: {.incremental}\n\n-   prob. function to define the RV ($\\textbf{y}$)\n-   parameters of the prob. function ($\\boldsymbol{\\mu},\\sigma$)\n-   link function ($\\text{g}(\\boldsymbol{\\mu})$); deterministic transformation of parameters to new scale\n-   inverse-link function ($\\text{g}^{-1}(\\boldsymbol{\\textbf{X}\\boldsymbol{\\beta}})$); deterministic transformation of linear combination back to parameter scale\n\n-   design matrix of the explanatory variables ($\\textbf{X}$); these are known\n-   coefficient parameters ($\\boldsymbol{\\beta}$); needs estimating\n:::\n\n\n## Linear Regression  {.scrollable}\n\n#### index notation\n$$\n\\begin{align*}\ny_{i} \\sim& \\text{Normal}(\\mu_{i},\\sigma) \\\\\n\\mu_{i} =& \\beta_{0} + \\beta_{1}x_{1i} + \\beta_{2}x_{2i}\n\\end{align*}\n$$\n\n::: {.fragment}\n\n#### matrix notation\n$$\n\\begin{align*}\n\\textbf{y}\\sim& [\\textbf{y}|\\boldsymbol{\\mu},\\sigma]\\\\\n[\\textbf{y}|\\boldsymbol{\\mu},\\sigma]=& \\text{Normal}(\\boldsymbol{\\mu},\\sigma)\\\\\n\\boldsymbol{\\mu} =& \\text{g}^{-1}(\\textbf{X}\\boldsymbol{\\beta}) =  \\textbf{X}\\boldsymbol{\\beta} \\times 1\\\\\n\\text{g}(\\boldsymbol{\\mu}) =&   \\textbf{X}\\boldsymbol{\\beta}\n\\end{align*}\n$$\n\n:::\n\n::: {.fragment}\n\n\n\\begin{align*}\n\n\\textbf{y} =\n\\begin{bmatrix}\ny_{1} \\\\\ny_{2} \\\\\ny_{3} \\\\\n. \\\\\n. \\\\\ny_{n}  \n\\end{bmatrix}\n\n\n\\textbf{X} =\n\\begin{bmatrix}\n1 & x^{1}_1 & x^{2}_1 \\\\\n1 & x^{1}_2 & x^{2}_2 \\\\\n1 & x^{1}_3 & x^{2}_3 \\\\\n. & . .\\\\\n. & . .\\\\\nn & x^{1}_n & x^{2}_n\n\\end{bmatrix}\n\n\\boldsymbol{\\beta} = \\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\beta_2 \\\\\n\\end{bmatrix}\n\\end{align*}\n\nn = sample size\nx$_{1}$ & x$_{2}$ are independent variables\n\n:::\n\n## Linear Algebra {.scrollable}\n\n$\\text{g}(\\boldsymbol{\\mu}) = \\textbf{X}\\boldsymbol{\\beta}$\n\n<br>\n\n$\\textbf{X}$ is called the Design Matrix.\n\n$\\boldsymbol{\\beta}$ is a vector of coefficients.\n\n. . .\n\n$$\n\\textbf{X}=\n\\begin{bmatrix}\n1 & x_{1,2} & x_{1,3} \\\\\n1 & x_{2,2} & x_{2,3} \\\\\n1 & x_{3,2} & x_{3,3}\n\\end{bmatrix}\n\\boldsymbol{\\beta} =\n\\begin{bmatrix}\n\\beta_0  \\\\\n\\beta_1 \\\\ \n\\beta_2   \n\\end{bmatrix}\n$$\n\n. . .\n\n$$\n\\textbf{X}\\boldsymbol{\\beta} = \n\\begin{bmatrix}\n\\beta_0\\times 1 + \\beta_1\\times x_{1,2} + \\beta_2\\times x_{1,3} \\\\\n\\beta_0\\times 1 + \\beta_1\\times x_{2,2} + \\beta_2\\times x_{2,3} \\\\\n\\beta_0\\times 1 + \\beta_1\\times x_{3,2} + \\beta_2\\times x_{3,3} \\\\\n\\end{bmatrix}\\\\\n$$\n\n. . .\n\n$$\n \\textbf{X}\\boldsymbol{\\beta} = \n\\begin{bmatrix}\n\\beta_0 + \\beta_1 x_{1,2} + \\beta_2 x_{1,3} \\\\\n\\beta_0 + \\beta_1 x_{2,2} + \\beta_2 x_{2,3} \\\\\n\\beta_0 + \\beta_1 x_{3,2} + \\beta_2 x_{3,3} \\\\\n\\end{bmatrix}\\\\\n$$\n\n$$\n \\text{g}(\\boldsymbol{\\mu}) = \\textbf{X}\\boldsymbol{\\beta} = \\begin{bmatrix}\n\\text{lt}_{1} \\\\\n\\text{lt}_{2} \\\\\n\\text{lt}_{3} \n\\end{bmatrix}\n$$\nlt = linear terms\n\n\n. . .\n\n$$\n\\boldsymbol{\\mu} = \\text{g}^{-1}(\\textbf{X}\\boldsymbol{\\beta})\\ = \\textbf{X}\\boldsymbol{\\beta}  / 1= \\begin{bmatrix}\n\\text{lt}_{1}/1 \\\\\n\\text{lt}_{2}/1 \\\\\n\\text{lt}_{3}/1\n\\end{bmatrix}\n$$\n\n. . .\n\n$$\n\\boldsymbol{\\mu} = \\textbf{X}\\boldsymbol{\\beta} = \\begin{bmatrix}\n\\mu_{1} \\\\\n\\mu_{2} \\\\\n\\mu_{3}\n\\end{bmatrix}\n$$\n\n\n\n\n## Vector Notation {.scrollable}\n<p style=\"color:purple\">Row vectors</p>\n\n-   $\\textbf{y} \\equiv (y_1, y_2, . . ., y_n)$\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny <- matrix(c(1,2,3),nrow=1,ncol=3)\ny\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    1    2    3\n```\n\n\n:::\n:::\n\n\n<br>\n\n. . .\n\n\n<p style=\"color:purple\">Column vectors</p>\n\n-   $\\textbf{y} \\equiv (y_1, y_2, . . ., y_n)'$\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny <- matrix(c(1,2,3),nrow=3,ncol=1)\ny\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n[1,]    1\n[2,]    2\n[3,]    3\n```\n\n\n:::\n:::\n\n\n\n\n## Matrix Notation\n\n-   $\\textbf{X}\\equiv (\\textbf{x}_1,\\textbf{x}_2,...,\\textbf{x}_p)$\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\np=3\nX <- matrix(c(1,2,3,4,5,6,7,8,9),nrow=3,ncol=p,byrow=FALSE)\nX\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n```\n\n\n:::\n:::\n\n\n## Linear Algebra {.scrollable}\n\n-   $\\textbf{y}'\\textbf{y}$\n\n-   $\\textbf{y}' \\cdot \\textbf{y}$\n\n. . .\n\n$$\n=\\begin{bmatrix}\n1 & 2 & 3\n\\end{bmatrix}\n\\begin{bmatrix}\n1 \\\\\n2 \\\\\n3 \\\\\n\\end{bmatrix}\n$$\n\n. . .\n\n$$\n=\\begin{bmatrix}\n(1\\times1) + (2\\times2) + (3\\times3)\\\\\n\\end{bmatrix} \\\\= [14]\n$$\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt(y)%*%y\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n[1,]   14\n```\n\n\n:::\n:::\n\n\n## Linear Algebra\n\nWhen can we do matrix multiplication?\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfirst=t(y)\ndim(first)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1 3\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsecond=y\ndim(second)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3 1\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#When this is true\n  ncol(first)==nrow(second)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n```{=html}\n<span><center>(1 x 3) (3 x 1)</center></span>\n```\n\n\n## Linear Algebra\n\nNote that \n\n$\\textbf{y}'\\textbf{y} \\neq \\textbf{y}\\textbf{y}'$\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt(y)%*%y\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n[1,]   14\n```\n\n\n:::\n\n```{.r .cell-code}\ny%*%t(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    2    4    6\n[3,]    3    6    9\n```\n\n\n:::\n:::\n\n\n\n## Elephant Linear Regression Example {.scrollable}\n\n#### Categorical Variable\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n     weight    sex\n1 11488.991   Male\n2  4105.442 Female\n3  4299.308 Female\n```\n\n\n:::\n:::\n\n\n## Elephant Linear Regression Example {.scrollable}\n\n#### Categorical Variable\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel = glm(weight~sex, \n            data=dat,\n            family=gaussian(link = identity)\n            )\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = weight ~ sex, family = gaussian(link = identity), \n    data = dat)\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   4970.9      120.9   41.11   <2e-16 ***\nsexMale       6848.1      180.2   37.99   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 1608053)\n\n    Null deviance: 2639772182  on 199  degrees of freedom\nResidual deviance:  318394574  on 198  degrees of freedom\nAIC: 3429.7\n\nNumber of Fisher Scoring iterations: 2\n```\n\n\n:::\n:::\n\n\n\n## Elephant Linear Regression Example {.scrollable}\n\n#### Categorical Variable\n\n$x_{2}$ as an indicator of sex, female (0) or male (1)\n\n$x_{3}$ elephant age \n\n::: {.fragment}\n$$\n\\textbf{weight} \\sim \\text{Normal}(\\boldsymbol{\\mu},\\sigma)\\\\ \\\\\n\\boldsymbol{\\mu} =\\textbf{X}\\boldsymbol{\\beta} = \n\\begin{bmatrix}\n\\beta_0 + (\\beta_1\\times 1) + (\\beta_2\\times 10) \\\\\n\\beta_0 + (\\beta_1\\times 0) + (\\beta_2\\times 12) \\\\\n\\beta_0 + (\\beta_1\\times 0) + (\\beta_2\\times 15) \\\\\n\\end{bmatrix}\\\\\n$$\n:::\n\n::: {.fragment}\n\n$$\n\\hat{\\boldsymbol{\\mu}} = \\textbf{X}\\hat{\\boldsymbol{\\beta}} = \n\\begin{bmatrix}\n2552.82 + (6828.96\\times 1) + (145.74\\times 10) \\\\\n2552.82 + (6828.96\\times 0) + (145.74\\times 12) \\\\\n2552.82 + (6828.96\\times 0) + (145.74\\times 15) \\\\\n\\end{bmatrix}\\\\\n$$\n:::\n\n::: {.fragment}\n$$\n\\hat{\\boldsymbol{\\mu}} = \\textbf{X}\\hat{\\boldsymbol{\\beta}} = \n\\begin{bmatrix}\n2552.82 + 6828.96 + 1457.4 \\\\\n2552.82 + 0 + 1748.88 \\\\\n2552.82 + 0 + 2186.1 \\\\\n\\end{bmatrix}\\\\\n$$\n:::\n\n::: {.fragment}\n\n$$\n\\hat{\\boldsymbol{\\mu}} = \\textbf{X}\\hat{\\boldsymbol{\\beta}} = \n\\begin{bmatrix}\n10401.98     \\\\\n6779.52 \\\\\n5322.02 \\\\\n\\end{bmatrix}\\\\\n$$\n\n:::\n\n::: {.fragment}\n\n**So, what does $\\beta_1$ mean?**\n\n:::\n\n## glm and design matix\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# GLM coefs\n  X = model.matrix(~sex+age.years,data=dat)\n  head(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept) sexMale age.years\n1           1       1        10\n2           1       0        12\n3           1       0        15\n4           1       0        12\n5           1       1        20\n6           1       1         4\n```\n\n\n:::\n:::\n\n\n::: {.fragment}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n  glm(weight~0+X,data=dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:  glm(formula = weight ~ 0 + X, data = dat)\n\nCoefficients:\nX(Intercept)      XsexMale    Xage.years  \n      2552.8        6829.0         145.7  \n\nDegrees of Freedom: 200 Total (i.e. Null);  197 Residual\nNull Deviance:\t    1.561e+10 \nResidual Deviance: 75320000 \tAIC: 3143\n```\n\n\n:::\n:::\n\n\n:::\n\n**Sex variable is arranged by 'Dummy Coding'**\n\n## MLE Estimator with Linear Algebra\n\n$$\n\\hat{\\boldsymbol{\\beta}} = (\\textbf{X}'\\textbf{X})^{-1}\\textbf{X}'\\textbf{y}\n$$\n\n::: {.fragment}\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nX(Intercept)     XsexMale   Xage.years \n   2552.8203    6828.9645     145.7483 \n```\n\n\n:::\n:::\n\n\n:::\n\n<br>\n\n::: {.fragment}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Linear Algebra Maximum Likelihood Estimate\n  y=dat$weight\n  c(solve(t(X)%*%X)%*%t(X)%*%y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2552.8203 6828.9645  145.7483\n```\n\n\n:::\n:::\n\n\n:::\n\n## Link functions {.scrollable}\n\n- $\\text{g}(\\boldsymbol{\\mu}) = \\textbf{X}\\boldsymbol{\\beta}$\n\n- $\\boldsymbol{\\mu} = \\text{g}^{-1}(\\textbf{X}\\boldsymbol{\\beta})$\n\n. . .\n\nLink functions map parameters from one <span style=\"color:red\">support</span> to another.\n\n. . .\n\n<br>\n\n<span style=\"color:blue\">Why is that important for us?</span>\n\n<br>\n\n. . .\n\nTo put a linear model on parameters of interest and ensure the parameter support is maintained.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](glm1_files/figure-revealjs/unnamed-chunk-15-1.png){width=960}\n:::\n:::\n\n\n. . . \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](glm1_files/figure-revealjs/unnamed-chunk-16-1.png){width=960}\n:::\n:::\n\n\n\n. . .\n\n<br>\n\nComputers do not like boundaries (e.g., 0 or 1). It's easier to **guess** values to evaluate in a maximum liklihood optimization when there are no bounds. ($-\\infty$, $\\infty$)\n\n## Link functions {.scrollable}\n\n### <span style=\"color:blue\">Identity </span>\n$\\text{g}(\\boldsymbol{\\mu}) = \\mathbf{1}'\\boldsymbol{\\mu}$\n\n\n\n#### Linear Regression\n\n$$\n\\begin{align*}\n\\textbf{y}\\sim& [\\textbf{y}|\\boldsymbol{\\mu},\\sigma]\\\\\n[\\textbf{y}|\\boldsymbol{\\mu},\\sigma]=& \\text{Normal}(\\boldsymbol{\\mu},\\sigma)\\\\\n\\boldsymbol{\\mu} =& \\text{g}^{-1}(\\textbf{X}\\boldsymbol{\\beta}) =  \\textbf{X}\\boldsymbol{\\beta}\n\\end{align*}\n$$\n\n\nNo transformation is needed because the parameter support is maintained.\n$$\n\\begin{align*}\n\\mu \\in& (-\\infty,\\infty)\\\\\n\\textbf{X}\\boldsymbol{\\beta} \\in& (-\\infty,\\infty)\n\\end{align*}\n$$\n\n## Linear Regression Simulation {.scrollable}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Design matrix\n  set.seed(6454)\n  Var1 = seq(0,20,by=1)+rnorm(21,0,2)\n  X = model.matrix(~Var1)\n  head(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept)     Var1\n1           1 1.871755\n2           1 1.965267\n3           1 3.221875\n4           1 1.761916\n5           1 3.627037\n6           1 8.805140\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Marginal Coefficients  \n  beta = matrix(c(0,5))\n\n#linear terms\n  lt = X%*%beta\n\n#mu (link function)\n  mu = lt*1\n\n# Plot relationship b/w mean (mu) and variable of interest\n  plot(X[,2],mu,type=\"l\",lwd=4)  \n```\n\n::: {.cell-output-display}\n![](glm1_files/figure-revealjs/unnamed-chunk-18-1.png){width=960}\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#sample\n  set.seed(5435)\n  sigma = 3\n  y = rnorm(length(mu),mu,sd=sigma)\n  y\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1]  13.51028  10.74365  17.12227  10.72545  17.98375  44.88165  29.59957\n [8]  47.05866  49.08274  34.52391  25.02376  54.88971  55.74619  63.30454\n[15]  67.55550  87.75727  83.90235  70.66524  99.03730 103.14573  79.35456\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot relationship b/w mean (mu) and variable of interest\n  plot(X[,2],mu,type=\"l\",lwd=4)  \n  points(X[,2],y,pch=18,col=2,cex=2)\n```\n\n::: {.cell-output-display}\n![](glm1_files/figure-revealjs/unnamed-chunk-20-1.png){width=960}\n:::\n:::\n\n\n## Linear Regression Estimation {.scrollable}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit model to sample\n  model1=glm(y~0+X,family = gaussian(link = \"identity\"))\n```\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\n  summary(model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = y ~ 0 + X, family = gaussian(link = \"identity\"))\n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \nX(Intercept)   1.2914     1.4716   0.878    0.391    \nXVar1          4.8688     0.1254  38.839   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 11.43063)\n\n    Null deviance: 71532.62  on 21  degrees of freedom\nResidual deviance:   217.18  on 19  degrees of freedom\nAIC: 114.66\n\nNumber of Fisher Scoring iterations: 2\n```\n\n\n:::\n:::\n\n\n<br>\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(equatiomatic)\nextract_eq(model1)\n```\n\n::: {.cell-output-display}\n$$\nE( \\operatorname{y} ) = \\beta_{1}(\\operatorname{X}_{\\operatorname{(Intercept)}}) + \\beta_{2}(\\operatorname{X}_{\\operatorname{Var1}})\n$$\n\n:::\n:::\n\n\n## Linear Regression Evaluation {.scrollable}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n #sample many times  \n  y.many = replicate(1000,rnorm(length(mu),mean=c(mu), sd=sigma))\n  dim(y.many)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]   21 1000\n```\n\n\n:::\n\n```{.r .cell-code}\n #Estimate coefs for all 100 samples\n  coef.est=apply(y.many,2,FUN=function(y){\n              model1=glm(y~0+X,family = gaussian(link = \"identity\"))\n              model1$coefficients\n  })\n\n  dim(coef.est)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]    2 1000\n```\n\n\n:::\n:::\n\n\n<br>\n\n. . .\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](glm1_files/figure-revealjs/unnamed-chunk-25-1.png){width=960}\n:::\n\n::: {.cell-output-display}\n![](glm1_files/figure-revealjs/unnamed-chunk-25-2.png){width=960}\n:::\n:::\n\n\n\n## A Model by another name {.scrollable}\n\n<style>\ntd {\n  font-size: 23px\n}\n</style>\n\n| Model Name|$[y|\\boldsymbol{\\theta}]$| Link|\n|-----------|-------------------------|-----|\n| ANOVA ($x_{1}$ is categorical/multiple levels) | Normal        | identity | \n| ANCOVA ($x_{1}$ is categorical, $x_{2}$ is continuous))              | Normal        | identity |\n| t-test ($x_{1}$ is categorical with 2 levels)      | Normal        | identity |\n| Linear Regression $x_{1}$ is continuous   | Normal        | identity |\n| Multiple Linear Regression $x_{p}$ is continuous   | Normal        | identity |\n| Logistic Regression  | Binomial      | logit    |\n| Probit Regression    | Binomial      | probit   |\n| Log-linear Regression| Poisson       | log      |\n| Poisson Regression   | Poisson       | log      |\n| Survival Analysis    | Exponential   | log      | \n| Inverse Polynomial  | Gamma         | Reciprocal|\n\n: {tbl-colwidths=\"\\[60,10,10\\]\"}\n\n[Nelder and Wedderburn (1972)](https://www.jstor.org/stable/2344614)\n\n\n\n\n## Logistic Regression (logit link) {.scrollable}\n\n\n$$\n\\begin{align*}\n\\textbf{y}\\sim& [\\textbf{y}|N,\\boldsymbol{p}]\\\\\n\\end{align*}\n$$\n\n::: {.fragment}\n\n$$\n\\begin{align*}\n[\\textbf{y}|N,\\boldsymbol{p}] =& \\text{Binomial}(N,\\boldsymbol{p})\n\\end{align*}\n$$\n\n:::\n\n::: {.fragment}\n$$\n\\begin{align*}\n\\text{g}(\\boldsymbol{p}) =& \\text{logit}(\\boldsymbol{p}) = \\text{log}(\\frac{\\boldsymbol{p}}{1-\\boldsymbol{p}})\n\\end{align*}\n$$\n\n::: \n\n\n## inverse-logit (expit)\n\n$$\n\\boldsymbol{p} = g^{-1}(\\boldsymbol{\\textbf{X}\\boldsymbol{\\beta}}) = \\text{logit}^{-1}(\\textbf{X}\\boldsymbol{\\beta}) = \\frac{e^{\\textbf{X}\\boldsymbol{\\beta}}}{e^{\\textbf{X}\\boldsymbol{\\beta}}+1}\n$$\n\n## Logistic Regression\n\n#### Full model\n\n$$\n\\begin{align*}\n\\textbf{y} \\sim& \\text{Binomial}(N,\\boldsymbol{p})\\\\\n\\text{logit}(\\boldsymbol{p}) =& \\textbf{X}\\boldsymbol{\\beta}\n\\end{align*}\n$$\n\n::: {.fragment}\n\n<span style=\"color:blue\">Remember,</span>\n<br>\n$\\boldsymbol{p} \\in [0,1]$\n\n$\\textbf{X}\\boldsymbol{\\beta} \\in (-\\infty,\\infty)$\n\n:::\n\n## logit/p mapping {.scrollable}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\np=seq(0.001,0.999,by=0.01)\nlogit.p=qlogis(p)\npar(cex.lab=1.5,cex.axis=1.5)\nplot(p,logit.p,type=\"l\",lwd=4,col=3,xlab='p',ylab=\"logit(p)\")\n```\n\n::: {.cell-output-display}\n![](glm1_files/figure-revealjs/unnamed-chunk-26-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Logistic Regression Simulation {.scrollable}\n\n. . .\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n#Design matrix\n  set.seed(43534)\n  Var1 = rnorm(100)\n  X = model.matrix(~Var1)\n  head(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept)       Var1\n1           1  1.4371396\n2           1 -0.4835090\n3           1  1.0370452\n4           1 -0.5894099\n5           1  1.2633564\n6           1 -1.9008071\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# marginal coefficients (on logit-scale)\n  beta=c(-2,4)\n\n#linear terms\n  lt = X%*%beta\n\n#transformation via link function to probability scale\n  p=plogis(lt)\n  head(round(p,digits=2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  [,1]\n1 0.98\n2 0.02\n3 0.90\n4 0.01\n5 0.95\n6 0.00\n```\n\n\n:::\n\n```{.r .cell-code}\n#sample\n  set.seed(14353)\n  y = rbinom(n=length(p),size=1,p)\n  y\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  [1] 1 0 1 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0\n [38] 0 1 1 0 1 0 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0\n [75] 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n#Plot the linear 'terms'  and explantory variable\n  par(cex.lab=1.2,cex.axis=1.2)\n  plot(Var1,lt,type=\"b\",lwd=3,col=2,\n       xlab=\"x\",ylab=\"Linear Terms (logit-value)\")  \n```\n\n::: {.cell-output-display}\n![](glm1_files/figure-revealjs/unnamed-chunk-29-1.png){fig-align='center' width=960}\n:::\n\n```{.r .cell-code}\n  index=order(Var1)\n  plot(Var1[index],p[index],type=\"b\",lwd=3,col=2,xlab=\"x\",ylab=\"Probability\")  \n```\n\n::: {.cell-output-display}\n![](glm1_files/figure-revealjs/unnamed-chunk-29-2.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Logistic Regression Estimation {.scrollable}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit model to sample \n  model1=glm(y~0+X, family = binomial(link = \"logit\"))\n  summary(model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = y ~ 0 + X, family = binomial(link = \"logit\"))\n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \nX(Intercept)  -2.5304     0.6473  -3.909 9.25e-05 ***\nXVar1          5.4198     1.2232   4.431 9.38e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 138.63  on 100  degrees of freedom\nResidual deviance:  39.14  on  98  degrees of freedom\nAIC: 43.14\n\nNumber of Fisher Scoring iterations: 7\n```\n\n\n:::\n:::\n\n\n## Logistic Regression Evaluation {.scrollable}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n #sample many times  \n  n.sim=1000\n  y.many = replicate(n.sim,rbinom(n=length(p),size=1,p))\n  dim(y.many)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  100 1000\n```\n\n\n:::\n\n```{.r .cell-code}\n #Estimate coefs for all 100 samples\n  coef.est=apply(y.many,2,FUN=function(y){\n          model1=glm(y~0+X, family = binomial(link = \"logit\"))\n          \n  model1$coefficients\n  })\n\n  dim(coef.est)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]    2 1000\n```\n\n\n:::\n:::\n\n\n\n<br>\n\n. . .\n\n## Plot Intercept\n\n::: {.cell}\n::: {.cell-output-display}\n![](glm1_files/figure-revealjs/unnamed-chunk-32-1.png){width=960}\n:::\n:::\n\n\n## Plot Slope\n\n::: {.cell}\n::: {.cell-output-display}\n![](glm1_files/figure-revealjs/unnamed-chunk-33-1.png){width=960}\n:::\n:::\n\n\n## Evaluate Sampling Distributions\n\nIs our model biased?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Relative Bias\n  (mean(coef.est[1,])-beta[1])/beta[1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0879452\n```\n\n\n:::\n\n```{.r .cell-code}\n  (mean(coef.est[2,])-beta[2])/beta[2]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.09751027\n```\n\n\n:::\n:::\n\n\n## Evaluate Sampling Distributions\n\nAre we going to estimate the sign of the coef correctly?\n\n::: {.cell}\n\n```{.r .cell-code}\n# Probability of estimating a coef sign correctly\n  length(which(coef.est[1,]<0))/n.sim\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1\n```\n\n\n:::\n\n```{.r .cell-code}\n  length(which(coef.est[2,]>0))/n.sim\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1\n```\n\n\n:::\n:::\n\n\n## Evaluate Sampling Distributions\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Probability of estimating the slope 2x the truth\n    length(which(coef.est[2,]>2*beta[2]))/n.sim\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.013\n```\n\n\n:::\n:::\n\n\n## Evaluate Sampling Distributions\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Probability of estimating the slope within 1\nlength(which(coef.est[2,]>=beta[2]-1 & coef.est[2,]<=beta[2]+1))/n.sim\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.741\n```\n\n\n:::\n:::\n\n\n## Logistic Regression Biased Correction {.scrollable}\n\nbrglm: Bias Reduction in Binomial-Response Generalized Linear Models\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbrglm::brglm(y~0+X, family = binomial(link = \"logit\"))\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Relative Bias\n  (mean(coef.est[1,])-beta[1])/beta[1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.007785692\n```\n\n\n:::\n\n```{.r .cell-code}\n  (mean(coef.est[2,])-beta[2])/beta[2]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.008799186\n```\n\n\n:::\n:::\n\n  \n\n# Recap\n\n. . .\n\n-   GLM framework\n-   matrix notation\n-   link functions\n-   glm function\n-   categorical independent variable\n-   linear and logistic regression\n",
    "supporting": [
      "glm1_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}