{
  "hash": "bb27847fb6f6414d3f3470ddb643dae5",
  "result": {
    "engine": "knitr",
    "markdown": "## Multi-Collinearity\n\n![](/img/graham.png){fig-align=\"center\" width=\"275\"}\n\n. . .\n\nCorrelation among explantory variables\n\n-   Can happen when $r \\geq 0.28$ or $r^2 \\geq 0.08$\n-   causes inaccurate estimation\n-   decreases statistical power\n-   leads to exclusion of important predictor variables\n\n\n## Multi-Collinearity (Code) {.scrollable}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(faux)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n************\nWelcome to faux. For support and examples visit:\nhttps://debruine.github.io/faux/\n- Get and set global package options with: faux_options()\n************\n```\n\n\n:::\n\n```{.r .cell-code}\nn=100\nset.seed(543531)\nx.var <- rnorm_multi(n = n, \n                  mu = c(10, 20),\n                  sd = c(1, 1),\n                  r = c(0), \n                  varnames = c(\"A\", \"B\"),\n                  empirical = FALSE)\n\nset.seed(54353)\nx.var.cor <- rnorm_multi(n = n, \n                  mu = c(10, 20),\n                  sd = c(1, 1),\n                  r = c(0.8), \n                  varnames = c(\"A\", \"B\"),\n                  empirical = FALSE)\n\n#Correlation\n  cor(x.var)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           A          B\nA 1.00000000 0.04340106\nB 0.04340106 1.00000000\n```\n\n\n:::\n\n```{.r .cell-code}\n  cor(x.var.cor)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          A         B\nA 1.0000000 0.8354425\nB 0.8354425 1.0000000\n```\n\n\n:::\n:::\n\n\n## Multi-Collinearity (Code)\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npar(mfrow=c(1,2))\n  plot(x.var.cor$A,x.var.cor$B)\n  plot(x.var$A,x.var$B)\n```\n\n::: {.cell-output-display}\n![](temp.multicollinear_files/figure-html/unnamed-chunk-2-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n## Multi-Collinearity (Code)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Design matrices\n  X.cor=model.matrix(~x.var.cor$A+x.var.cor$B)\n  X=model.matrix(~x.var$A+x.var$B)\n\n#True coefs  \n  beta=c(1,2,3)\n\n#Derive mu  \n  mu=X%*%beta\n  mu.cor=X.cor%*%beta\n\n#simulate data  \n  set.seed(54353)\n  y=rnorm(n,mu,2)\n  y.cor=rnorm(n,mu.cor,2)\n```\n:::\n\n\n## Model Estimates\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(glm(y~0+X))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = y ~ 0 + X)\n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \nX(Intercept)   0.3514     4.5494   0.077    0.939    \nXx.var$A       2.0619     0.2100   9.820 3.29e-16 ***\nXx.var$B       3.0029     0.2087  14.390  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 4.288366)\n\n    Null deviance: 645623.40  on 100  degrees of freedom\nResidual deviance:    415.97  on  97  degrees of freedom\nAIC: 434.33\n\nNumber of Fisher Scoring iterations: 2\n```\n\n\n:::\n:::\n\n\n\n## Model Estimates\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(glm(y~0+X.cor))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = y ~ 0 + X.cor)\n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \nX.cor(Intercept) 38.361082   7.926825   4.839 4.91e-06 ***\nX.corx.var.cor$A  0.002467   0.681426   0.004  0.99712    \nX.corx.var.cor$B  2.094430   0.634710   3.300  0.00135 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 13.3163)\n\n    Null deviance: 645623.4  on 100  degrees of freedom\nResidual deviance:   1291.7  on  97  degrees of freedom\nAIC: 547.64\n\nNumber of Fisher Scoring iterations: 2\n```\n\n\n:::\n:::",
    "supporting": [
      "temp.multicollinear_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}