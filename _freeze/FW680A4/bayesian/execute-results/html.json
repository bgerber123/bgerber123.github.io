{
  "hash": "d1bfac59d2c02cb060531f58da4b9812",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle:  <span style=\"color:purple\">Bayes the way</span>\ntitle-slide-attributes:\n  data-background-image: /img//background4.png\n  background-opacity: \"0.45\"\nformat:\n  revealjs:\n    chalkboard: true\n    multiplex: true    \n---\n\n\n\n\n## All things Bayesian\n\n```{=html}\n<style type=\"text/css\">\n\ncode.r{\n  font-size: 30px;\n}\n</style>\n```\n\n\n-   <p style=\"color:orange\">Bayesian Inference</p>\n\n-   <p style=\"color:orange\">Bayes Thereom</p>\n\n-   <p style=\"color:orange\">Bayesian Components</p>\n    - likelihood, prior, evidence, posterior\n\n-   <p style=\"color:orange\">Bayesian Computation</p>\n    - Conjugacy, Markov Chain Monte Carlo\n\n## Probability, Data, and Parameters\n\n**What do we want our model to tell us?**\n\n. . .\n\n<p style=\"color:orange\">Do we want to make probability statements about our data?</p>\n\n<br>\n\n. . .\n\n\n**Likelihood** = P(data|parameters)\n\n<br>\n\n. . .\n\n**90\\% CI**: the long-run proportion of corresponding CIs that will contain the true value 90\\% of the time.\n\n\n## Probability, Data, and Parameters\n\n**What do we want our model to tell us?**\n\n<p style=\"color:orange\">Do we want to make probability statements about our parameters?</p>\n\n<br>\n\n. . .\n\n**Posterior** = P(parameters|data)\n\n<br>\n\n**Alternative Interval**: 90\\% probability that the true value lies within the interval, given the evidence from the observed data.\n\n\n\n## Likelihood Inference\n\nEstimate of the population size of hedgehogs at two sites.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](bayesian_files/figure-revealjs/unnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n\n## Bayesian Inference\n\n<span style=\"color:orange\">Posterior Samples</span>\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 102.67671  81.11546  81.75260  87.77246  73.99043  80.70631  76.26219\n [8]  83.99927  64.74208  26.93133\n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](bayesian_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Bayesian Inference\n\n<span style=\"color:orange\">Posterior Samples</span>\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 102.67671  81.11546  81.75260  87.77246  73.99043  80.70631  76.26219\n [8]  83.99927  64.74208  26.93133\n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](bayesian_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Bayesian Inference\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](bayesian_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=960}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndiff=post2-post1\nlength(which(diff>0))/length(diff)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8362\n```\n\n\n:::\n:::\n\n\n## Likelihood Inference (coeficient) {.scrollable}\n\n\n::: {.cell}\n\n:::\n\n\ny is Body size of a beetle species\n\nx is elevation\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(glm(y~x))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = y ~ x)\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.9862     0.2435   4.049 0.000684 ***\nx             0.5089     0.4022   1.265 0.221093    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 1.245548)\n\n    Null deviance: 25.659  on 20  degrees of freedom\nResidual deviance: 23.665  on 19  degrees of freedom\nAIC: 68.105\n\nNumber of Fisher Scoring iterations: 2\n```\n\n\n:::\n:::\n\n\n## Bayesian Inference (coeficient)\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](bayesian_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\n## Bayesian Inference (coeficient) {.scrollable}    \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Posterior Mean\n  mean(post)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5185186\n```\n\n\n:::\n:::\n\n\n<br>\n\n. . .\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Credible/Probability Intervals \n  quantile(post,prob=c(0.025,0.975))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     2.5%     97.5% \n-1.461887  2.403232 \n```\n\n\n:::\n:::\n\n\n<br>\n\n. . .\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# #Probabilty of a postive effect\n length(which(post>0))/length(post)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.709\n```\n\n\n:::\n:::\n\n\n<br>\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# #Probabilty of a negative effect\n length(which(post<0))/length(post)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.291\n```\n\n\n:::\n:::\n\n\n## Bayes Theorem \n\n::: columns\n::: {.column width=\"40%\"}\n\n-   <p style=\"color:purple\">Marginal Probability</p>\n    - $P(A)$\n    - $P(B)$\n:::\n::: {.column width=\"60%\"}\n\n<span><center>Sampled N = 10 locations</center></span>\n\n![](Bayesian_files/bayes.png){fig-align=\"center\" width=\"700\"}\n\n:::\n:::\n\n## Bayes Theorem \n\n::: columns\n::: {.column width=\"40%\"}\n\n-   <p style=\"color:purple\">Joint Probability</p>\n    - $P(A \\cap B)$\n    - $P(A \\cap \\overline{B})$\n    - $P(B \\cap \\overline{A})$\n:::\n::: {.column width=\"60%\"}\n\n<span><center>Sampled N = 10 locations</center></span>\n\n![](Bayesian_files/bayes.png){fig-align=\"center\" width=\"700\"}\n\n:::\n:::\n\n## Bayes Theorem \n\n::: columns\n::: {.column width=\"40%\"}\n\n-   <p style=\"color:purple\">Conditional Probability</p>\n    - $P(A|B)$ \n    - $P(B|A)$\n    - $P(B|\\overline{A})$\n    - $P(A|\\overline{B})$\n:::\n::: {.column width=\"60%\"}\n\n<span><center>Sampled N = 10 locations</center></span>\n\n![](Bayesian_files/bayes.png){fig-align=\"center\" width=\"700\"}\n\n:::\n:::\n\n\n## Bayes Theorem \n\n::: columns\n::: {.column width=\"40%\"}\n\n-   <p style=\"color:purple\">OR Probability</p>\n    - $P(A\\cup B)$\n:::\n::: {.column width=\"60%\"}\n\n<span><center>Sampled N = 10 locations</center></span>\n\n![](Bayesian_files/bayes.png){fig-align=\"center\" width=\"700\"}\n\n:::\n:::\n\n\n## Notice that... {.scrollable}\n\n$$\n\\begin{equation}\nP(A \\cap B) = \\\\\nP(A|B)P(B) = \\\\\nP(B|A)P(A) = \\\\\n\\end{equation}\n$$\n\n. . .\n\n$$\n\\begin{equation}\nP(A|B)P(B) = P(A \\cap B) \\\\\nP(B|A)P(A) = P(A \\cap B) \\\\\n\\end{equation}\n$$\n\n. . .\n\n$$\n\\begin{equation}\nP(B|A)P(A) = P(A|B)P(B)\n\\end{equation}\n$$\n\n\n. . .\n\n\n### <span style=\"color:orange\">Bayes Theoreom</span> \n\n\n$$\n\\begin{equation}\nP(B|A) = \\frac{P(A|B)P(B)}{P(A)} \\\\\n\\end{equation}\n$$\n\n. . .\n\n$$\n\\begin{equation}\nP(B|A) = \\frac{P(A \\cap B)}{P(A)} \\\\\n\\end{equation}\n$$\n\n## Bayes Components {.scrollable}\n\nparam = parameters\n$$\n\\begin{equation}\nP(\\text{param}|\\text{data}) = \\frac{P(\\text{data}|\\text{param})P(\\text{param})}{P(\\text{data})} \\\\\n\\end{equation}\n$$\n\n. . .\n\n<p style=\"color:orange\">Posterior Probability/Belief</p> \n\n. . .\n\n<p style=\"color:orange\">Likelihood</p>\n\n. . .\n\n<p style=\"color:orange\">Prior Probability</p> \n\n. . .\n\n<p style=\"color:orange\">Evidence or Marginal Likelihood</p> \n\n\n## Bayes Components {.scrollable}\n\nparam = parameters\n\n$$\n\\begin{equation}\nP(\\text{param}|\\text{data}) = \\frac{P(\\text{data}|\\text{param})P(\\text{param})}{\\int_{\\forall \\text{ Param}} P(\\text{data}|\\text{param})P(\\text{param})} \n\\end{equation}\n$$\n\n<p style=\"color:orange\">Posterior Probability/Belief</p> \n\n<p style=\"color:orange\">Likelihood</p>\n\n<p style=\"color:orange\">Prior Probability</p> \n\n<p style=\"color:orange\">Evidence or Marginal Likelihood</p> \n\n## Bayes Components {.scrollable}\n\n$$\n\\begin{equation}\n\\text{Posterior} = \\frac{\\text{Likelihood} \\times \\text{Prior}}{\\text{Evidence}} \\\\\n\\end{equation}\n$$\n\n. . .\n\n$$\n\\begin{equation}\n\\text{Posterior} \\propto \\text{Likelihood} \\times \\text{Prior} \\end{equation}\n$$\n\n. . .\n\n$$\n\\begin{equation}\n\\text{Posterior} \\propto \\text{Likelihood} \n\\end{equation}\n$$\n\n## The Prior {.scrollable}\n\nAll parameters in a Bayesian model require a prior specified\n\n\n$y_{i} \\sim \\text{Binom}(N, \\theta)$\n\n::: {.fragment}\n$\\theta \\sim \\text{Beta}(\\alpha = 4, \\beta=2)$\n::: \n\n::: {.fragment}\n\n::: {.cell}\n\n```{.r .cell-code}\ncurve(dbeta(x, 4,2),xlim=c(0,1),lwd=5)\n```\n\n::: {.cell-output-display}\n![](bayesian_files/figure-revealjs/prior-1.png){width=960}\n:::\n:::\n\n\n::: \n\n\n## The Prior {.scrollable}\n\nAll parameters in a Bayesian model require a prior specified\n\n\n$y_{i} \\sim \\text{Binom}(N, \\theta)$\n\n$\\theta \\sim \\text{Beta}(\\alpha = 1, \\beta=1)$\n\n::: {.fragment}\n\n::: {.cell}\n\n```{.r .cell-code}\ncurve(dbeta(x, 1,1),xlim=c(0,1),lwd=5)\n```\n\n::: {.cell-output-display}\n![](bayesian_files/figure-revealjs/prior2-1.png){width=960}\n:::\n:::\n\n\n::: \n\n\n## The Prior\n\n::: {.incremental}\n\n-   The prior describes what we know about the parameter before we collect any data\n\n-   Priors can contain a lot of information  (<span style=\"color:purple\">informative priors </span>) or very little (<span style=\"color:purple\">diffuse priors </span>)\n\n-   Well-constructed priors can also improve the behavior of our models (computational advantage)\n\n:::\n\n\n## The Prior\n\n<span style=\"color:purple\">Use diffuse priors as a starting point</span>\n\n. . .\n\nIt's fine to use diffuse priors as you develop your model but you should always prefer to use \"appropriate, well-contructed informative priors\" (Hobbs \\& Hooten, 2015)\n\n\n## The Prior\n\n<span style=\"color:purple\">Use your \"domain knowledge\"</span>\n\n. . .\n\nWe can often come up with weakly informative priors just by knowing something about the range of plausible values of our parameters.\n\n## The Prior\n\n<span style=\"color:purple\">Dive into the literature</span>\n\n. . .\n\nFind published estimates and use moment matching and other methods to convert published estimates into prior distributions\n\n## The Prior\n\n<span style=\"color:purple\">Visualize your prior distribution</span>\n\n. . .\n\nBe sure to look at the prior in terms of the parameters you want to make inferences about (use simulation!)\n\n## The Prior\n\n<span style=\"color:purple\">Do a sensitivity analysis</span>\n\n. . .\n\nDoes changing the prior change your posterior inference? If not, don't sweat it. If it does, you'll need to return to point 2 and justify your prior choice\n\n## Bayesian Model {.scrollable}\n\n<p style=\"color:purple\">Model</p> \n$$\n\\textbf{y} \\sim \\text{Bernoulli}(p)\n$$\n\n. . .\n\n<p style=\"color:purple\">Prior</p> \n\n$$\np \\sim \\text{Beta}( \\alpha, \\beta) \\\\\n$$\n\n. . .\n\nThese are called Prior hyperparameters\n\n$$\n\\alpha = 1 \\\\\n\\beta = 1\n$$\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncurve(dbeta(x,1,1),xlim=c(0,1),lwd=3,col=2,xlab=\"p\",\n      ylab = \"Prior Probability Density\")\n```\n\n::: {.cell-output-display}\n![](bayesian_files/figure-revealjs/unnamed-chunk-15-1.png){width=960}\n:::\n:::\n\n\n<!-- sum(dbinom(y,size=1,p,log=TRUE)) -->\n<!-- dbeta(p,1,1,log=TRUE) -->\n\n## Conjugate Distribution {.scrollable}\n\n<p style=\"color:purple\">Likelihood (Joint Probability of y)</p> \n\n$$\n\\mathscr{L}(p|y) = \\prod_{i=1}^{n} P(y_{i}|p)  = \\prod_{i=1}^{N}(p^{y}(1-p)^{1-y_{i}})\n$$\n\n. . .\n\n<p style=\"color:purple\">Prior Distribution</p> \n\n$$\nP(p) = \\frac{p^{\\alpha-1}(1-p)^{\\beta-1}}{B(\\alpha,\\beta)}\n$$\n\n. . .\n\n<p style=\"color:purple\">Posterior Distribution of p</p> \n\n$$\nP(p|y) = \\frac{\\prod_{i=1}^{N}(p^{y}(1-p)^{1-y_{i}}) \\times \\frac{p^{\\alpha-1}(1-p)^{\\beta-1}}{B(\\alpha,\\beta)} }{\\int_{p}(\\text{numerator})}\n$$\n\n::: {.fragment}\n\n<center><span style=\"color:purple\">CONJUGACY!</span></center>\n\n:::\n\n\n::: {.fragment}\n$$\nP(p|y) \\sim \\text{Beta}(\\alpha^*,\\beta^*)\n$$\n\n:::\n\n::: {.fragment}\n$\\alpha^*$ and $\\beta^*$ are called Posterior hyperparameters\n\n$$\n\\alpha^* = \\alpha + \\sum_{i=1}^{N}y_i \\\\\n\\beta^* = \\beta + N - \\sum_{i=1}^{N}y_i \\\\\n$$\n\n[Wikipedia Conjugate Page](https://en.wikipedia.org/wiki/Conjugate_prior)\n\n[Conjugate Derivation](https://towardsdatascience.com/conjugate-prior-explained-75957dc80bfb)\n\n:::\n\n## Hippos\n\nWe do a small study on hippo survival and get these data...\n\n![](Bayesian_files/hippos.png){fig-align=\"center\" width=\"300\"}\n\n<span><center>7 Hippos Died </center></span>\n<span><center>2 Hippos Lived</center></span>\n\n## Hippos: Likelihood Model\n\n$$\n\\begin{align*}\n\\textbf{y} \\sim& \\text{Binomial}(N,p)\\\\\n\\end{align*}\n$$\n\n::: {.fragment}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Survival outcomes of three adult hippos\n  y1=c(0,0,0,0,0,0,0,1,1)\n  N1=length(y1)\n  mle.p=mean(y1)\n  mle.p\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2222222\n```\n\n\n:::\n:::\n\n\n::: \n\n## Hippos: Bayesian Model (Prior 1) {.scrollable}\n\n$$\n\\begin{align*}\n\\textbf{y} \\sim& \\text{Binomial}(N,p)\\\\\np \\sim& \\text{Beta}(\\alpha,\\beta)\n\\end{align*}\n$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n  alpha.prior1=1\n  beta.prior1=1\n```\n:::\n\n\n::: {.fragment}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](bayesian_files/figure-revealjs/unnamed-chunk-18-1.png){width=960}\n:::\n:::\n\n\n:::\n\n\n## Hippos: Bayesian Model (Prior 2) {.scrollable}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n  alpha.prior2=10\n  beta.prior2=2\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](bayesian_files/figure-revealjs/unnamed-chunk-20-1.png){width=960}\n:::\n:::\n\n\n## Hippos: Bayesian Model (Prior 3) {.scrollable}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n  alpha.prior3=150\n  beta.prior3=15\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](bayesian_files/figure-revealjs/unnamed-chunk-22-1.png){width=960}\n:::\n:::\n\n\n## Hippos: Bayesian Model (Posteriors) {.scrollable}\n\n$$\nP(p|y) \\sim \\text{Beta}(\\alpha^*,\\beta^*)\\\\\n\\alpha^* = \\alpha + \\sum_{i=1}^{N}y_i \\\\\n\\beta^* = \\beta + N - \\sum_{i=1}^{N}y_i \\\\\n$$\n\n::: {.cell}\n\n```{.r .cell-code}\n# Note- the data are the same, but the prior is changing.\n# Gibbs sampler\n  post.1=rbeta(10000,alpha.prior1+sum(y1),beta.prior1+N1-sum(y1))\n  post.2=rbeta(10000,alpha.prior2+sum(y1),beta.prior2+N1-sum(y1))\n  post.3=rbeta(10000,alpha.prior3+sum(y1),beta.prior3+N1-sum(y1))\n```\n:::\n\n\n## Hippos: Bayesian Model (Posteriors)\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](bayesian_files/figure-revealjs/unnamed-chunk-24-1.png){width=960}\n:::\n:::\n\n\n## Hippos: Bayesian Model (Posteriors)\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](bayesian_files/figure-revealjs/unnamed-chunk-25-1.png){width=960}\n:::\n:::\n\n\n## Hippos: Bayesian Model (Posteriors)\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](bayesian_files/figure-revealjs/unnamed-chunk-26-1.png){width=960}\n:::\n:::\n\n\n## Hippos: More data! (Prior 1)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny2=c(0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,\n     1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1)\nlength(y2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 40\n```\n\n\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](bayesian_files/figure-revealjs/unnamed-chunk-28-1.png){width=960}\n:::\n:::\n\n\n## Hippos: More data! (Prior 2)\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](bayesian_files/figure-revealjs/unnamed-chunk-29-1.png){width=960}\n:::\n:::\n\n\n## Hippos: More data! (Prior 3)\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](bayesian_files/figure-revealjs/unnamed-chunk-30-1.png){width=960}\n:::\n:::\n\n\n## Hippos: Data/prior Comparison {.scollable}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](bayesian_files/figure-revealjs/unnamed-chunk-31-1.png){width=960}\n:::\n:::\n\n\n## Markov Chain Monte Carlo {.scollable}\n\nOften don't have conjugate likelihood and priors, so we use MCMC algorithims to sample posteriors.\n\nOptions\n\n-   Write your own algorithim\n-   BUGS, JAGS, Stan, Nimble\n\n## MCMC Sampling \n\n-   number of sample (iterations)\n-   thinning (which iterations to keep), every one (1), ever other one (2), every third one (3)\n-   burn-in (how many of the first samples to remove)\n-   chains (uniqe sets of samples; needed for convergence tests)\n",
    "supporting": [
      "bayesian_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}