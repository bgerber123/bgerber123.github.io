---
title: <span style="color:orange">Probability</span>
title-slide-attributes:
    data-background-image: /img/chalkboard.png
format:
  revealjs:
    chalkboard: true
    multiplex: true
---



## Objectives

<!-- knitr::purl("./FW680A4/probability.qmd", output="./FW680A4/probability.R") -->

- Connect random variables, probabilities, and parameters

- define prob. functions

  - discrete and continuous random variables
  
- use/plot prob. functions

- learn some notation

## Probability/Statistics

```{=html}
<style type="text/css">

body, td {
   font-size: 14px;
}
code.r{
  font-size: 30px;
}
pre {
  font-size: 20px
}
</style>
```


<br>

<span style="color:red">Probability</span> and <span style="color:blue">statistics</span> are the opposite sides of the same coin.

<br>

. . .

To understand <span style="color:blue">statistics</span>, we need to understand <span style="color:red">probability</span> and <span style="color:red">probability functions</span>. 


<br>

. . .

The two key things to understand this connection is the <span style="color:orange">random variable (RV)</span> and <span style="color:orange">parameters</span> (e.g., $\theta$, $\sigma$, $\epsilon$, $\mu$).

## Motivation {.fit-text}
<p style="text-align:center">
Why learn about RVs and probability math?
</p>

. . .


<span style="color:red">Foundations of:</span> 

-   linear regression
-   generalized linear models
-   mixed models

. . .

<span style="color:red">Our Goal:</span> 

- conceptual framework to think about *data*, *probabilities*, and *parameters*
- mathematical connections and notation

## Not Random Variables

$$
\begin{align*} 
a =& 10 \\
b =& \text{log}(a) \times 12 \\
c =& \frac{a}{b} \\
y =& \beta_0 + \beta_1*c
\end{align*}
$$

. . .

All variables here are <span style="color:red">scalars</span>. They are what they are and that is it.  $\beta$ variables and $y$ are currently unknown, but still <span style="color:red">scalars</span>. 

. . .

<p style="text-align:center">
<span style="color:red">Scalars</span> are quantities that are fully described by a magnitude (or numerical value) alone.
</p>

## Random Variables {.scrollable}

$$
y \sim f(y)
$$

$y$ is a random variable which may change values each observation; it changes based on a probability function, $f(y)$. 

<br>

. . .


The tilde ($\sim$) denotes "has the probability distribution of".

<br>

. . .


Which value (y) is observed is predictable. Need to know *parameters* ($\theta$) of the probability function $f(y)$.

<br>

. . .

Specifically, $f(y|\theta)$, where '|' is read as 'given'. 

. . .

<p style="text-align:center">
<span style="color:blue">Toss of a coin</span> <br>
<span style="color:blue">Roll of a die</span> <br>
<span style="color:blue">Weight of a captured elk</span> <br>
<span style="color:blue">Count of plants in a sampled plot</span> <br>
</p>

<br>

. . .

<p style="text-align:center"> The values observed can be understand based on the frequency within the <span style="color:red">population</span> or presumed <span style="color:red">super-population</span>. These frequencies can be described by probabilities. </p>

## Frequency / Probabilitities {.scrollable}

```{r, eval=TRUE,echo=FALSE}
set.seed(452)
y=rpois(1000,10)
main="Distribution of counts of plants \nin all possible plots"
```

```{r, eval=TRUE,echo=TRUE}
#| code-line-numbers: 2,3
par(mfrow=c(1,2))
hist(y, breaks=20,xlim=c(0,25),main=main)
hist(y, breaks=20,xlim=c(0,25),freq = FALSE,main=main)
```

. . . 

We often only get to see **ONE** sample from this distribution.

```{r, eval=TRUE,echo=FALSE}
par(mfrow=c(1,2))
set.seed(452)
y=rpois(50,10)
hist(y, main="Sample of counts of plants (n=50)",breaks=10,xlim=c(0,25))
hist(y, main="Sample of counts of plants (n=50)",breaks=10,xlim=c(0,25),freq = FALSE)
```


## Random Variables 

We are often interested in the characteristics of the whole population of frequencies,

- <span style="color:blue">central tendency</span> (mean, mode, median)
- <span style="color:blue">variability</span>  (var, sd)
- <span style="color:blue">proportion of the population that meets some condition</span><br> P($8 \leq y \leq$ 12) =0.68

. . .


We infer what these are based on our sample (i.e., statistical inference). 

## Philosophy

<span style="color:blue">Frequentist Paradigm:</span>  

Data (e.g., $y$) are random variables that can be described by probability distributions with unknown parameters that (e.g., $\theta$) are *fixed* (scalars). 

<br>

. . .

<span style="color:blue">Bayesian Paradigm:</span>  

Data (e.g., $y$) are random variables that can be described by probability functions where the unknown parameters (e.g., $\theta$) are also random variables that have probability functions that describe them.

## Random Variables 
$$
\begin{align*}
y =& \text{ event/outcome} \\
f(y|\boldsymbol{\theta}) =& [y|\boldsymbol{\theta}]=  \text{ process governing the value of } y \\
\boldsymbol{\theta} =& \text{ parameters} \\
\end{align*}
$$

. . .

$f()$ or [ ] is conveying a function (math). 

. . .

It is called a PDF when $y$ is continuous and a PMF when $y$ is discrete.

-  PDF: **probability density function** 
-  PMF: **probability mass function** 


## Functions
We commonly use *deterministic* functions (indicated by non-italic letter); e.g., log(), exp(). Output is always the same with the same input.
$$ 
\hspace{-12pt}\text{g} \\
x \Longrightarrow\fbox{DO STUFF
} \Longrightarrow \text{g}(x)
$$ 

. . .

$$ 
\hspace{-14pt}\text{g} \\
x \Longrightarrow\fbox{+7
} \Longrightarrow \text{g}(x)
$$ 

. . .

$$ 
\text{g}(x) = x + 7
$$ 

. . .


## Random Variables

<span style="color:red">Probability</span>: Interested in $y$, the data, and the probability function that "generates" the data.
$$
\begin{align*}
y \leftarrow& f(y|\boldsymbol{\theta}) \\
\end{align*}
$$

. . .

<span style="color:blue">Statistics</span>: Interested in population characteristics of $y$; i.e., the parameters,

$$
\begin{align*}
y \rightarrow& f(y|\boldsymbol{\theta}) \\
\end{align*}
$$


## Probability Functions {.scrollable}

Special functions with rules to guarantee our logic of probabilities are maintained. 

. . .

### Discrete RVs {.numbered}

$y$ can only be a certain <span style="color:red">set</span> of values. 

::: {.incremental}

1.   $y \in  \{0,1\}$
      - 0 = dead, 1 = alive
2.   $y \in  \{0,1, 2\}$ 
      - 0 = site unoccupied, 1 = site occupied w/o young, 2 = site occupied with young
3.   $y \in  \{0, 1, 2, ..., 15\}$ 
      - count of pups in a litter; max could by physiological constraint
      
:::

::: {.fragment}

These sets are called the <span style="color:blue">sample space</span> ($\Omega$) or the <span style="color:blue">support</span> of the RV.

:::

## PMF {.scrollable}

$$
f(y) = P(Y=y)
$$

. . .

Data has two outcomes (0 = dead, 1 = alive) 

$y \in  \{0,1\}$

. . .

There are two probabilities

-   $f(0) = P(Y=0)$
-   $f(1) = P(Y=1)$

. . .

<span style="color:blue">Axiom 1:</span> The probability of an event is greater than or equal to zero and less than or equal to 1.


$$
0 \leq f(y) \leq 1
$$
Example,

-   $f(0) = 0.1$
-   $f(1) = 0.9$ 

. . .

<span style="color:blue">Axiom 2:</span> The sum of the probabilities of all possible values (sample space) is one. 

. . .

$$
\sum_{i} f(y_i) = f(y_1) + f(y_2) + ... = P(\Omega) =1
$$
Example, 

-   $f(0) + f(1) = 0.1 + 0.9 = 1$


## PMF

Still need to define $f()$, our PMF for $y \in  \{0,1\}$


. . .

The [Bernoulli distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution)

$$
f(y|\theta) = [y|\theta]=
  \begin{align}
        \theta^{y}\times(1-\theta)^{1-y}
  \end{align}
$$

::: {.fragment}
$\theta$ = P(Y = 1) = 0.2 
:::

::: {.fragment}
$$
f(y|\theta) = [y|\theta]=
  \begin{align}
        = 0.2^{1}\times(1-0.2)^{0-0} 
  \end{align}
$$
:::


::: {.fragment}
$$
f(y|\theta) = [y|\theta]=
  \begin{align}
        = 0.2 \times (0.8)^{0} = 0.2
  \end{align}
$$
:::


## Bernoulli PMF

$$
f(y|\theta) = [y|\theta]=
  \begin{align}
        \theta^{y}\times(1-\theta)^{1-y}
  \end{align}
$$


Sample space support ($\Omega$):

-   $y \in  \{0,1\}$


::: {.fragment}
Parameter space support ($\Theta$):

-   $\theta \in  [0,1]$
-   General: $\theta \in \Theta$

::: 

## Bernoulli PMF (Code)

What would our data look like for 10 ducks that had a probability of survival (Y=1) of 0.20?

```{r bern, echo=TRUE, eval=TRUE}
#Define inputs
  theta=0.2;  N=1 

#Random sample - 1 duck
  rbinom(n=1,size=N,theta)

#Random sample - 10 ducks
  rbinom(n=10,size=N,theta)
```

## **Why is this useful to us?**

How about to evaluate the sample size of ducks needed to estimate $\theta$?

. . .

```{r bern2, echo=TRUE, eval=TRUE}
y.mat = replicate(1000,rbinom(n = 10,size=N,theta))
theta.hat = apply(y.mat, 2, mean)
```

```{r bern3, echo=FALSE, eval=TRUE,  out.width="75%"}
hist(theta.hat,freq=TRUE,breaks=40, main=bquote("Sampling Distribution of"~theta),xlab=expression(theta))
```



## Binomial PMF

The Bernoulli is a special case of the [Binomial Distribution](https://en.wikipedia.org/wiki/Binomial_distribution). 

$$
f(y|\theta) = [y|\theta]=
  \begin{align}
        {N\choose y} \theta^{y}\times(1-\theta)^{N-y}
  \end{align}
$$

::: {.fragment}

$N$ = total trials / tagged and released animals

:::

<br>

::: {.fragment}

$y$ = number of successes / number of alive animals at the of the study.

:::

## Binomial PMF (Code) {.scrollable}


```{r eval=TRUE, echo=TRUE}
# 1 duck tagged/released and one simulation
  theta=0.2;  N=1 
  rbinom(n=1,size=N,theta)
```



::: {.fragment}

```{r eval=TRUE, echo=TRUE}
# 1000 ducks tagged/released and one simulation
  theta=0.2;  N=1000 
  rbinom(n=1,size=N,theta)
```

:::

::: {.fragment}

```{r eval=TRUE, echo=TRUE}
# 1000 ducks tagged/released and 10 simulation
  theta=0.2;  N=1000 
  rbinom(n=10,size=N,theta)
```

:::


::: {.fragment}

```{r eval=TRUE, echo=TRUE}
# 1 duck tagged for each of 1000 simulations
  theta=0.2;  N=1
  y = rbinom(n=1000,size=N,theta)
  y
```

:::


::: {.fragment}


```{r eval=TRUE, echo=TRUE}
sum(y)
```


:::




## Support 

Use a probability function that makes sense for your data/RV. In Bayesian infernece, we also pick prob. functions that make sense for parameters.

<br>

The sample space and parameter support can be found on Wikipedia for many probability functions. 



## Normal PDF 

For example, the [Normal/Gaussian distribution ](https://en.wikipedia.org/wiki/Normal_distribution) describes the sample space for all values on the real number line. 

$$y \sim \text{Normal}(\mu, \sigma) \\ y \in (-\infty, \infty) \\ y \in \mathbb{R}$$

What is the parameter space for $\mu$ and $\sigma$?

## Normal Distribution {.scrollable}

We collect data on adult alligator lengths (in). 
```{r,eval=TRUE,echo=FALSE}
shape=100
rate=1
scale=1/rate

set.seed(4454)
round(rgamma(10,shape,rate),digits=2)

```

<br>

<p style="text-align:center">
<span style="color:blue">
Should we use the Normal Distribution <br>to estimate the mean? <br>
</span>
</p>

. . .

<p style="text-align:center">
Does the support of our data match <br>the support of the PDF?
</p>

. . .

<p style="text-align:center">
What PDF does?
</p>

. . .

```{r, echo=FALSE,eval=TRUE}
mu=shape*scale
var=shape*scale^2
curve(dgamma(x, shape=shape,rate=rate),xlim=c(0,200),lwd=4,xlab="y",
      ylab="dgamma(y,shape = 100, rate = 1",main="Equivalent means and variances")
curve(dnorm(x, mu,sqrt(var)),add=TRUE,col=2,lwd=4,lty=2)
legend("topright",legend = c("Gamma PDF (shape = 100, rate = 1)", "Normal PDF (mu = 100, var = 100)"),lwd=3,col=c(1,2))
```

<p style="text-align:center"> <span style="color:blue">Are they exactly the same?</span></p>

<br>

. . .

The issue is when the data are near 0, we might estimate non-sensical values (e.g. negative).

```{r, echo=FALSE,eval=TRUE}
shape=1
rate=1
scale=1/rate

mu=shape*scale
var=shape*scale^2
curve(dgamma(x, shape=shape,rate=rate),xlim=c(-5,5),lwd=4,xlab="y",
      ylab="dgamma(y,shape = 1, rate = 1",main="Equivalent means and variances")
curve(dnorm(x, mu,sqrt(var)),add=TRUE,col=2,lwd=4,lty=2)
legend("topright",legend = c("Gamma PDF (shape = 1, rate = 1)", "Normal PDF (mu = 1, var = 1)"),lwd=3,col=c(1,2))
```


## PDF

### Continuous RVs {.numbered}

$y$ are an uncountable set of values.

<br>

::: {.fragment}
Provide ecological data examples that match the support?
:::

::: incremental 

1. <span style="color:blue">Gamma</span>:  $y \in  (0,\infty)$
2. <span style="color:blue">Beta</span>:   $y \in  (0,1)$ 
3. <span style="color:blue">Continuous Uniform</span>:   $y \in  [a,b]$

:::


## PDF {.scrollable}

PDFs of continious RVs follow the same rules as PMFs.

#### Confusing Differences

::: {.fragment}

<span style="color:blue">Axiom 1:</span>

-   $f(y) \geq 0$


PDFs output <span style="color:orange">probability densities</span>, not probabilities.

:::

::: {.fragment}

<br>

<span style="color:blue">Axiom 2:</span>

-   Probs are the area b/w a lower and upper value of $y$; i.e, area under the curve

:::


::: {.fragment}

$$
y \sim \text{Normal}(\mu, \sigma) \\
f(y|\mu,\sigma ) = \frac{1}{\sigma\sqrt{2\pi}}e^{\frac{1}{2}(\frac{y-\mu}{\sigma})^{2}} \\
$$

:::

::: {.fragment}

```{r, echo=FALSE,eval=TRUE}
library(visualize)
```

```{r,echo=TRUE,eval=TRUE}
visualize.it(dist = 'norm', stat = c(100),
             list(mu = 100 , sd = 10), section = "upper")
```

:::


::: {.fragment}

```{r,echo=FALSE,eval=TRUE}
library(visualize)
visualize.it(dist = 'norm', stat = c(120),
             list(mu = 100 , sd = 10), section = "upper")
```

:::


::: {.fragment}

The math,

$$
\int_{120}^{\infty} f(y| \mu, \sigma)dy = P(120<Y<\infty)
$$
:::


::: {.fragment}

Read this as "the integral of the probability density function between 120 and infinity (on the left-hand side) is equal to the probability that the outcome of the random variable is between 120 and infinity (on the right-hand side)".

:::

<br>

::: {.fragment}

The code
```{r, echo=TRUE}
pnorm(120,mean=100,sd=10,lower.tail = FALSE)
```

:::

<br>

::: {.fragment}

Or, we could reverse the question. 
```{r, echo=TRUE}
qnorm(0.02275,100,10,lower.tail = FALSE)
```

::: 

<!-- How do we use quantiles/probabilities to justify 95\% confidencen intervals? -->

<!-- ```{r,echo=FALSE,eval=TRUE} -->
<!-- visualize.it(dist = 'norm', stat = c(-1.96,1.96), -->
<!--              list(mu = 0 , sd = 1), section = "tails") -->
<!-- ``` -->
<!-- . . . -->

<!-- Code as, -->

<!-- ```{r, echo=TRUE} -->

<!-- 0.975-0.025 -->

<!-- qnorm(0.025) -->

<!-- qnorm(0.975) -->
<!-- ``` -->


<!-- . . . -->

<!-- What is the probability of $y$ = 80?  -->

<!-- . . . -->

<!-- $$ -->
<!-- \begin{align*} -->
<!-- &\int_{80}^{80} f(y|\mu=100, \sigma=10) dy\\ -->
<!-- \end{align*} -->
<!-- $$ -->
<!-- . . . -->

<!-- A definite integral of a continuous function is the limit of a Riemann sum as the number of subdivisions (n) approaches infinity.  -->

<!-- $$ -->
<!-- \begin{align*} -->
<!-- &= \lim_{n \to \infty} \sum_{i=1}^{n} \Delta y_i \times f(y_{i}|\mu=100, \sigma=10) = 0\\ -->
<!-- &\text{where } \Delta y = \frac{80-80}{n} -->
<!-- \end{align*} -->
<!-- $$ -->


## PDF {.scrollable}

<span style="color:blue">Axiom 3:</span>

-   $\int_{\text{lower support}}^{\text{upper suppport}}f(y)dy = 1$

The sum of the probability densities of all possible outcomes is equal to 1.


## Normal Distribution (PDF Code)

```{r pdf1, echo=TRUE, eval=TRUE}
y = rnorm(1000, mean = 20, sd = 3)
hist(y,freq=FALSE,ylim=c(0,0.14))
lines(density(y),lwd=3,col=4)
```

## Normal Distribution (PDF Code)

```{r pdf1b, echo=TRUE, eval=TRUE}
curve(dnorm(x, mean= 20, sd = 3),
      xlim=c(0,40),lwd=3,col=2,ylab="Probability Density",xlab="y")
abline(v=20, lwd=3, col=1, lty=4)
```

## Normal Distribution (PDF Code)

```{r pdf2a, echo=TRUE, eval=FALSE}
curve(dnorm(x, mean = 10, sd = 3),xlim=c(0,40),lwd=4,col=3,add=TRUE)
```

```{r pdf2b, echo=FALSE, eval=TRUE}
curve(dnorm(x, mean = 20, sd = 3),xlim=c(0,40),lwd=3,col=2,ylab="Probability Density",xlab="y")
abline(v=20, lwd=3, col=1, lty=4)
curve(dnorm(x, mean= 10, sd = 3),xlim=c(0,40),lwd=4,col=3,add=TRUE)
```

## Moments

Properties of all probability functions.

::: {.fragment}

- 1${^{st}}$ moment is central tendency
- 2${^{nd}}$ moment is the dispersion
-  ...

:::

::: {.fragment}
**Normal Distribution**: parameters ($\mu$ and $\sigma$) are 1${^{st}}$ and 2${^{nd}}$ moments
:::


## Moments {.scrollable}

[**Gamma Distribution**](https://en.wikipedia.org/wiki/Gamma_distribution): parameters are not moments

#### Parameters 

Shape = $\alpha$, Rate = $\beta$ 


**OR** 

Shape = $\kappa$, Scale = $\theta$, where $\theta = \frac{1}{\beta}$

::: {.fragment}
**NOTE**: probability functions can have *Alternative Parameterizations*, such they have different parameters.
:::

::: {.fragment}
Moments are functions of these parameters: 

- mean = $\kappa\theta$ or $\frac{\alpha}{\beta}$

- var = $\kappa\theta^2$ or $\frac{\alpha}{\beta^2}$


:::


## Gamma Distribution {.scrollable}

<span style="color:red">Probability</span>:
Interested in the variation of y,
$$
\begin{align*}
y \leftarrow& f(y|\boldsymbol{\theta'}) \\
\end{align*}
$$

. . .


$$
\begin{align*}
\boldsymbol{\theta'}  =& \begin{matrix} [\kappa & \theta] \end{matrix} \\
f(y|\boldsymbol{\theta}') &= \text{Gamma(}\kappa, \theta) \\
\end{align*}
$$

. . .


$$
\begin{align*}
f(y|\boldsymbol{\theta}') &= \frac{1}{\Gamma(\kappa)\theta^{\kappa}}y^{\kappa-1} e^{-y/\theta} \\
\end{align*}
$$

. . .


[Sample/parameter Support:](https://en.wikipedia.org/wiki/Log-normal_distribution)

-   $y \in (0,\infty)$
-   $\kappa \in (0,\infty)$
-   $\theta \in (0,\infty)$


## Gamma Distribution (PDF Code)

[Gamma Wikipedia](https://en.wikipedia.org/wiki/Gamma_distribution)

```{r pdf3, echo=TRUE, eval=TRUE}
shape =10
scale = 2

mean1 = shape*scale
mean1

mode1 = (shape-1)*scale
mode1

stdev = sqrt(shape*scale^2)
stdev
```

## Gamma Distribution (PDF Code)

```{r pdf4, echo=FALSE, eval=TRUE}
curve(dgamma(x, shape = shape, scale=scale),xlim=c(0,50),lwd=3,col=2,ylab="Probability Density",xlab="y")
abline(v=mean1, lwd=3, col=1, lty=4); abline(v=mode1, lwd=3, col=3, lty=4)
legend("topright",lty=3, col=c(1,2),legend=c("Mean","Mode"),lwd=3)
```


## Gamma Distribution {.scrollable}




What is the probability we would sample a value >40? <br>
In this population, how common is a value >40?

. . .


$$
\begin{align*}
p(y>40) = \int_{40}^{\infty} f(y|\boldsymbol{\theta}) \,dy 
\end{align*}
$$


```{r,eval=TRUE, echo=TRUE}
pgamma(q=40, shape=10, scale=2,lower.tail=FALSE)
```

<br>

. . .


What is the probability of observing $y$ < 20

```{r,eval=TRUE, echo=TRUE}
pgamma(q=20,shape=10, scale=2,lower.tail=TRUE)
```

<br>

. . .


What is the probability of observing 20 < $y$ < 40

```{r,eval=TRUE, echo=TRUE}
pgamma(q=40,shape=10, scale=2,lower.tail=TRUE)-
pgamma(q=20,shape=10, scale=2,lower.tail=TRUE)
```

<br>

. . .



Reverse the question: What values of $y$ and lower have a probability of 0.025

```{r,eval=TRUE, echo=TRUE}
qgamma(p=0.025,shape=10, scale=2,lower.tail=TRUE)
```

<br>

. . .

What values of $y$ and higher have a probability of 0.025

```{r,eval=TRUE, echo=TRUE}
qgamma(p=0.025,shape=10, scale=2,lower.tail=FALSE)
```

. . .

```{r,eval=TRUE, echo=TRUE}
curve(dgamma(x,shape=10, scale=2),xlim=c(0,50),lwd=3,
      xlab="y", ylab="dgamma(x,shape=10, scale=2)")
abline(v=c(9.590777,34.16961),lwd=3,col=2)
```


. . .

We can consider samples from this population,

```{r,echo=TRUE, eval=TRUE}
set.seed(154434)
y <- rgamma(100, shape=10, scale=2)
```


```{r,echo=FALSE, eval=TRUE}
curve(dgamma(x,shape=10, scale=2),xlim=c(0,50),lwd=3)
hist(y,col=adjustcolor("red",alpha.f = 0.5),freq=FALSE,add=TRUE,breaks=100)
```

  <!-- . . . -->
  
<!-- <p style="text-align:center"> -->
<!-- What use is this to us? -->
<!-- </p> -->




## The others side of the coin {.scrollable}
<span style="color:blue">Statistics</span>:
Interested in estimating population-level characteristics; i.e., the parameters

$$
\begin{align*}
y \rightarrow& f(y|\boldsymbol{\theta}) \\
\end{align*}
$$

. . .

<p style="text-align:center">
<span style="color:red">
**REMEMBER**
</span>
</p>
$f(y|\boldsymbol{\theta})$ is a probability statement about $y$, <span style="color:red"> **NOT** </span> $\boldsymbol{\theta}$.

<br>

