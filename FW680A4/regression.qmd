---
title: <span style="color:white">Linear Regression</span>
title-slide-attributes:
    data-background-image: /img/backgroundforest.png
format:
  revealjs:
    chalkboard: true
    multiplex: true
---

<!-- knitr::purl("./FW680A4/regression.qmd", output="./classfiles/regressionlab/regression.R") -->

## Objectives

-   fundamentals
-   assumptions
-   lm / glm functions
-   confidence intervals
-   case study

## 

```{=html}

<br>
<br>
<br>
<br>
<br>
<center><span style="color:#C70039; font-size: 65px;";><b>Fundamental Idea</b></span></center>
```

## Why model data?

In ecology, we use models to,

-   describe relationships among outcomes and processes
-   to estimate hidden (latent) processes
-   predict unobserved values
-   forecast future outcomes, such as responses to management

## Linear Regression (motivation)
```{r, echo=FALSE,eval=TRUE}
  n = 1000
  b0 = 10
  b1 = 2
  x = rnorm(n,40,10)
  mu = b0+b1*x
  y = rnorm(n,mu,sd=30)
  
  plot(x,y,xlim=c(0,70))
```

## Linear Regression (Equation)

![](/img/index.notation.png){fig-align="center" width="400"}

## Linear Regression (Equation 2) 

<br>

$$
y_{i} \sim \text{Normal}(\mu_{i}, \sigma)\\
\mu_{i} = \beta_0 + \beta_1 \times x_i \\
$$

## Linear Regression Line 1 {.scrollable}
$$
\hat{\mu_{i}} = 9.06 + 2\times x_i
$$

```{r,echo=FALSE,eval=TRUE, fig.align='center',width="1000"}
#" (beta0) = 9.06 \n Slope of x (beta1) = 2.0")
plot(x,y,cex.lab=1.3,cex.axis=1.3, main = expression(paste("Intercept (",hat(beta[0]),") = 9.06, Slope of x (",hat(beta[1]),") = 2.0")),xlim=c(0,80))

lm.out=lm(y~x)
abline(lm.out,lwd=3,col=3)
text(x=0,y=9.06,expression(paste(beta[0])),cex=2)
legend("topleft",col=c("green"),lty=2,legend=c("Mean"),lwd=4)

```

## Linear Regression Line 2 
$$
\hat{\mu_{i}} = 9.06 + 2\times x_i
$$

```{r,echo=FALSE,eval=TRUE, fig.align='center',width="1000"}
plot(x,y,cex.lab=1.3,cex.axis=1.3, main = expression(paste("Intercept (",hat(beta[0]),") = 9.06, Slope of x (",hat(beta[1]),") = 2.0")),xlim=c(0,80))
lm.out=lm(y~x)

lines(x,predict(lm.out),lwd=3,col=3)

newx = seq(min(x),max(x),by = 0.05)
conf_interval <- predict(lm.out, newdata=data.frame(x=newx), interval="confidence", level = 0.95)

matlines(newx, conf_interval[,2:3], col = "blue", lty=2,lwd=4)

legend("topleft",col=c("green","blue"),lty=2,legend=c("Mean","95% Confidence Interaval (mean)"),lwd=4)

```

## Linear Regression Line 3 
$$
\hat{\mu_{i}} = 9.06 + 2\times x_i
$$

```{r,echo=FALSE,eval=TRUE, fig.align='center',width="1000"}
plot(x,y,cex.lab=1.3,cex.axis=1.3, main = expression(paste("Intercept (",hat(beta[0]),") = 9.06, Slope of x (",hat(beta[1]),") = 2.0")),xlim=c(0,80))
lm.out=lm(y~x)

lines(x,predict(lm.out),lwd=3,col=3)

newx = seq(min(x),max(x),by = 0.05)
conf_interval <- predict(lm.out, newdata=data.frame(x=newx), interval="confidence", level = 0.95)

pred_interval <- predict(lm.out, newdata=data.frame(x=newx), interval="prediction", level = 0.95)


matlines(newx, conf_interval[,2:3], col = "blue", lty=2,lwd=4)

matlines(newx, pred_interval[,2:3], col = "purple", lty=2,lwd=4)

legend("topleft",col=c("green","blue","purple"),lty=2,legend=c("Mean","95% Confidence Interaval (mean)", "95% Prediction Interval (data)"),lwd=4)

```
## Residuals

```{r}
library(ggplot2)
library(ggthemes)
m1=broom::augment(lm.out)

ggplot(m1, aes(x, y)) +
  geom_point() +
  stat_smooth(method = lm, se = FALSE,linewidth=2) +
  geom_segment(aes(xend = x, yend = .fitted), color = "red", size = 0.3)+theme_base()

```


<!-- ## Visual -->
<!-- ![](/img/regression.png){fig-align="center" width="450"} -->


## One Sample

$$
\begin{align*}
y_{i} \sim& \text{Normal}(\mu_{i},\sigma) \\
\mu_{i} =& \beta_{0} + \beta_{1}x_{i}\\
\mu_{i} =& 1 + 0.5 \times x_{i}
\end{align*}
$$



```{r, eval=TRUE, echo=FALSE,fig.align='center'}
# x variable (independent and known)
  x = seq(0,2,by=0.1)

# marginal coefficients 
  beta0 = 1
  beta1 = 0.5

# sd of y
  sigma = 0.5

# Derive mu  
  mu = beta0 + beta1*x

# Single random realization of y, 
#  based on mu and sigma  
  set.seed(43252)
  y =  rnorm(length(mu),mean = mu, sd = sigma)
  
#plotting
  par(cex.axis=1.3,cex.lab=1.3)
  plot(x,y,type="p",col=4,lwd=4,ylim=c(-2,4),
       pch=18,cex=2)
  lines(x,mu,lwd=3)

```


## Sampling Distributions of y

$$
\begin{align*}
\mu_{i} = 1 + 0.5 \times x_{i}
\end{align*}
$$

```{r, eval=TRUE, echo=FALSE, cache=TRUE}
#sample many times  
  y.many = replicate(1000,rnorm(length(mu),mu, sigma))
#  dim(y.many)
```


```{r, eval=TRUE, echo=FALSE}
#plot all samples with true mu
  par(cex.axis=1.3,cex.lab=1.3)
  matplot(y.many,type="p",pch=18,xaxt="n",col=2,cex=2,xlab="x",ylab="y",ylim=c(-2,4))
  axis(1,at=1:length(x),lab=x)
  lines(1:length(x),mu,lwd=3)
  points(1:length(x),y,type="p",col=4,lwd=4,ylim=c(-2,3),
       pch=18,cex=2)
  legend("bottomright",lwd=2, col=c(1,4,2),legend=c("Truth","Sample","Sampling Distributions"))
```


## Sampling Distributions of $\hat{\mu}$

$$
\begin{align*}
\mu_{i} = 1 + 0.5 \times x_{i}
\end{align*}
$$

```{r, eval=TRUE, echo=FALSE}
temp.x=x
preds.many = apply(y.many,2,FUN=function(x){
                    predict(lm(x~temp.x)
                   )
})

matplot(preds.many,type="l",xaxt="n",col=grDevices::adjustcolor("black",alpha.f = 0.05),
        lwd=3,
        ylim=c(0,3),xlab="x",ylab=expression(paste(hat(mu))))
        axis(1,at=1:length(x))
lines(1:length(x),mu,type="l",lwd=4,col=2)
legend("bottomright",lwd=3, col=c(2,grDevices::adjustcolor("black",alpha.f = 0.05)),legend=c("Truth", "Estimate"))

```



## 

```{=html}

<br>
<br>
<br>
<br>
<br>
<center><span style="color:#C70039; font-size: 65px;";><b>Assumptions</b></span></center>
```


## Assumptions
#### Independence of the errors


<br>

Correlation($\epsilon_{i}$,$\epsilon_{j}$) = 0, $\forall$ pairs of $i$ and $j$

<br>

This means that knowing how far observation  $i$ will be from the true regression line tells us nothing about how far observation  $j$  will be from the regression line.

## Assumptions {.scrollable}

#### Homogeniety of the variance

var($\epsilon_{i}$) = $\sigma^2$

Constancy in the scatter of observations above and below the line, going left to right.


:::{.fragment}
```{r,eval=TRUE,echo=FALSE, width = 200}

n = 1000
b0 = 10
b1 = 2
x = rnorm(n,40,10)
mu = b0+b1*x
y = rnorm(n,mu,sd=30)

plot(x,y,xlim=c(0,70))

```
:::

## Assumptions

#### Heteroskedasticity


```{r,eval=TRUE,echo=FALSE,fig.align='center'}
n = 1000
b0 = 10
b1 = 20
x = rnorm(n,40,10)
mu = b0+b1*x

sigma =5*x

y = rnorm(n,mu,sd=sigma)

plot(x,y,xlim=c(0,70))

```


## Assumptions

#### Linearity

<br>
$$
E[y_i|x_i] = \mu_i = \beta_0 + \beta_1 \times x_i \\
$$
<br>

The hypothesis about the variables included in the model (e.g., $x_i$) characterizes the mean well. 

## Assumptions
#### Normality

<br>
$$
\epsilon_i \sim \text{Normal}(0,\sigma)
$$
<br>

Each $i^{th}$ residual 

::: {.incremental}

-   comes from a Normal distribution with a  mean of zero
-   is symmetrically disributed around zero 
-   varies around zero by $\sigma$, which is the same for each residual. 
:::

## Assumption Violations

### Robustness

<br>


Linearity and constant variance are often more important than the assumption of normality (see e.g., [Knief & Forstmeier, 2021](https://link.springer.com/article/10.3758/s13428-021-01587-5) and references therein)


<br>

::: {.fragment}

<p style="text-align:center">
<b>This is especially true for large sample sizes</b>
</p>

:::

## 

```{=html}

<br>
<br>
<br>
<br>
<br>
<center><span style="color:#C70039; font-size: 65px;";><b>lm and glm </b></span></center>
```


## Intercept-Only Model

$$
y_{i} \sim \text{Normal}(\mu_{i}, \sigma^2)\\
\mu_{i} = \beta_0
$$

::: {.fragment}

#### Generate Data

```{r,eval=TRUE,echo=TRUE,fig.align='center'}
#Setup parameters
  n = 100 # sample size
  beta0 = 10 # true mean
  sigma = 2 # true std.dev

# Simulate a data set of observations
  set.seed(43243)
  y = rnorm(n, mean = beta0, sd = sigma)
```  

:::

## Visualize Intercept-Only Model

```{r,eval=TRUE,echo=TRUE,fig.align='center'}
  hist(y)
```  

## Fit Intercept-Only Model 

```{r,eval=TRUE,echo=TRUE}
# Fit model/hypothesis using maximum likelihood
  model1.0 = lm(y~1)

  model1.1 = glm(y~1)

  model1.2 = glm(y~1, family=gaussian(link = identity))

  
  
# Compare Results  
  data.frame(intercept=c(model1.0$coefficients,model1.1$coefficients, model1.2$coefficients),
             SE = c(summary(model1.0)$coefficients[, 2], summary(model1.1)$coefficients[, 2],summary(model1.2)$coefficients[, 2])
            )
```

## Fit Intercept-Only Model 

```{r,eval=TRUE,echo=TRUE}
    
# Summary of model results  
  summary(model1.0)
```


## Fitted-values {.scrollable}

```{r,eval=TRUE,echo=TRUE}
#Predict response for all data
  preds = predict(model1.0, se.fit = TRUE)
  preds
```  

## 

```{=html}

<br>
<br>
<br>
<br>
<br>
<center><span style="color:#C70039; font-size: 65px;";><b>Confidence Intervals </b></span></center>
```


## Confidence Intervals 
#### Normal Approximation

```{r,eval=TRUE,echo=TRUE}
# Get 90% confidence intervals (Type I error = 0.1)
  c(
    (preds$fit+preds$se.fit*qnorm(0.05))[1],
    (preds$fit+preds$se.fit*qnorm(0.95))[1]
   )
```

:::{.fragment}
```{r,eval=TRUE,echo=TRUE}
  CI.Normal=confint(model1.0, level=0.9)
  CI.Normal
```
:::  
  


## What is a CI?

. . .

"A confidence interval for a parameter is an interval computed using sample data ...

. . .

"... by a method that will contain the parameter for a specified proportion of all samples.

. . .

The success rate (proportion of all samples whose intervals contain the parameter) is known as the confidence level." [R. H. Lock et al. (2020)](https://www.wileyplus.com/math-and-statistics/lock-statistics-unlocking-the-power-of-data-3e-eprof20502/)

## What is a CI?

#### Key

-   the parameter we are trying to estimate is a fixed unknown (i.e., it is not varying across samples)


:::{fragment}
-   the endpoints of our confidence interval are random and will change every time we collect a new data set (the endpoints themselves actually have a sampling distribution!)
:::

[More at Stats4Ecologists](https://statistics4ecologists-v2.netlify.app/linreg#confidence-intervals)


## What is a CI?

![](/img/CI.png){fig-align="center"}

## Bootstrapping 


Instead of relying on the 95% intervals from an assumed normal distribution, we will create a distribution by 
resampling our data.

<br>

See, [Stats4Ecologists](https://fw8051statistics4ecologists.netlify.app/boot.html)   

## Bootstrapping (idea)
![](/img/fieberg0.png){fig-align="center" width="450"}

## Bootstrapping (idea)

![](/img/fieberg1.png){fig-align="center" width="450"}

## Bootstrapping (idea)
![](/img/fieberg3.png){fig-align="center" width="450"}

## Bootstrapping (code)

```{r, echo=TRUE, include=TRUE, eval=TRUE}

# Setup
  nboot <- 1000 # number of bootstrap samples
  nobs <- length(y)
  bootcoefs <- rep(NA, nboot)
# Start loop  
for(i in 1:nboot){
  set.seed(43243+i)
  # Create bootstrap data set by sampling original observations w/ replacement  
  bootdat <- y[sample(1:nobs, nobs, replace=TRUE)] 
  # Calculate bootstrap statistic
  glmboot <- glm(bootdat ~ 1)
  bootcoefs[i] <- coef(glmboot)
}
```  
## Bootstrapping (code)
```{r}
par(mfrow = c(1, 1))
hist(bootcoefs, main = expression(paste("Bootstrap distribution of ", hat(beta)[0])), xlab = "")
```

## Bootstrapping (code)
```{r, echo=TRUE, include=TRUE, eval=TRUE}
# Calculate bootstrap standard errors
  boot.se = sd(bootcoefs)

# boostrap-normal CI
  boot.normal = c(
        (preds$fit+boot.se*qnorm(0.05))[1],
        (preds$fit+boot.se*qnorm(0.95))[1]
        )

# bootstrap percentile
confdat.boot.pct <- quantile(bootcoefs, probs = c(0.05, 0.95))

```

## Comparison

```{r, eval=TRUE, echo=FALSE}
confdata <- data.frame(LCL=c(boot.normal[1],CI.Normal[1],confdat.boot.pct[1]),
           UCL=c(boot.normal[2],CI.Normal[2],confdat.boot.pct[2]),
           method=c("Normal Assumption", "Bootstrap-Normal", "Bootstrap-percentile")
          )
confdata$estimate <- rep(coef(model1.0),3)
library(ggplot2)
ggplot(confdata, aes(y = estimate, x = " ", col = method)) + 
  geom_point() +
  geom_pointrange(aes(ymin = LCL, ymax = UCL, size = 0.1),linewidth = 3,  
                  position = position_dodge(width = 0.9))

```



## 

```{=html}

<br>
<br>
<br>
<br>
<br>
<center><span style="color:#C70039; font-size: 65px;";><b>Independent Variables</b></span></center>
```


## Prairie Dog Calling {.scrollable}

```{r, echo=FALSE, include=TRUE, eval=TRUE}  
n = 100
b0=1
b1=-0.01
b2=0.02
b3=0.0005

set.seed(543543)
x1 = seq(40,100,length.out=n)+rnorm(n,0,10)
x2 = seq(10,500,length.out=n)+rnorm(n,0,2)
sigma=2

mu = b0 + b1*x1 + b2*x1 + b3*(x1*x2)

set.seed(43441)
y = rnorm(n, mu,sigma)
y[which(y<0)]=0

dat = data.frame(y=y,
                 temp = x1,
                 dist.center = x2
                 )
```

![](/img/pdog.png){fig-align="center" width="1200"}

 y = # of calls per 5 minute
 
 temp = temperature, degrees F
 
 dist.center = location from center of colony

```{r, echo=TRUE, include=TRUE, eval=TRUE}  
  head(dat)
```


## Exploratory {.scrollable}

```{r, echo=FALSE, include=TRUE, eval=TRUE}  
  hist(dat$y)
  hist(dat$temp)
  hist(dat$dist.center)
```
## Model Fitting 1

```{r, echo=TRUE, include=TRUE, eval=TRUE}  
  model = lm(y ~ temp+dist.center, data=dat)
  summary(model)
```

## Model Notation 1

```{r, echo=TRUE, include=TRUE, eval=TRUE}  
equatiomatic::extract_eq(model)

equatiomatic::extract_eq(model, use_coefs = TRUE)
```


## Model Fitting 2

Mean-center the intercept. Slopes do not change.
```{r, echo=TRUE, include=TRUE, eval=TRUE}  
  model = lm(y ~ I(temp - mean(temp)) + dist.center, data=dat)
  summary(model)
```


## Normalizing x

Mean-center and standardize by std. deviation
```{r, echo=TRUE, include=TRUE, eval=TRUE}  
  dat$temp.sc = scale(dat$temp)
  dat$dist.sc = scale(dat$dist.center)
```

:::{.fragment}

```{r, echo=TRUE, include=TRUE, eval=TRUE}  
  mean(dat$temp.sc)
  sd(dat$temp.sc)
```

:::

## Comparison

```{r, echo=FALSE, include=TRUE, eval=TRUE}  
  par(mfrow=c(1,2))
  hist(dat$temp)
  hist(dat$temp.sc)
  
```


## Model Fitting 3 {.scrollable}

What does the slope mean now?

$$
\mu_{i} = \beta_0 + \beta_1 \times 1 + \beta_2 \times 0
$$
```{r, echo=TRUE, include=TRUE, eval=TRUE}  
  model = lm(y ~ temp.sc + dist.sc, data=dat)
  summary(model)
```




## Marginal Predictions

$$
\hat{\mu_{i}} = \hat{\beta_{0}} + \hat{\beta_{1}}\times 0 +  \hat{\beta_{2}} \times \text{dist.center}_i
$$

```{r, echo=FALSE,eval=TRUE}
model = lm(y ~ I(temp - mean(temp)) + dist.center, data=dat)
```

```{r, echo=TRUE, include=TRUE, eval=TRUE}  
# Plot marginal predictions of dist.center
  marginaleffects::plot_predictions(model, condition=c("dist.center"))
```

## Add data points
```{r, echo=TRUE, include=TRUE, eval=TRUE}  
# Plot marginal predictions of dist.center
  plot1 = marginaleffects::plot_predictions(model, condition=c("dist.center"))
  plot1+geom_point(data=dat, aes(x=dist.center,y=y))
```


## Combined Predictions

`````{r, echo=TRUE, include=TRUE, eval=TRUE}  
marginaleffects::plot_predictions(model, condition=list("temp","dist.center"))
`````

## Combined Predictions

`````{r, echo=TRUE, include=TRUE, eval=TRUE}  
marginaleffects::plot_predictions(model, condition=list("temp","dist.center" = 0:5))
`````



## Taking Control 

```{r, echo=TRUE, include=TRUE, eval=TRUE}  
newdata = expand.grid(dat$temp,0:5)
colnames(newdata)=c("temp","dist.center")

preds = predict(model,newdata = newdata,interval="confidence", level=0.95)

pred.plot = data.frame(newdata,preds)
```

## Combined Predictions 2 {.scrollable}
```{r, echo=FALSE, include=TRUE, eval=TRUE}  

ggplot(data=pred.plot) +
  geom_line(aes(temp, fit, group = dist.center,color = dist.center), linewidth = 1.1)+ 
  geom_ribbon(aes(x = temp, ymax = upr, ymin = lwr, group = dist.center, color = dist.center), alpha = 0.3) +
  theme_bw()
                       
```




## Evaluating Assumptions
### Largely done based on the residuals

$y_{i} - \hat{y}_{i}$

```{r, echo=TRUE, include=TRUE, eval=TRUE,fig.align='center'}  
hist(model$residuals)
```


<!-- ## [Linearity & Variance Assumption](https://fw8051statistics4ecologists.netlify.app/linreg.html#exploring-assumptions-using-r) -->

<!-- ![](/img/assumptions2.png){fig-align="center" width="1000"} -->


## Linearity Assumption

```{r, echo=TRUE, include=TRUE, eval=TRUE,fig.align='center'}  
plot(model,1)
```
Ideally, there will be no pattern and the red line should be roughly horizontal near zero.


## Homogeneity of Variance

```{r, echo=TRUE, include=TRUE, eval=TRUE,fig.align='center'}  
plot(model,3)
```

Residuals should be spread equally along the ranges of predictors. We want a horizontal red line; otherwise, suggests a non-constant variances in the residuals (i.e., heteroscedasticity).


## Normality of Residuals

```{r, echo=TRUE, include=TRUE, eval=TRUE,fig.align='center'}  
plot(model,2)
```

Shows theoretical quantiles versus empirical quantiles of the residuals. We want to see cicles on the dotted line.

## Outliers

```{r, echo=TRUE, include=TRUE, eval=TRUE,fig.align='center'}  
par(mfrow=c(1,2))
plot(model,4)
plot(model,5)
```

Outlier: extreme value that can affect the $\beta$ estimate. Leverage plot: points in the upper right and lower right corner. 

## Exploring Assumptions {.scrollable}

Nicer looking Plots
```{r, echo=TRUE, include=TRUE, eval=TRUE}  
library(ggResidpanel)
resid_panel(model)
```

## Go to Lab
