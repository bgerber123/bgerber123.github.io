---
title: <span style="color:black">Likelihood / Optimization</span>
title-slide-attributes:
    data-background-image: /img/likelihood.png
format:
  revealjs:
    chalkboard: true
    multiplex: true
---



## Objectives

<!-- knitr::purl("./FW680A4/likelihood.qmd", output="./FW680A4/likelihood.R") -->

- likelihood principle
- likelihood connection to probability function
- optimization / parameter estimation 

## The others side of the coin {.scrollable}
<span style="color:blue">Statistics</span>:
Interested in estimating population-level characteristics; i.e., the parameters

$$
\begin{align*}
y \rightarrow& f(y|\boldsymbol{\theta}) \\
\end{align*}
$$

::: {.fragment}

#### Estimation
  
  - likelihood
  - Bayesian
  
:::

## Likelihood  

  [Likelihood principle](https://en.wikipedia.org/wiki/Likelihood_principle)
  
  All the evidence/information in a sample ($\textbf{y}$, i.e., data) relevant to making inference on model parameters ($\theta$) is contained in the *likelihood function*.
  
  
## The pieces
  
  -   The sample data, $\textbf{y}$
  
  -   A probability function for $\textbf{y}$:
  
      -   $f(\textbf{y};\theta)$ or $[\textbf{y}|\theta]$ or $P(\textbf{y}|\theta)$
  
      -   the unknown parameter(s) ($\theta$) of the probability function
  

## The Likelihood Function {.scrollable}


$$
\begin{align*}
\mathcal{L}(\boldsymbol{\theta}|y) = P(y|\boldsymbol{\theta})  = f(y|\boldsymbol{\theta})
\end{align*}
$$

. . .

The likelihood ($\mathcal{L}$) of the unknown parameters, given our data, can be calculated using our probability function. 

## The Likelihood Function {.scrollable}

For example, for $y_{1} \sim \text{Normal}(\mu,\sigma)$

CODE:
```{r,eval=TRUE,echo=TRUE}
# A data point
  y = c(10)

#the likelihood the mean is 8, given our data
  dnorm(y, mean = 8)
```  

<br>

. . .

If we knew the mean is truly 8, it would also be the probability density of the observation y = 10. But, we don't know what the mean truly is.

<br>

. . . 


The **key** is to understand that the likelihood values are relative, which means we need many guesses.


<br>

. . . 



```{r,eval=TRUE,echo=TRUE}
#the likelihood the mean is 9, given our data
  dnorm(y, mean = 9)
```  



## Optimization 

#### Grid Search

```{r,eval=TRUE,echo=TRUE}
  means = seq(0, 20,by = 0.1) # many guesses of the mean
  likelihood = dnorm(y, mean = means, sd = 1) # likelihood of each guess of the mean
```

::: {.fragment}

```{r,eval=TRUE,echo=FALSE, fig.align='center'}
#Look at guesses and likelihood
  plot(means,likelihood,xlab="Guesses for the Mean")
  abline(v=10,lwd=3,col=3)
  legend("topright",legend=c("Max Likelihood"),
         lwd=3,col=3)
```

:::

<!-- ## Double Check -->

<!-- ```{r,eval=TRUE,echo=TRUE} -->
<!--   means = seq(0,20,by=0.1) -->
<!--   likelihood = dnorm(y, mean=means, sd=1) -->
<!--   sum(likelihood*0.1) -->
<!-- ``` -->

<!-- ::: {.fragment} -->

<!-- ```{r,eval=TRUE,echo=TRUE} -->
<!--   means = seq(0,20,by=0.01) -->
<!--   likelihood = dnorm(y, mean=means, sd=1) -->
<!--   sum(likelihood*0.1) -->
<!--   sum(likelihood*0.01) -->
<!-- ``` -->


<!-- ::: -->

## Maximum Likelihood Properties {.scrollablew}

::: {.incremental}

- Central Tenet: evidence is relative.  

- Parameters are not RVs. They are not defined by a PDF/PMF.

- MLEs are <span style="color:red">consistent</span>. As sample size increases, they will converge to the true parameter value.

- MLEs are <span style="color:red">asymptotically unbiased</span>. The $E[\hat{\theta}]$ converges to $\theta$ as the sample size gets larger.

- No guarantee that MLE is unbiased at small sample size. Can be tested!

- MLEs will have the minimum variance among all estimators, as the sample size gets larger.

:::

## MLE with n > 1

What is the mean height of King Penguins? 

![](/img/penguin.png){fig-align="center" width="483"}


## MLE with n > 1 {.scrollable}

We go and collect data,

$\boldsymbol{y} = \begin{matrix} [4.34 & 3.53 & 3.75] \end{matrix}$

<br>

. . .

Let's decide to use the Normal Distribution as our PDF. 


::: {.fragment}
$$
\begin{align*}
f(y_1 = 4.34|\mu,\sigma)  &= \frac{1}{\sigma\sqrt(2\pi)}e^{-\frac{1}{2}(\frac{y_{1}-\mu}{\sigma})^2} \\
\end{align*}
$$

:::

::: {.fragment}
<span style="color:red">AND</span>

$$
\begin{align*}
f(y_2 = 3.53|\mu,\sigma)  &= \frac{1}{\sigma\sqrt(2\pi)}e^{-\frac{1}{2}(\frac{y_{2}-\mu}{\sigma})^2} \\
\end{align*}
$$
:::

::: {.fragment}
<span style="color:red">AND</span>

$$
\begin{align*}
f(y_3 = 3.75|\mu,\sigma)  &= \frac{1}{\sigma\sqrt(2\pi)}e^{-\frac{1}{2}(\frac{y_{3}-\mu}{\sigma})^2} \\
\end{align*}
$$

::: 

## Need to connect data together {.scrollable}

**Or simply**, 

$$
\textbf{y} \stackrel{iid}{\sim} \text{Normal}(\mu, \sigma)
$$

$iid$ = independent and identically distributed


## Need to connect data together

The joint probability of our data with shared parameters $\mu$ and $\sigma$,

$$
\begin{align*}
& P(Y_{1} = y_1,Y_{2} = y_2, Y_{3} = y_3 | \mu, \sigma) \\
\end{align*}
$$

::: {.fragment}

$$
\begin{align*}
& P(Y_{1} = 4.34,Y_{2} = 3.53, Y_{3} = 3.75 | \mu, \sigma) \\
&= \mathcal{L}(\mu, \sigma|\textbf{y})
\end{align*}
$$

:::
## Need to connect data together

<span style="color:red">IF</span> each $y_{i}$ is **independent**, the *joint probability* of our data are simply the multiplication of all three probability densities,

$$
\begin{align*}
=& f(4.34|\mu, \sigma)\times f(3.53|\mu, \sigma)\times f(3.75|\mu, \sigma) \end{align*}
$$
$$
\begin{align*}
=& \prod_{i=1}^{3} f(y_{i}|\mu, \sigma) \\
=& \mathcal{L}(\mu, \sigma|y_{1},y_{2},y_{3})
\end{align*}
$$


## Conditional Independence Assumption

We can do this because we are assuming knowing one observation does not tell us any new information about another observation. Such that,

$P(y_{2}|y_{1}) = P(y_{2})$




## Code 

Translate the math to code...

```{r, echo=TRUE, eval=TRUE}
# penguin height data
  y = c(4.34, 3.53, 3.75)

# Joint likelihood of mu=3, sigma =1, given our data
  prod(dnorm(y, mean = 3,sd = 1))
```

## Optimization Code (Grid Search)

Calculate likelihood of many guesses of $\mu$ and $\sigma$ simultaneously,

```{r, echo=TRUE, eval=TRUE}
# The Guesses
  mu = seq(0,6,0.05)
  sigma = seq(0.01,2,0.05)
  try = expand.grid(mu,sigma)
  colnames(try) = c("mu","sigma")

# function
fun = function(a,b){
          prod(dnorm(y,mean = a, sd = b))
      }

# mapply the function with the inputs
  likelihood = mapply(a = try$mu, b = try$sigma, FUN=fun)
```

## MLE Code 

```{r, echo=TRUE, eval=TRUE}
# maximum likelihood of parameters
  try[which.max(likelihood),]
```

<br>

. . .

Lets compare to,
```{r, echo=TRUE, eval=TRUE}
sum(y)/length(y)
```

## Likelihood plot (3D) {.scrollable}

```{r,echo=FALSE,eval=TRUE, out.width="150%"}
library(plotly)
f <- list(
    size = 15,
    family = 'sans-serif'
  )
  m <- list(
    l = 2,
    r = 0,
    b = 2,
    t = 2,
    pad = 0
  )
all=cbind(try,likelihood)
colnames(all)=c("mu","sigma","likelihood")
fig <- plot_ly(all, x = ~mu, y = ~sigma, z = ~likelihood, marker = list(size = 5),width = 800, height = 800)
fig <- fig %>% add_markers(color=~likelihood)
fig <- fig %>% layout(scene = list(xaxis = list(title = 'mu'),
                     yaxis = list(title = 'sigma'),
                     zaxis = list(title = 'Likelihood')))
fig %>% layout(font = f, margin = m)

#fig
```


## Sample Size  {.scrollable}
What happens to the likelihood if we increase the sample size to N=100?

. . .

```{r,eval=TRUE,echo=FALSE}
set.seed(154541)
y=rnorm(100,3.8,1)

try=expand.grid(seq(0,6,0.01),seq(0.01,2,0.01))
  colnames(try)=c("mu","sigma")

# mapply the function with the inputs
  likelihood=mapply(try$mu,try$sigma, FUN=fun)

  library(plotly)
f <- list(
    size = 15,
    family = 'sans-serif'
  )
  m <- list(
    l = 2,
    r = 0,
    b = 2,
    t = 2,
    pad = 0
  )
all=cbind(try,likelihood)
colnames(all)=c("mu","sigma","likelihood")
fig <- plot_ly(all, x = ~mu, y = ~sigma, z = ~likelihood, marker = list(size = 5),width = 800, height = 800)
fig <- fig %>% add_markers(color=~likelihood)
fig <- fig %>% layout(scene = list(xaxis = list(title = 'Mean'),
                     yaxis = list(title = 'SD'),
                     zaxis = list(title = 'Likelihood')))
fig %>% layout(font = f, margin = m)

  
```

## Relativeness

#### log-likelihood
```{r,eval=TRUE,echo=TRUE}
fun.log = function(a,b){
              sum(dnorm(y,mean = a, sd = b, log=TRUE))
          }

log.likelihood = mapply(a = try$mu, b = try$sigma, FUN=fun.log)
  
# maximum log-likelihood of parameters
  try[which.max(log.likelihood),]

```

## Optimization Code (Numerical)  {.scrollable}

Let's let the computer do some smarter guessing, i.e., optimization.

```{r echo=TRUE}

# Note: optim function uses minimization, not maximization. 
# WE want to find the minimum negative log-likelihood
# THUS, need to put negative in our function

neg.log.likelihood=function(par){
  -sum(dnorm(y,mean=par[1],sd=par[2],log=TRUE))
  }

#find the values that minimizes the function
#c(1,1) are the initial values for mu and sigma
fit <- optim(par=c(1,1), fn=neg.log.likelihood,
             method="L-BFGS-B",
             lower=c(0,0),upper=c(10,1)
             )

#Maximum likelihood estimates for mu and sigma
fit$par
```

## Linear Regression

King Penguin Height Data (N=100)

```{r echo=TRUE}
out = lm(y~1)
summary(out)
```

