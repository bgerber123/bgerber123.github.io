  ## Statistical Information {.scrollable}
  
  Let's turn to the field of statistics to understand *Information*
  
  
  <br>
  
  . . .
  
  [Likelihood principle](https://en.wikipedia.org/wiki/Likelihood_principle)
  
  Given a *statistical model*, all the evidence/information in a sample ($\textbf{y}$, i.e., data) relevant to model parameters ($\theta$) is contained in the *likelihood function*.
  
  . . .
  
  [Fisher Information](https://en.wikipedia.org/wiki/Fisher_information)
  
  The information an observable random variable ($\textbf{y}$) has about an unknown parameter $\theta$ upon which the probability of $\textbf{y}$ $(f(\textbf{y};\theta)$ depends.
  
  <br>
  
  . . .
  
  ```{=html}
  <br><span style="color:#FF0000";><center><em>Information is conditional</center></em></span><br> 
  ```
  . . .
  
  To learn about $\theta$ from $\textbf{y}$, we need to link them together via a special function, $f(\textbf{y};\theta)$
  
  ## Statistical Information
  
  <br> The pieces:
  
  ::: incremental
  -   The sample data, $\textbf{y}$
  
  -   A probability function for $\textbf{y}$:
  
      -   $f(\textbf{y};\theta)$
  
      -   $[\textbf{y}|\theta]$
  
  
  
  -   The unknown parameter: $\theta$
  
      -   specified in the probability function
  
  :::
  
  ## Statistical Information (an example)
  
  We want to know the proportion of a wetlands that contain a rare plant species. We can not sample the whole area.
  
  ![](Information_files/wetlands.png){fig-align="center" width="494"}
  
  ## Rare Plant Data
  
  We randomly select plots and look for our plant.\
  Our data are
  
  ```{r, echo = TRUE}
  #| echo: TRUE
  #| eval: TRUE
  #| code-line-numbers: "1|2,3"
  y <- c(0,1,1,1,1,0,1,0,0,0)
  n <- length(y)
  n
  ```
  
  . . .
  
  <br>
  
  $\textbf{y}$ is a random variable as it depends on random events.
  
  <br>
  
  . . .
  
  In this case, when we induced a random selection of sites.
  
  ## Probability/Likelihood Function
  
  $f(\textbf{y};\theta)$ describes the probability for each $i^{th}$ data, $y_i$. 
  
  But, not just for the data we observed, for all possible data that could be observed.
  
  . . .
  
  **Rules about our data:** <br> $y \in \{0,1\}$,
  
  . . .
  
  ```{=html}
  <p style="font-size:25px">
  where the curly brackets imply discrete values in our "set".
  ```
  . . .
  
  **Not this:** $y \in [0,1]$,
  
  ```{=html}
  <p style="font-size:25px">
  where the square brackets indicate all real numbers from 0 to 1, including 0 and 1.
  ```
  
  . . .
  
  **Not this:** $y \in (0,1)$,
  
  ```{=html}
  <p style="font-size:25px">
  where the parantheses indicate all real numbers from 0 to 1, not including 0 and 1.
  ```
  
  
  ## Probability Function {.scrollable}
  
  Two outcomes, 0 or 1, so we need two probabilities to describe our random variable.
  
  . . .
  
  $$
    f(y;\theta) = [y|\theta]= 
    \begin{cases}
      \theta     & \text{if $y = 1$}, \\
      1 - \theta & \text{if $y = 0$}.
    \end{cases}
  $$
  
  . . .
  
  Also,
  
  $$
  f(y;\theta) = [y|\theta]=
    \begin{align}
          \theta^{y}\times(1-\theta)^{1-y}
    \end{align}
  $$
  
  . . .
  
  Also,
  
  $$
    \begin{align}
          P(Y=1) &= \theta \\
          P(Y=0) &= 1-\theta  
    \end{align}
  $$
  
  ```{=html}
  <center>
  This probability function is called ________?
  </center>
  ```
  ## Probability Function
  
  Probabilities, such as our parameter, have rules:
  
  ::: incremental
  -   $0 \leq \theta \leq 1$
  
  -   $0 \leq (1 - \theta) \leq 1$
  
  -   $\theta + (1-\theta) = 1$
  
  :::
  
  ## Probability Function {.scrollable}
  
  How do we find $\theta$ for our data?
  
  . . .
  
  We can use our probability function to calculate the likelihood of a parameter, given our data.
  
  . . .
  
  $$
  \mathcal{L}(\theta|y) = \prod_{i=1}^{n} p(y_{i};\theta)
  $$
  
  . . .
  
  ```{r,echo=TRUE}
  #| echo: TRUE
  #| eval: TRUE
  #| code-line-numbers: 2|5|8|11|14|17|20
  
  #Bernoulli probability function
    prob.function=function(theta){prod(theta^y*(1-theta)^y)}
  
  #possible probabilities
    theta.guess=matrix(seq(0.01,0.99,by=0.01))
  
  #implement function
    likelihood=apply(theta.guess,1,prob.function)
    
  #Find maximum likelood
    max.index=which.max(likelihood)
  
  #Theta that maximizes our probability function
    theta.est=theta.guess[max.index]
  
  #Define other probability
    q.est <- 1-theta.est
  
  #Alternative estimation
    theta.est2 <- sum(y)/n
  
    
  ```
  
  . . .
  
  ```{r,echo=FALSE}
  #plot likelihood profile
    plot(theta.guess,likelihood)
    abline(v=theta.est,lwd=3,col=1,lty=1)
    abline(v=theta.est2,lwd=3,col=4,lty=4)
    legend("topright",lwd=3,col=c(1,4),
           legend=c("theta.est","theta.est2"))
  ```
  
  ## Statistical Information (an example)
  
  Let's combine 1 observation of our random variable, our probability function, and $\theta$ to quantify the Fisher information ([Link](https://en.wikipedia.org/wiki/Fisher_information#Single-parameter_Bernoulli_experiment)
  ):
  
  . . .
  
  ```{=tex}
  \begin{align}
  I(\theta) &= -E\left[\frac{\partial^2}{\partial\theta^2}\text{log}(\theta^y(1-\theta)^{1-y}) \right]\\
  \end{align}
  ```
  . . .
  
  For all samples (n), this is reduced to
  
  ```{=tex}
  \begin{align}
  I(\theta) &= \frac{n}{\theta(1-\theta)}\\
  \end{align}
  ```
  
  
  
  ## Statistical Information (an example)
  
  Therefore, for our sample
  
  ```{r,echo=TRUE}
  I = n/(theta.est*q.est)
  I
  ```
  
  . . .
  
  Consider how information varies by $\theta$ for a given sample size....
  
  ## Statistical Information (an example) {.scrollable}
  
  ```{r,echo=TRUE, fig.height=6,fig.width=12, fig.align='center'}
  #| echo: TRUE
  #| eval: TRUE
  #| code-line-numbers: 1|3|5,6|8
  
  thetas=seq(0.05,0.95,0.1)
  
  Information <- n/(thetas*(1-thetas))
  
  par(cex.lab=2.5,cex.axis=2.5,mar=c(5,5,2,2))
  plot(thetas,Information,type="b",lwd=8)
  
  abline(v=theta.est,col=2,lwd=8)
  ```
  
  ## Statistical Information (an example) {.scrollable}
  
  Funny enough, Fisher Information is the reciprocal of the variance of our estimate of $\theta$,$$Var[\hat{\theta}] = \frac{\hat{\theta}  (1-\hat{\theta})}{n}$$
  
  ## Statistical Information (an example)
  
  ```{r,echo=TRUE,fig.align='center',fig.width=12}
  #| echo: TRUE
  #| eval: TRUE
  #| code-line-numbers: 1|3|4,5|6
  
  thetas=seq(0.05,0.95,0.1)
  
  Var.theta <- (thetas*(1-thetas))/n
  par(cex.main=2.5,cex.axis=2.5,cex.lab=2.5,mar=c(5,5,2,2))
  plot(thetas,Var.theta,type="b",lwd=8)
  abline(v=theta.est,col=2,lwd=8)
  ```
  
  . . .
  
  How does this knowledge about this probability function inform the design of a study?
  
  ## Statistics
  
  A field dedicated to observing the real world to gain informational data.
  
  . . .
  
  ```{=html}
  <span style="color:#FF0000";><center>BUT</center></span>
  ```
  Often statistics classes only focus on Power Analysis.
  
  . . .
  
  ```{=html}
  To obtain <span style="color:#FF0000";>informational data</span> , we need to think about
  ```
  ::: incremental
  -   how our data will be created
  -   our question of the data
  -   a probability function to use and its parameters
  -   the goal of the question
  :::
  
  ## Reading

![](Information_files/reading0.png){fig-align="left" width="1000"} 

<br>

[Link](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010033)