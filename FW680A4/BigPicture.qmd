---
title:  <span style="color:black">Big Picture <br> and Probability</span>
title-slide-attributes:
  data-background-image: /img/thinking.png
  background-opacity: "0.45"
format: 
  revealjs:
    theme: simple
    slide-number: true
    show-slide-number: all
    chalkboard: true
    multiplex: true
---

## Sections

- Hypotheses and Predictions

- Big Data and Sampling

- Inference and Prediction

- Probability and Markdown

## Hypotheses/Predictions

#### Study Objective

#### Hypothesis

#### Prediction



## Placeholder

**Objective** to understand space-use of coyotes in urban areas.

**Justification** to provide guidance on managing urban coyotes 

**Hypothesis 1** urban coyotes have access to abundant food resources and thus require less space than non-urban coyotes 

**Prediction 1a** home range will be smaller for urban coyotes compared to non-urban coyotes
**Prediction 1b** territory (50% home range) will be smaller for urban coyotes compared to non-urban coyotes

**Hypothesis 2** urban coyotes are limited by habitat availability, not just food. Urban coyote space use is a function of available habitat.

**Prediction 2** coyote territory size will increase with increasing connected areas that have 'natural cover'

```{=html}
<style type="text/css">

body, td {
   font-size: 14px;
}
code.r{
  font-size: 30px;
}

pre {
  font-size: 45px;
}
</style>
<br>
<center>Data is everywhere; it is the era of <span style="color:#FF0000";> BIG DATA </span></center>
```
![](/img/BigData.png){fig-align="center" width="469"}

## Big Data Problems

<br>

["*The hidden Biases of Big Data*" by Kate Crawford](https://hbr.org/2013/04/the-hidden-biases-in-big-data)

**Harvard Business Review (2013)**



. . .

```{=html}
<br><span style="color:#FF0000";><center><em>"Can numbers actually speak for themselves?"</center></em></span>
```
![](/img/spreadsheet.png){fig-align="center" width="500"}

## Big Data Problems

<br>

["*The hidden Biases of Big Data*" by Kate Crawford](https://hbr.org/2013/04/the-hidden-biases-in-big-data)

**Harvard Business Review (2013)**


```{=html}
<br><span style="color:#FF0000";><center><em>"Data and data sets are not objective;"</center></em></span>
```
. . .

```{=html}
<span style="color:#FF0000";><center><em>"they are creations of human design."</center></em></span>
```
![](/img/thinking.png){fig-align="center" width="459"}

## Big Data Problems {.scrollable}

<br>

[**The Annals of Applied Statistics (2018); Xiao Li Meng,**](https://projecteuclid.org/journals/annals-of-applied-statistics/volume-12/issue-2/Statistical-paradises-and-paradoxes-in-big-data-I--Law/10.1214/18-AOAS1161SF.full)

```{=html}
<br><center><span><em>The Big Data Paradox: </em></span></center>
```

. . .

```{=html}
<center><span style="color:#FF0000";><em>
"the bigger the data, the surer we fool ourselves”</em></span> ... when we fail to account for our sampling process. </center>
```

. . .

<br>
<center>
Sampling Processes == Human Design
</center>

. . .

<br>

**What is worse?**

-   a very precise inaccurate result <br>
-   an accurate imprecise result


## Big Data Problems

<br>

![[link](https://www.nature.com/articles/s41586-021-04198-4)](Information_files/survey.data.png){fig-align="center" width="1000"}

. . .

```{=html}
<br><center><span style="color:#FF0000";><em>"...data quality matters more than data quantity, and that compensating the former with the latter is a mathematically provable losing proposition."</em></span>   </center>
```
## Big Data Problems {.scrollable}

Using eBird data w/o accounting for sampling biases.

. . .

![](Information_files/ebird.png){fig-align="center" fig-width="500"}

[Link1](https://www.pnas.org/doi/10.1073/pnas.2023170118). [Link2](https://www.pnas.org/doi/10.1073/pnas.2113862119). 

. . .

```{=html}
<br><center><span style="color:#FF0000";><em>"eBird reporting rates also depend heavily on species’ overlap with the activity of eBird users, which also varies by region, time, and habitat."</em></span>   </center>
```

. . .

## The Questioning Scientist {.scrollable}

<br>

In regard to *data* and *statistical models*, [21^st^ century scientists should be pragmatic, excited, and questioning.]{style="color:red"}

::: {.fragment .fade-in-then-semi-out}
-   How and why did these data come to be?
    -   *understand the study design*
    -   *ask this even when you design the study, after data collection*
:::

::: {.fragment .fade-in-then-semi-out}
-   What do these data look like?
    -   *visualize the data in many dimensions*
    -   *keep in mind - not all outcomes are visible in data -- example?*
:::

::: {.fragment .fade-in-then-semi-out}
-   How does this statistical model work?
    -   *statistical notation, explicit and implicit assumptions*, *optimization*
:::

::: {.fragment .fade-in-then-semi-out}
-   How does this statistical model fail in theory and in practice?
    -   *statistical robustness and identifiability*
:::

## Data vs. Information

```{=html}
<span style="color:#FF0000";><center>Data = Numbers/Groupings<br><br>
Data ≠ Information
</center>
</span>
```

. . .


![](Information_files/info_diagram.png){fig-align="center"}

## Information

```{=html}
Data contains <span><u>information</u></span>, depending on ...
```
::: incremental
-   the question being asked of the data
-   how the data came to be
-   the goal of the question
:::

## The question and the data

<br>

Ecological surveillance monitoring will often have *low quality information* regarding post-hoc hypotheses.

<br>

```{=html}
<span style="color:#FF0000";><center>
Example?
</center>
</span>
```

. . .

![](Information_files/banding.png){fig-align="center" width="494"}

## The goal of the question

::: incremental
-   learn about the data (<span style="color:red">data summary</span>)
-   apply learning outside of the data (<span style="color:red">inference</span>)
-   learn about conditions relevant to but not observed in the data (<span style="color:red">prediction</span>)
:::

<br>

. . .

```{=html}
<center>
<span style="color:#FF0000";>
<em>inference</em></span></span> and <span style="color:#FF0000";><em>prediction</em></span> are different goals, <em>optimally</em> requiring different data and statistical models, but  <br>
</span>
<br>
are also <u>not mutually exclusive</u>.
</center>
```
## Inference and Prediction 

```{=html}
<br>
<p style="font-size:45px; font-family:Garamond;">
<a href="https://www.jstor.org/stable/27836590">From "The strategy of model building in population biology" by Richard Levins (American Scientists, 1966) </a>:
</p>
<br>
```
```{=html}
"It is of course desirable to work with manageable models which maximize generality, realism, and precision toward the overlapping but not identical goals of <span style="color:#FF0000";><u>understanding</u></span>, <span style="color:#FF0000";><u>predicting</u></span>, and <span style="color:#FF0000";><u>modifying nature</u></span>. But this cannot be done."
```

## Infernence and Prediction {.scrollable}

```{=html}
<br>
<p style="font-size:45px; font-family:Garamond;">
<a href="https://projecteuclid.org/journals/statistical-science/volume-25/issue-3/To-Explain-or-to-Predict/10.1214/10-STS330.full">From "To Explain or to Predict" by Galit Shmueli (Statistical Science, 2010)</a>:
</p>
```

Explanatory modeling focuses on minimizing bias to obtain the most accurate representation of the underlying theory.

<br>

. . .

Predictive modeling focuses on minimizing both bias and estimation variance;

<br>

. . .

this may sacrifice theoretical accuracy for improved empirical precision.

## Infernence and Prediction

```{=html}
<span style="color:#0000FF";>This leads to a strange result: </span>
```
<br>

```{=html}
the <span style="color:#FF0000";>"wrong" </span> statistical model can predict better than the correct one.
```
. . .

<br>

```{=html}
<span style="color:#FF0000";><center>
Why?
</center>
</span>
```

## Statistical Information {.scrollable}

Let's turn to the field of statistics to understand *Information*


<br>

. . .

[Likelihood principle](https://en.wikipedia.org/wiki/Likelihood_principle)

Given a *statistical model*, all the evidence/information in a sample ($\textbf{y}$, i.e., data) relevant to model parameters ($\theta$) is contained in the *likelihood function*.

. . .

[Fisher Information](https://en.wikipedia.org/wiki/Fisher_information)

The information an observable random variable ($\textbf{y}$) has about an unknown parameter $\theta$ upon which the probability of $\textbf{y}$ $(f(\textbf{y};\theta)$ depends.

<br>

. . .

```{=html}
<br><span style="color:#FF0000";><center><em>Information is conditional</center></em></span><br> 
```
. . .

To learn about $\theta$ from $\textbf{y}$, we need to link them together via a special function, $f(\textbf{y};\theta)$

## Statistical Information

<br> The pieces:

::: incremental
-   The sample data, $\textbf{y}$

-   A probability function for $\textbf{y}$:

    -   $f(\textbf{y};\theta)$

    -   $[\textbf{y}|\theta]$



-   The unknown parameter: $\theta$

    -   specified in the probability function

:::

## Statistical Information (an example)

We want to know the proportion of a wetlands that contain a rare plant species. We can not sample the whole area.

![](Information_files/wetlands.png){fig-align="center" width="494"}

## Rare Plant Data

We randomly select plots and look for our plant.\
Our data are

```{r, echo = TRUE}
#| echo: TRUE
#| eval: TRUE
#| code-line-numbers: "1|2,3"
y <- c(0,1,1,1,1,0,1,0,0,0)
n <- length(y)
n
```

. . .

<br>

$\textbf{y}$ is a random variable as it depends on random events.

<br>

. . .

In this case, when we induced a random selection of sites.

## Probability/Likelihood Function

$f(\textbf{y};\theta)$ describes the probability for each $i^{th}$ data, $y_i$. 

But, not just for the data we observed, for all possible data that could be observed.

. . .

**Rules about our data:** <br> $y \in \{0,1\}$,

. . .

```{=html}
<p style="font-size:25px">
where the curly brackets imply discrete values in our "set".
```
. . .

**Not this:** $y \in [0,1]$,

```{=html}
<p style="font-size:25px">
where the square brackets indicate all real numbers from 0 to 1, including 0 and 1.
```

. . .

**Not this:** $y \in (0,1)$,

```{=html}
<p style="font-size:25px">
where the parantheses indicate all real numbers from 0 to 1, not including 0 and 1.
```


## Probability Function {.scrollable}

Two outcomes, 0 or 1, so we need two probabilities to describe our random variable.

. . .

$$
  f(y;\theta) = [y|\theta]= 
  \begin{cases}
    \theta     & \text{if $y = 1$}, \\
    1 - \theta & \text{if $y = 0$}.
  \end{cases}
$$

. . .

Also,

$$
f(y;\theta) = [y|\theta]=
  \begin{align}
        \theta^{y}\times(1-\theta)^{1-y}
  \end{align}
$$

. . .

Also,

$$
  \begin{align}
        P(Y=1) &= \theta \\
        P(Y=0) &= 1-\theta  
  \end{align}
$$

```{=html}
<center>
This probability function is called ________?
</center>
```
## Probability Function

Probabilities, such as our parameter, have rules:

::: incremental
-   $0 \leq \theta \leq 1$

-   $0 \leq (1 - \theta) \leq 1$

-   $\theta + (1-\theta) = 1$

:::

## Probability Function {.scrollable}

How do we find $\theta$ for our data?

. . .

We can use our probability function to calculate the likelihood of a parameter, given our data.

. . .

$$
\mathcal{L}(\theta|y) = \prod_{i=1}^{n} p(y_{i};\theta)
$$

. . .

```{r,echo=TRUE}
#| echo: TRUE
#| eval: TRUE
#| code-line-numbers: 2|5|8|11|14|17|20

#Bernoulli probability function
  prob.function=function(theta){prod(theta^y*(1-theta)^y)}

#possible probabilities
  theta.guess=matrix(seq(0.01,0.99,by=0.01))

#implement function
  likelihood=apply(theta.guess,1,prob.function)
  
#Find maximum likelood
  max.index=which.max(likelihood)

#Theta that maximizes our probability function
  theta.est=theta.guess[max.index]

#Define other probability
  q.est <- 1-theta.est

#Alternative estimation
  theta.est2 <- sum(y)/n

  
```

. . .

```{r,echo=FALSE}
#plot likelihood profile
  plot(theta.guess,likelihood)
  abline(v=theta.est,lwd=3,col=1,lty=1)
  abline(v=theta.est2,lwd=3,col=4,lty=4)
  legend("topright",lwd=3,col=c(1,4),
         legend=c("theta.est","theta.est2"))
```

## Statistical Information (an example)

Let's combine 1 observation of our random variable, our probability function, and $\theta$ to quantify the Fisher information ([Link](https://en.wikipedia.org/wiki/Fisher_information#Single-parameter_Bernoulli_experiment)
):

. . .

```{=tex}
\begin{align}
I(\theta) &= -E\left[\frac{\partial^2}{\partial\theta^2}\text{log}(\theta^y(1-\theta)^{1-y}) \right]\\
\end{align}
```
. . .

For all samples (n), this is reduced to

```{=tex}
\begin{align}
I(\theta) &= \frac{n}{\theta(1-\theta)}\\
\end{align}
```



## Statistical Information (an example)

Therefore, for our sample

```{r,echo=TRUE}
I = n/(theta.est*q.est)
I
```

. . .

Consider how information varies by $\theta$ for a given sample size....

## Statistical Information (an example) {.scrollable}

```{r,echo=TRUE, fig.height=6,fig.width=12, fig.align='center'}
#| echo: TRUE
#| eval: TRUE
#| code-line-numbers: 1|3|5,6|8

thetas=seq(0.05,0.95,0.1)

Information <- n/(thetas*(1-thetas))

par(cex.lab=2.5,cex.axis=2.5,mar=c(5,5,2,2))
plot(thetas,Information,type="b",lwd=8)

abline(v=theta.est,col=2,lwd=8)
```

## Statistical Information (an example) {.scrollable}

Funny enough, Fisher Information is the reciprocal of the variance of our estimate of $\theta$,$$Var[\hat{\theta}] = \frac{\hat{\theta}  (1-\hat{\theta})}{n}$$

## Statistical Information (an example)

```{r,echo=TRUE,fig.align='center',fig.width=12}
#| echo: TRUE
#| eval: TRUE
#| code-line-numbers: 1|3|4,5|6

thetas=seq(0.05,0.95,0.1)

Var.theta <- (thetas*(1-thetas))/n
par(cex.main=2.5,cex.axis=2.5,cex.lab=2.5,mar=c(5,5,2,2))
plot(thetas,Var.theta,type="b",lwd=8)
abline(v=theta.est,col=2,lwd=8)
```

. . .

How does this knowledge about this probability function inform the design of a study?

## Statistics

A field dedicated to observing the real world to gain informational data.

. . .

```{=html}
<span style="color:#FF0000";><center>BUT</center></span>
```
Often statistics classes only focus on Power Analysis.

. . .

```{=html}
To obtain <span style="color:#FF0000";>informational data</span> , we need to think about
```
::: incremental
-   how our data will be created
-   our question of the data
-   a probability function to use and its parameters
-   the goal of the question
:::

## Reading

![](Information_files/reading0.png){fig-align="left" width="1000"} 

<br>

[Link](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010033)